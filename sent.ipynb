{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2wdScfHeGOSfApZcky2VJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0e39f6ac05149698f9cf5c166f9e6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db9ca2a3d7ba4671a6a6d00c43a3b9dc",
              "IPY_MODEL_fb5d4818700c4355b3b6d6c4779e7bb3",
              "IPY_MODEL_91283276948046a49741ca6b393ca3a2"
            ],
            "layout": "IPY_MODEL_8153242e366a49ab9ad47d33b0a17dcb"
          }
        },
        "db9ca2a3d7ba4671a6a6d00c43a3b9dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_717ba67e7ad54833be1e4680e2e5b0ec",
            "placeholder": "​",
            "style": "IPY_MODEL_4eae68a1f1ca441fb5c3b0032a712048",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fb5d4818700c4355b3b6d6c4779e7bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_207172f2d3dc4ac69c33fdf9ce29fc51",
            "max": 252,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fabb4371434c4291b68254109b595560",
            "value": 252
          }
        },
        "91283276948046a49741ca6b393ca3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8887ead01ba429b9ae6fb55926224d4",
            "placeholder": "​",
            "style": "IPY_MODEL_b467ad6cfbb5450fac836ece15651b18",
            "value": " 252/252 [00:00&lt;00:00, 27.3kB/s]"
          }
        },
        "8153242e366a49ab9ad47d33b0a17dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717ba67e7ad54833be1e4680e2e5b0ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eae68a1f1ca441fb5c3b0032a712048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "207172f2d3dc4ac69c33fdf9ce29fc51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabb4371434c4291b68254109b595560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8887ead01ba429b9ae6fb55926224d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b467ad6cfbb5450fac836ece15651b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "decf91cdbf8b4af6a78635ed313151d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee6d4e77bc3e4c34a3c6acbd6f95e895",
              "IPY_MODEL_901ed1d16f424f89a4b10c74ee599882",
              "IPY_MODEL_b3ff2dd50eb44622906712690227f7b0"
            ],
            "layout": "IPY_MODEL_ddea3253219e42a18e3f901600aec94c"
          }
        },
        "ee6d4e77bc3e4c34a3c6acbd6f95e895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79fa3c1fa2df48878d02dbe97f91ade4",
            "placeholder": "​",
            "style": "IPY_MODEL_5d859755e675404e9770c720334e8d35",
            "value": "config.json: 100%"
          }
        },
        "901ed1d16f424f89a4b10c74ee599882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb9e95f5e264960b522076bae221b7b",
            "max": 758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b105bcf3816b4cc4b2a43c6dd78e3ef4",
            "value": 758
          }
        },
        "b3ff2dd50eb44622906712690227f7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_753963e0ca2c4464aa92c37360adc214",
            "placeholder": "​",
            "style": "IPY_MODEL_a6db9bc940814ed7891ff91ea70c015f",
            "value": " 758/758 [00:00&lt;00:00, 48.8kB/s]"
          }
        },
        "ddea3253219e42a18e3f901600aec94c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fa3c1fa2df48878d02dbe97f91ade4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d859755e675404e9770c720334e8d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eb9e95f5e264960b522076bae221b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b105bcf3816b4cc4b2a43c6dd78e3ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "753963e0ca2c4464aa92c37360adc214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6db9bc940814ed7891ff91ea70c015f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0010038f6644ddeaeffaf8d3a9ab37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d45f62a14342d3b46f650d7e853934",
              "IPY_MODEL_294c197c786045e38c00cf990a71196c",
              "IPY_MODEL_bdc4c7002e9646e88916bb56b5d27391"
            ],
            "layout": "IPY_MODEL_e5f1adc2e02a457e857c44a2f74bc887"
          }
        },
        "36d45f62a14342d3b46f650d7e853934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cbd0913c93d4a33a4c64aa8a21ca870",
            "placeholder": "​",
            "style": "IPY_MODEL_b22fc19fa22c4bf3a679eb584976d0f6",
            "value": "vocab.txt: "
          }
        },
        "294c197c786045e38c00cf990a71196c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a418140d32e34fea9aab47ee20caf8ed",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_baa578e8f4bb408abef448dd509b8a19",
            "value": 1
          }
        },
        "bdc4c7002e9646e88916bb56b5d27391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04bd1c14dec465b8cbb32f252de6426",
            "placeholder": "​",
            "style": "IPY_MODEL_5369ae862aa04903aca26d9887cd8108",
            "value": " 232k/? [00:00&lt;00:00, 17.4MB/s]"
          }
        },
        "e5f1adc2e02a457e857c44a2f74bc887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbd0913c93d4a33a4c64aa8a21ca870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b22fc19fa22c4bf3a679eb584976d0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a418140d32e34fea9aab47ee20caf8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "baa578e8f4bb408abef448dd509b8a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a04bd1c14dec465b8cbb32f252de6426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5369ae862aa04903aca26d9887cd8108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79974058a6274d9aab5f2c958ff08501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51968e84992b4dc4b509f0d7d3533563",
              "IPY_MODEL_5ad39b1ac123410d9c6fc05638975338",
              "IPY_MODEL_ae6bc0741e1746a29ca4cc51d8b0e2aa"
            ],
            "layout": "IPY_MODEL_d3d6b62ee3a64e088f90c875957f82b8"
          }
        },
        "51968e84992b4dc4b509f0d7d3533563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3afa73cc7fdb4a98aaf7747012c44608",
            "placeholder": "​",
            "style": "IPY_MODEL_b7966ef8e82845c6937ecf5c757414f4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5ad39b1ac123410d9c6fc05638975338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d4dd096fe424d4eaad3f59535287ac8",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1ff8530100a48a7a18ba36e52c1d5df",
            "value": 112
          }
        },
        "ae6bc0741e1746a29ca4cc51d8b0e2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27b98e474ac6408a8e01fb42a20ae3bf",
            "placeholder": "​",
            "style": "IPY_MODEL_7150414a8fc44530abd0befae43e9869",
            "value": " 112/112 [00:00&lt;00:00, 12.2kB/s]"
          }
        },
        "d3d6b62ee3a64e088f90c875957f82b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afa73cc7fdb4a98aaf7747012c44608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7966ef8e82845c6937ecf5c757414f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d4dd096fe424d4eaad3f59535287ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ff8530100a48a7a18ba36e52c1d5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27b98e474ac6408a8e01fb42a20ae3bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7150414a8fc44530abd0befae43e9869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "682e9b0941fa4524a81d84c202816f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc9d60e1af124d028ee3daf5dc00b73f",
              "IPY_MODEL_aa90b2c1af624e178efa3aeed56254b3",
              "IPY_MODEL_ec29bb3a41b64dc4895f32e0cddf5406"
            ],
            "layout": "IPY_MODEL_fe48956d116a459485b353f5bcce225b"
          }
        },
        "bc9d60e1af124d028ee3daf5dc00b73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8488a6c95ea4f89a8a05226044ff610",
            "placeholder": "​",
            "style": "IPY_MODEL_ff0e296373b34561bbceadb9d0a5b78d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "aa90b2c1af624e178efa3aeed56254b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dc32aadaee049f783ba7e9e6614f7a9",
            "max": 437992753,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce868c0c61944fee940fd3e7655ae8f0",
            "value": 437992753
          }
        },
        "ec29bb3a41b64dc4895f32e0cddf5406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae9ab5ec7e1a4acab63093d62baa168a",
            "placeholder": "​",
            "style": "IPY_MODEL_262bc50970b849e6a25b59777e88b592",
            "value": " 438M/438M [00:12&lt;00:00, 51.7MB/s]"
          }
        },
        "fe48956d116a459485b353f5bcce225b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8488a6c95ea4f89a8a05226044ff610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0e296373b34561bbceadb9d0a5b78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dc32aadaee049f783ba7e9e6614f7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce868c0c61944fee940fd3e7655ae8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae9ab5ec7e1a4acab63093d62baa168a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262bc50970b849e6a25b59777e88b592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09b708b5b61048b4882cf23c862bdb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bd96d0dbec944a7a8d0bd1d00d9f870",
              "IPY_MODEL_0a7f517e79f64b0589107493eb991f7a",
              "IPY_MODEL_c4cf2ab3b3174856a5478ef10356f35e"
            ],
            "layout": "IPY_MODEL_caaf1b66cf264985a7f10629d371554a"
          }
        },
        "5bd96d0dbec944a7a8d0bd1d00d9f870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fcb4acc84be4922afb669bab0f08da3",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9011845b644dcd94a03e4be2921318",
            "value": "model.safetensors: 100%"
          }
        },
        "0a7f517e79f64b0589107493eb991f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae845a4e0b04ce98e56873f4dca74a5",
            "max": 437965908,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa9f6bf2a0ab47868229907f70547630",
            "value": 437965908
          }
        },
        "c4cf2ab3b3174856a5478ef10356f35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f89267eea844475ca60a556986c26d34",
            "placeholder": "​",
            "style": "IPY_MODEL_062360c5d2674183909c76a9f668cdaa",
            "value": " 438M/438M [00:05&lt;00:00, 116MB/s]"
          }
        },
        "caaf1b66cf264985a7f10629d371554a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcb4acc84be4922afb669bab0f08da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9011845b644dcd94a03e4be2921318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ae845a4e0b04ce98e56873f4dca74a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9f6bf2a0ab47868229907f70547630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f89267eea844475ca60a556986c26d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "062360c5d2674183909c76a9f668cdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veerendra4923/Cam_Grey/blob/master/sent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htx4EJW7eBps",
        "outputId": "e40fcff7-2b08-4d0b-99a5-9a6dd41cf18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=7be7ca269820afa85d53dae93ca8268f1cc99e753fa7a768966a8f7a8b20a380\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, feedparser, update_checker, prawcore, nvidia-cusparse-cu12, nvidia-cudnn-cu12, praw, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed feedparser-6.0.11 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 praw-7.8.1 prawcore-2.4.0 sgmllib3k-1.0.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch yfinance pandas feedparser praw requests numpy scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finbert_finetuning.py - Fine-tune FinBERT on Financial Sentiment Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🔧 Using device: {device}\")\n",
        "\n",
        "class FinancialSentimentDataset(Dataset):\n",
        "    \"\"\"Custom dataset for financial sentiment analysis\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class FinBERTTrainer:\n",
        "    \"\"\"FinBERT Fine-tuning class\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"ProsusAI/finbert\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "    def load_and_prepare_data(self, csv_file_path):\n",
        "        \"\"\"Load and prepare the financial sentiment dataset\"\"\"\n",
        "        print(\"📥 Loading dataset...\")\n",
        "\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        print(f\"📊 Loaded {len(df)} samples\")\n",
        "\n",
        "        # Display dataset info\n",
        "        print(f\"\\n📋 Dataset Info:\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "\n",
        "        # Check for required columns\n",
        "        required_columns = ['text', 'label']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            print(f\"❌ Missing required columns. Expected: {required_columns}\")\n",
        "            print(f\"Available columns: {list(df.columns)}\")\n",
        "            return False\n",
        "\n",
        "        # Remove any rows with missing values\n",
        "        df = df.dropna(subset=['text', 'label'])\n",
        "        print(f\"📊 After removing NaN: {len(df)} samples\")\n",
        "\n",
        "        # Display label distribution\n",
        "        print(f\"\\n📊 Label Distribution:\")\n",
        "        label_counts = df['label'].value_counts()\n",
        "        print(label_counts)\n",
        "\n",
        "        # Map labels to integers\n",
        "        label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "        df['label_encoded'] = df['label'].map(label_mapping)\n",
        "\n",
        "        # Check for unmapped labels\n",
        "        unmapped = df[df['label_encoded'].isna()]\n",
        "        if len(unmapped) > 0:\n",
        "            print(f\"⚠️ Found {len(unmapped)} unmapped labels:\")\n",
        "            print(unmapped['label'].unique())\n",
        "            df = df.dropna(subset=['label_encoded'])\n",
        "            print(f\"📊 After removing unmapped labels: {len(df)} samples\")\n",
        "\n",
        "        # Extract texts and labels\n",
        "        texts = df['text'].tolist()\n",
        "        labels = df['label_encoded'].astype(int).tolist()\n",
        "\n",
        "        # Split data\n",
        "        print(f\"\\n🔄 Splitting data...\")\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        print(f\"📊 Train: {len(X_train)} samples\")\n",
        "        print(f\"📊 Validation: {len(X_val)} samples\")\n",
        "        print(f\"📊 Test: {len(X_test)} samples\")\n",
        "\n",
        "        # Store splits\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "        self.X_val, self.y_val = X_val, y_val\n",
        "        self.X_test, self.y_test = X_test, y_test\n",
        "        self.label_mapping = label_mapping\n",
        "        self.reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "        return True\n",
        "\n",
        "    def prepare_model_and_tokenizer(self):\n",
        "        \"\"\"Initialize FinBERT model and tokenizer\"\"\"\n",
        "        print(f\"\\n🤖 Loading FinBERT model: {self.model_name}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Load model for sequence classification\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.model_name,\n",
        "            num_labels=3,  # negative, neutral, positive\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        self.model.to(device)\n",
        "\n",
        "        print(\"✅ Model and tokenizer loaded successfully!\")\n",
        "\n",
        "    def create_datasets(self, max_length=512):\n",
        "        \"\"\"Create PyTorch datasets\"\"\"\n",
        "        print(f\"\\n📦 Creating datasets with max_length={max_length}...\")\n",
        "\n",
        "        self.train_dataset = FinancialSentimentDataset(\n",
        "            self.X_train, self.y_train, self.tokenizer, max_length\n",
        "        )\n",
        "\n",
        "        self.val_dataset = FinancialSentimentDataset(\n",
        "            self.X_val, self.y_val, self.tokenizer, max_length\n",
        "        )\n",
        "\n",
        "        self.test_dataset = FinancialSentimentDataset(\n",
        "            self.X_test, self.y_test, self.tokenizer, max_length\n",
        "        )\n",
        "\n",
        "        print(\"✅ Datasets created successfully!\")\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        \"\"\"Compute metrics for evaluation\"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "        }\n",
        "\n",
        "    def train_model(self, output_dir=\"./finbert_financial_sentiment\",\n",
        "                   num_epochs=3, batch_size=16, learning_rate=2e-5):\n",
        "        \"\"\"Fine-tune the FinBERT model\"\"\"\n",
        "        print(f\"\\n🚀 Starting fine-tuning...\")\n",
        "        print(f\"📊 Training parameters:\")\n",
        "        print(f\"   • Epochs: {num_epochs}\")\n",
        "        print(f\"   • Batch size: {batch_size}\")\n",
        "        print(f\"   • Learning rate: {learning_rate}\")\n",
        "        print(f\"   • Output directory: {output_dir}\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Check transformers version and set appropriate parameters\n",
        "        import transformers\n",
        "        transformers_version = transformers.__version__\n",
        "        print(f\"🔧 Transformers version: {transformers_version}\")\n",
        "\n",
        "        try:\n",
        "            # Try with newer parameter name first\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=output_dir,\n",
        "                num_train_epochs=num_epochs,\n",
        "                per_device_train_batch_size=batch_size,\n",
        "                per_device_eval_batch_size=batch_size,\n",
        "                warmup_steps=500,\n",
        "                weight_decay=0.01,\n",
        "                learning_rate=learning_rate,\n",
        "                logging_dir=f'{output_dir}/logs',\n",
        "                logging_steps=100,\n",
        "                eval_strategy=\"steps\",  # Updated parameter name\n",
        "                eval_steps=500,\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=500,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"accuracy\",\n",
        "                greater_is_better=True,\n",
        "                report_to=None,  # Disable wandb/tensorboard\n",
        "                save_total_limit=2,\n",
        "                dataloader_pin_memory=False,  # For Colab compatibility\n",
        "            )\n",
        "            print(\"✅ Using 'eval_strategy' parameter\")\n",
        "        except TypeError:\n",
        "            try:\n",
        "                # Fallback to older parameter name\n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir=output_dir,\n",
        "                    num_train_epochs=num_epochs,\n",
        "                    per_device_train_batch_size=batch_size,\n",
        "                    per_device_eval_batch_size=batch_size,\n",
        "                    warmup_steps=500,\n",
        "                    weight_decay=0.01,\n",
        "                    learning_rate=learning_rate,\n",
        "                    logging_dir=f'{output_dir}/logs',\n",
        "                    logging_steps=100,\n",
        "                    evaluation_strategy=\"steps\",  # Older parameter name\n",
        "                    eval_steps=500,\n",
        "                    save_strategy=\"steps\",\n",
        "                    save_steps=500,\n",
        "                    load_best_model_at_end=True,\n",
        "                    metric_for_best_model=\"accuracy\",\n",
        "                    greater_is_better=True,\n",
        "                    report_to=None,  # Disable wandb/tensorboard\n",
        "                    save_total_limit=2,\n",
        "                    dataloader_pin_memory=False,  # For Colab compatibility\n",
        "                )\n",
        "                print(\"✅ Using 'evaluation_strategy' parameter (older version)\")\n",
        "            except TypeError:\n",
        "                # Minimal configuration as last resort\n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir=output_dir,\n",
        "                    num_train_epochs=num_epochs,\n",
        "                    per_device_train_batch_size=batch_size,\n",
        "                    per_device_eval_batch_size=batch_size,\n",
        "                    learning_rate=learning_rate,\n",
        "                    logging_steps=100,\n",
        "                    save_strategy=\"epoch\",\n",
        "                    load_best_model_at_end=True,\n",
        "                    report_to=None,\n",
        "                    save_total_limit=2,\n",
        "                    dataloader_pin_memory=False,\n",
        "                )\n",
        "                print(\"⚠️ Using minimal TrainingArguments configuration\")\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=self.train_dataset,\n",
        "            eval_dataset=self.val_dataset,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "        )\n",
        "\n",
        "        # Start training\n",
        "        print(\"🏋️ Training started...\")\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        # Save the fine-tuned model\n",
        "        print(f\"\\n💾 Saving fine-tuned model to {output_dir}...\")\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Save training info\n",
        "        training_info = {\n",
        "            'model_name': self.model_name,\n",
        "            'num_epochs': num_epochs,\n",
        "            'batch_size': batch_size,\n",
        "            'learning_rate': learning_rate,\n",
        "            'train_samples': len(self.X_train),\n",
        "            'val_samples': len(self.X_val),\n",
        "            'test_samples': len(self.X_test),\n",
        "            'label_mapping': self.label_mapping,\n",
        "            'training_time': str(datetime.now()),\n",
        "            'train_loss': train_result.training_loss,\n",
        "            'transformers_version': transformers_version,\n",
        "        }\n",
        "\n",
        "        with open(f'{output_dir}/training_info.json', 'w') as f:\n",
        "            json.dump(training_info, f, indent=2)\n",
        "\n",
        "        print(\"✅ Model saved successfully!\")\n",
        "        return trainer\n",
        "\n",
        "    def evaluate_model(self, trainer, output_dir=\"./finbert_financial_sentiment\"):\n",
        "        \"\"\"Evaluate the fine-tuned model\"\"\"\n",
        "        print(f\"\\n📊 Evaluating model...\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_results = trainer.evaluate()\n",
        "        print(f\"📊 Validation Results:\")\n",
        "        for key, value in val_results.items():\n",
        "            print(f\"   • {key}: {value:.4f}\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_results = trainer.evaluate(self.test_dataset)\n",
        "        print(f\"\\n📊 Test Results:\")\n",
        "        for key, value in test_results.items():\n",
        "            print(f\"   • {key}: {value:.4f}\")\n",
        "\n",
        "        # Generate detailed predictions for test set\n",
        "        predictions = trainer.predict(self.test_dataset)\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = self.y_test\n",
        "\n",
        "        # Calculate detailed metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        print(f\"\\n📊 Detailed Test Metrics:\")\n",
        "        print(f\"   • Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Classification report\n",
        "        target_names = [self.reverse_label_mapping[i] for i in range(3)]\n",
        "        report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "        print(f\"\\n📋 Classification Report:\")\n",
        "        print(report)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=target_names, yticklabels=target_names)\n",
        "        plt.title('Confusion Matrix - Fine-tuned FinBERT')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Save evaluation results\n",
        "        eval_results = {\n",
        "            'val_accuracy': val_results.get('eval_accuracy', 0),\n",
        "            'test_accuracy': accuracy,\n",
        "            'classification_report': report,\n",
        "            'confusion_matrix': cm.tolist(),\n",
        "        }\n",
        "\n",
        "        with open(f'{output_dir}/evaluation_results.json', 'w') as f:\n",
        "            json.dump(eval_results, f, indent=2)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def test_sample_predictions(self, output_dir=\"./finbert_financial_sentiment\", num_samples=10):\n",
        "        \"\"\"Test model with sample predictions\"\"\"\n",
        "        print(f\"\\n🧪 Testing sample predictions...\")\n",
        "\n",
        "        # Load the saved model for testing\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Sample some test examples\n",
        "        sample_indices = np.random.choice(len(self.X_test), min(num_samples, len(self.X_test)), replace=False)\n",
        "\n",
        "        print(f\"\\n📋 Sample Predictions:\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            text = self.X_test[idx]\n",
        "            true_label = self.reverse_label_mapping[self.y_test[idx]]\n",
        "\n",
        "            # Make prediction\n",
        "            inputs = tokenizer(text, return_tensors='pt', truncation=True,\n",
        "                             padding=True, max_length=512)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "                confidence = torch.max(predictions).item()\n",
        "\n",
        "            predicted_label = self.reverse_label_mapping[predicted_class]\n",
        "\n",
        "            print(f\"\\n🔍 Sample {i+1}:\")\n",
        "            print(f\"Text: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
        "            print(f\"True: {true_label}\")\n",
        "            print(f\"Predicted: {predicted_label} (confidence: {confidence:.3f})\")\n",
        "            print(f\"Correct: {'✅' if true_label == predicted_label else '❌'}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "def load_dataset(file_path=\"final_merged_financial_sentiment_dataset.csv\"):\n",
        "    \"\"\"Load the financial sentiment dataset\"\"\"\n",
        "    print(f\"📥 Loading dataset from: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"✅ Dataset loaded successfully!\")\n",
        "        print(f\"📊 Shape: {df.shape}\")\n",
        "        print(f\"📋 Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Display first few rows\n",
        "        print(f\"\\n📋 First 5 rows:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Check data quality\n",
        "        print(f\"\\n🔍 Data Quality Check:\")\n",
        "        print(f\"Missing values in 'text': {df['text'].isna().sum()}\")\n",
        "        print(f\"Missing values in 'label': {df['label'].isna().sum()}\")\n",
        "\n",
        "        # Label distribution\n",
        "        print(f\"\\n📊 Label Distribution:\")\n",
        "        print(df['label'].value_counts())\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ File not found: {file_path}\")\n",
        "        print(\"Please ensure the CSV file is in the correct location.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main fine-tuning pipeline\"\"\"\n",
        "    print(\"🚀 FINBERT FINE-TUNING FOR FINANCIAL SENTIMENT ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Configuration\n",
        "    CSV_FILE_PATH = \"final_merged_financial_sentiment_dataset.csv\"\n",
        "    OUTPUT_DIR = \"./finbert_financial_sentiment\"\n",
        "\n",
        "    print(f\"📋 Configuration:\")\n",
        "    print(f\"   • Dataset: {CSV_FILE_PATH}\")\n",
        "    print(f\"   • Output Directory: {OUTPUT_DIR}\")\n",
        "    print(f\"   • Device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    df = load_dataset(CSV_FILE_PATH)\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = FinBERTTrainer()\n",
        "\n",
        "    # Load and prepare data\n",
        "    if not trainer.load_and_prepare_data(CSV_FILE_PATH):\n",
        "        return\n",
        "\n",
        "    # Prepare model and tokenizer\n",
        "    trainer.prepare_model_and_tokenizer()\n",
        "\n",
        "    # Create datasets\n",
        "    trainer.create_datasets()\n",
        "\n",
        "    # Get training parameters from user\n",
        "    print(f\"\\n⚙️ Training Parameters:\")\n",
        "\n",
        "    try:\n",
        "        epochs = int(input(\"Enter number of epochs (default: 3): \") or \"3\")\n",
        "        batch_size = int(input(\"Enter batch size (default: 16): \") or \"16\")\n",
        "        learning_rate = float(input(\"Enter learning rate (default: 2e-5): \") or \"2e-5\")\n",
        "    except ValueError:\n",
        "        print(\"Using default parameters...\")\n",
        "        epochs = 3\n",
        "        batch_size = 16\n",
        "        learning_rate = 2e-5\n",
        "\n",
        "    print(f\"\\n🏋️ Starting training with:\")\n",
        "    print(f\"   • Epochs: {epochs}\")\n",
        "    print(f\"   • Batch size: {batch_size}\")\n",
        "    print(f\"   • Learning rate: {learning_rate}\")\n",
        "\n",
        "    # Train model\n",
        "    trainer_obj = trainer.train_model(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    accuracy = trainer.evaluate_model(trainer_obj, OUTPUT_DIR)\n",
        "\n",
        "    # Test sample predictions\n",
        "    trainer.test_sample_predictions(OUTPUT_DIR)\n",
        "\n",
        "    # Create download script for Colab\n",
        "    create_download_script(OUTPUT_DIR, accuracy)\n",
        "\n",
        "    print(f\"\\n🎉 Fine-tuning completed successfully!\")\n",
        "    print(f\"📊 Final test accuracy: {accuracy:.4f}\")\n",
        "    print(f\"💾 Model saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "def create_download_script(output_dir, accuracy):\n",
        "    \"\"\"Create a script to download the model in Colab\"\"\"\n",
        "    download_script = f\"\"\"\n",
        "# Download Fine-tuned FinBERT Model - Run this in a new cell\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def download_finetuned_model():\n",
        "    model_dir = \"{output_dir}\"\n",
        "    zip_filename = \"finbert_financial_sentiment_model.zip\"\n",
        "\n",
        "    print(\"📦 Creating zip file...\")\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(model_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Add file to zip with relative path\n",
        "                zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(model_dir)))\n",
        "\n",
        "    print(f\"✅ Model zipped successfully!\")\n",
        "    print(f\"📊 Model accuracy: {accuracy:.4f}\")\n",
        "    print(f\"📥 Downloading {{zip_filename}}...\")\n",
        "\n",
        "    # Download the zip file\n",
        "    files.download(zip_filename)\n",
        "\n",
        "    print(\"✅ Download complete!\")\n",
        "    print(\"\\\\n📋 To use this model:\")\n",
        "    print(\"1. Extract the zip file\")\n",
        "    print(\"2. Place the 'finbert_financial_sentiment' folder in your project\")\n",
        "    print(\"3. Update the model_path in your sentiment analysis script\")\n",
        "\n",
        "# Run the download\n",
        "download_finetuned_model()\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"download_model.py\", \"w\") as f:\n",
        "        f.write(download_script)\n",
        "\n",
        "    print(f\"\\n📥 Download script created: download_model.py\")\n",
        "    print(\"Run this script in a new Colab cell to download your fine-tuned model!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a0e39f6ac05149698f9cf5c166f9e6d8",
            "db9ca2a3d7ba4671a6a6d00c43a3b9dc",
            "fb5d4818700c4355b3b6d6c4779e7bb3",
            "91283276948046a49741ca6b393ca3a2",
            "8153242e366a49ab9ad47d33b0a17dcb",
            "717ba67e7ad54833be1e4680e2e5b0ec",
            "4eae68a1f1ca441fb5c3b0032a712048",
            "207172f2d3dc4ac69c33fdf9ce29fc51",
            "fabb4371434c4291b68254109b595560",
            "e8887ead01ba429b9ae6fb55926224d4",
            "b467ad6cfbb5450fac836ece15651b18",
            "decf91cdbf8b4af6a78635ed313151d7",
            "ee6d4e77bc3e4c34a3c6acbd6f95e895",
            "901ed1d16f424f89a4b10c74ee599882",
            "b3ff2dd50eb44622906712690227f7b0",
            "ddea3253219e42a18e3f901600aec94c",
            "79fa3c1fa2df48878d02dbe97f91ade4",
            "5d859755e675404e9770c720334e8d35",
            "4eb9e95f5e264960b522076bae221b7b",
            "b105bcf3816b4cc4b2a43c6dd78e3ef4",
            "753963e0ca2c4464aa92c37360adc214",
            "a6db9bc940814ed7891ff91ea70c015f",
            "e0010038f6644ddeaeffaf8d3a9ab37e",
            "36d45f62a14342d3b46f650d7e853934",
            "294c197c786045e38c00cf990a71196c",
            "bdc4c7002e9646e88916bb56b5d27391",
            "e5f1adc2e02a457e857c44a2f74bc887",
            "8cbd0913c93d4a33a4c64aa8a21ca870",
            "b22fc19fa22c4bf3a679eb584976d0f6",
            "a418140d32e34fea9aab47ee20caf8ed",
            "baa578e8f4bb408abef448dd509b8a19",
            "a04bd1c14dec465b8cbb32f252de6426",
            "5369ae862aa04903aca26d9887cd8108",
            "79974058a6274d9aab5f2c958ff08501",
            "51968e84992b4dc4b509f0d7d3533563",
            "5ad39b1ac123410d9c6fc05638975338",
            "ae6bc0741e1746a29ca4cc51d8b0e2aa",
            "d3d6b62ee3a64e088f90c875957f82b8",
            "3afa73cc7fdb4a98aaf7747012c44608",
            "b7966ef8e82845c6937ecf5c757414f4",
            "1d4dd096fe424d4eaad3f59535287ac8",
            "c1ff8530100a48a7a18ba36e52c1d5df",
            "27b98e474ac6408a8e01fb42a20ae3bf",
            "7150414a8fc44530abd0befae43e9869",
            "682e9b0941fa4524a81d84c202816f3e",
            "bc9d60e1af124d028ee3daf5dc00b73f",
            "aa90b2c1af624e178efa3aeed56254b3",
            "ec29bb3a41b64dc4895f32e0cddf5406",
            "fe48956d116a459485b353f5bcce225b",
            "e8488a6c95ea4f89a8a05226044ff610",
            "ff0e296373b34561bbceadb9d0a5b78d",
            "2dc32aadaee049f783ba7e9e6614f7a9",
            "ce868c0c61944fee940fd3e7655ae8f0",
            "ae9ab5ec7e1a4acab63093d62baa168a",
            "262bc50970b849e6a25b59777e88b592",
            "09b708b5b61048b4882cf23c862bdb01",
            "5bd96d0dbec944a7a8d0bd1d00d9f870",
            "0a7f517e79f64b0589107493eb991f7a",
            "c4cf2ab3b3174856a5478ef10356f35e",
            "caaf1b66cf264985a7f10629d371554a",
            "9fcb4acc84be4922afb669bab0f08da3",
            "2d9011845b644dcd94a03e4be2921318",
            "3ae845a4e0b04ce98e56873f4dca74a5",
            "fa9f6bf2a0ab47868229907f70547630",
            "f89267eea844475ca60a556986c26d34",
            "062360c5d2674183909c76a9f668cdaa"
          ]
        },
        "id": "EbvHoguWxkwJ",
        "outputId": "ee642184-77b5-4466-cf62-aee6d4132c52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Using device: cuda\n",
            "🚀 FINBERT FINE-TUNING FOR FINANCIAL SENTIMENT ANALYSIS\n",
            "======================================================================\n",
            "📋 Configuration:\n",
            "   • Dataset: final_merged_financial_sentiment_dataset.csv\n",
            "   • Output Directory: ./finbert_financial_sentiment\n",
            "   • Device: cuda\n",
            "📥 Loading dataset from: final_merged_financial_sentiment_dataset.csv\n",
            "✅ Dataset loaded successfully!\n",
            "📊 Shape: (2302, 3)\n",
            "📋 Columns: ['text', 'label', 'source']\n",
            "\n",
            "📋 First 5 rows:\n",
            "                                                text     label  \\\n",
            "0  According to Gran , the company has no plans t...   neutral   \n",
            "1  For the last quarter of 2010 , Componenta 's n...  positive   \n",
            "2  In the third quarter of 2010 , net sales incre...  positive   \n",
            "3  Operating profit rose to EUR 13.1 mn from EUR ...  positive   \n",
            "4  Operating profit totalled EUR 21.1 mn , up fro...  positive   \n",
            "\n",
            "                source  \n",
            "0  PhraseBank_AllAgree  \n",
            "1  PhraseBank_AllAgree  \n",
            "2  PhraseBank_AllAgree  \n",
            "3  PhraseBank_AllAgree  \n",
            "4  PhraseBank_AllAgree  \n",
            "\n",
            "🔍 Data Quality Check:\n",
            "Missing values in 'text': 0\n",
            "Missing values in 'label': 0\n",
            "\n",
            "📊 Label Distribution:\n",
            "label\n",
            "neutral     1402\n",
            "positive     590\n",
            "negative     310\n",
            "Name: count, dtype: int64\n",
            "📥 Loading dataset...\n",
            "📊 Loaded 2302 samples\n",
            "\n",
            "📋 Dataset Info:\n",
            "Columns: ['text', 'label', 'source']\n",
            "Shape: (2302, 3)\n",
            "📊 After removing NaN: 2302 samples\n",
            "\n",
            "📊 Label Distribution:\n",
            "label\n",
            "neutral     1402\n",
            "positive     590\n",
            "negative     310\n",
            "Name: count, dtype: int64\n",
            "\n",
            "🔄 Splitting data...\n",
            "📊 Train: 1472 samples\n",
            "📊 Validation: 369 samples\n",
            "📊 Test: 461 samples\n",
            "\n",
            "🤖 Loading FinBERT model: ProsusAI/finbert\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0e39f6ac05149698f9cf5c166f9e6d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "decf91cdbf8b4af6a78635ed313151d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0010038f6644ddeaeffaf8d3a9ab37e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79974058a6274d9aab5f2c958ff08501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "682e9b0941fa4524a81d84c202816f3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and tokenizer loaded successfully!\n",
            "\n",
            "📦 Creating datasets with max_length=512...\n",
            "✅ Datasets created successfully!\n",
            "\n",
            "⚙️ Training Parameters:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09b708b5b61048b4882cf23c862bdb01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of epochs (default: 3): 3\n",
            "Enter batch size (default: 16): 16\n",
            "Enter learning rate (default: 2e-5): 2e-5\n",
            "\n",
            "🏋️ Starting training with:\n",
            "   • Epochs: 3\n",
            "   • Batch size: 16\n",
            "   • Learning rate: 2e-05\n",
            "\n",
            "🚀 Starting fine-tuning...\n",
            "📊 Training parameters:\n",
            "   • Epochs: 3\n",
            "   • Batch size: 16\n",
            "   • Learning rate: 2e-05\n",
            "   • Output directory: ./finbert_financial_sentiment\n",
            "🔧 Transformers version: 4.54.0\n",
            "✅ Using 'eval_strategy' parameter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏋️ Training started...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mveerendrasaimogilicharla\u001b[0m (\u001b[33mveerendrasaimogilicharla-naventra\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250804_034525-632t5nn9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/veerendrasaimogilicharla-naventra/huggingface/runs/632t5nn9' target=\"_blank\">./finbert_financial_sentiment</a></strong> to <a href='https://wandb.ai/veerendrasaimogilicharla-naventra/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/veerendrasaimogilicharla-naventra/huggingface' target=\"_blank\">https://wandb.ai/veerendrasaimogilicharla-naventra/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/veerendrasaimogilicharla-naventra/huggingface/runs/632t5nn9' target=\"_blank\">https://wandb.ai/veerendrasaimogilicharla-naventra/huggingface/runs/632t5nn9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='276' max='276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [276/276 06:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Saving fine-tuned model to ./finbert_financial_sentiment...\n",
            "✅ Model saved successfully!\n",
            "\n",
            "📊 Evaluating model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Validation Results:\n",
            "   • eval_loss: 0.1438\n",
            "   • eval_accuracy: 0.9485\n",
            "   • eval_runtime: 10.6381\n",
            "   • eval_samples_per_second: 34.6870\n",
            "   • eval_steps_per_second: 2.2560\n",
            "   • epoch: 3.0000\n",
            "\n",
            "📊 Test Results:\n",
            "   • eval_loss: 0.1331\n",
            "   • eval_accuracy: 0.9436\n",
            "   • eval_runtime: 13.2300\n",
            "   • eval_samples_per_second: 34.8450\n",
            "   • eval_steps_per_second: 2.1920\n",
            "   • epoch: 3.0000\n",
            "\n",
            "📊 Detailed Test Metrics:\n",
            "   • Accuracy: 0.9436\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.92      0.86        62\n",
            "     neutral       0.98      0.99      0.98       281\n",
            "    positive       0.94      0.86      0.89       118\n",
            "\n",
            "    accuracy                           0.94       461\n",
            "   macro avg       0.91      0.92      0.91       461\n",
            "weighted avg       0.95      0.94      0.94       461\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Testing sample predictions...\n",
            "\n",
            "📋 Sample Predictions:\n",
            "================================================================================\n",
            "\n",
            "🔍 Sample 1:\n",
            "Text: Cash flow from operations in January-December 2008 was a negative EUR 18.1 mn compared to EUR 39.0 m...\n",
            "True: negative\n",
            "Predicted: negative (confidence: 0.937)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 2:\n",
            "Text: Nordstjernan has used its option to buy another 22.4 % stake of Salcomp 's shares and votes .\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.887)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 3:\n",
            "Text: Our key geographical markets are Europe , Russian Federation , Middle-East , South-Africa and Japan ...\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.980)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 4:\n",
            "Text: 25 November 2010 - Finnish paints and coatings company Tikkurila Oyj ( HEL : TIK1V ) said today that...\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.941)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 5:\n",
            "Text: Affecto has participated in the program for the development of the Norwegian pension system since 20...\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.979)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 6:\n",
            "Text: Cargotec Corporation , Press Release , August 26 , 2008 at 10 a.m. Finnish time Cargotec 's MacGREGO...\n",
            "True: positive\n",
            "Predicted: neutral (confidence: 0.865)\n",
            "Correct: ❌\n",
            "\n",
            "🔍 Sample 7:\n",
            "Text: The company said that the results of the third quarter do not include non-recurring items .\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.984)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 8:\n",
            "Text: Established in 1989 , CapMan manages Nordic buyout , mezzanine , technology , life science and real ...\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.982)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 9:\n",
            "Text: The company 's operating income ( EBIT ) totalled EUR 0.0 mn , up from EUR -0.3 mn year-on-year .\n",
            "True: positive\n",
            "Predicted: positive (confidence: 0.971)\n",
            "Correct: ✅\n",
            "\n",
            "🔍 Sample 10:\n",
            "Text: The company also appointed Leif Rosen head of the Special Plate unit which includes the quarto plate...\n",
            "True: neutral\n",
            "Predicted: neutral (confidence: 0.985)\n",
            "Correct: ✅\n",
            "================================================================================\n",
            "\n",
            "📥 Download script created: download_model.py\n",
            "Run this script in a new Colab cell to download your fine-tuned model!\n",
            "\n",
            "🎉 Fine-tuning completed successfully!\n",
            "📊 Final test accuracy: 0.9436\n",
            "💾 Model saved to: ./finbert_financial_sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Save your training accuracy\n",
        "training_info = {\n",
        "    \"test_accuracy\": 0.9436,\n",
        "    \"test_accuracy_percentage\": \"94.36%\",\n",
        "    \"model_type\": \"fine_tuned_finbert_financial\",\n",
        "    \"performance_rating\": \"excellent\"\n",
        "}\n",
        "\n",
        "# Save to model directory\n",
        "with open(\"./finbert_financial_sentiment/training_info.json\", 'w') as f:\n",
        "    json.dump(training_info, f, indent=2)\n",
        "\n",
        "print(\"✅ Training info saved! Accuracy will now display correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJgi_jeD-VQe",
        "outputId": "9a5b3738-3224-4a67-ff01-adea76db6495"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training info saved! Accuracy will now display correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# financial_sentiment_analysis.py - Complete Sentiment Analysis with Fine-tuned FinBERT on Financial Data\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Try to import optional libraries\n",
        "try:\n",
        "    import feedparser\n",
        "    HAS_FEEDPARSER = True\n",
        "except ImportError:\n",
        "    HAS_FEEDPARSER = False\n",
        "    print(\"⚠️ feedparser not installed. GNews and RSS feeds will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    import praw\n",
        "    HAS_REDDIT = True\n",
        "except ImportError:\n",
        "    HAS_REDDIT = False\n",
        "    print(\"⚠️ praw not installed. Reddit data will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    import torch\n",
        "    HAS_TRANSFORMERS = True\n",
        "except ImportError:\n",
        "    HAS_TRANSFORMERS = False\n",
        "    print(\"⚠️ transformers not installed. Fine-tuned FinBERT will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    import yfinance as yf\n",
        "    import pandas as pd\n",
        "    HAS_YFINANCE = True\n",
        "except ImportError:\n",
        "    HAS_YFINANCE = False\n",
        "    print(\"⚠️ yfinance not installed. Stock market data snapshot will be unavailable.\")\n",
        "\n",
        "# API Configuration - Using environment variables or None\n",
        "def get_api_key(key_name: str) -> Optional[str]:\n",
        "    \"\"\"Safely get API keys from environment or return None\"\"\"\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(key_name)\n",
        "    except:\n",
        "        return os.environ.get(key_name)\n",
        "\n",
        "API_KEYS = {\n",
        "    \"REDDIT_CLIENT_ID\": get_api_key('reddit_client_id'),\n",
        "    \"REDDIT_CLIENT_SECRET\": get_api_key('reddit_client_secret'),\n",
        "    \"REDDIT_USER_AGENT\": \"windows:FINANCIAL_SENTIMENT:1.0 (by u/FinancialAnalyst)\",\n",
        "    \"YOUTUBE_API_KEY\": get_api_key('YOUTUBE_API_KEY'),\n",
        "    \"TWITTER_BEARER_TOKEN\": get_api_key('twitter_api'),\n",
        "    \"ALPHA_VANTAGE_API_KEY\": get_api_key('alpha_vantage'),\n",
        "    \"NEWS_API_KEY\": get_api_key('news_api'),\n",
        "}\n",
        "\n",
        "class FinancialFinBERTAnalyzer:\n",
        "    \"\"\"Fine-tuned FinBERT sentiment analyzer trained on financial data\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"./finbert_financial_sentiment\"):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if HAS_TRANSFORMERS else None\n",
        "        self.label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "        self.training_accuracy = \"Unknown\"\n",
        "\n",
        "        print(f\"🚀 Initializing Financial Fine-tuned FinBERT model...\")\n",
        "        print(f\"📍 Model path: {model_path}\")\n",
        "        if self.device:\n",
        "            print(f\"🔧 Using device: {self.device}\")\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the fine-tuned financial sentiment model\"\"\"\n",
        "        if not HAS_TRANSFORMERS:\n",
        "            print(\"❌ transformers library not available - using fallback sentiment analysis\")\n",
        "            return\n",
        "\n",
        "        if not os.path.exists(self.model_path):\n",
        "            print(f\"❌ Model path not found: {self.model_path}\")\n",
        "            print(\"Please ensure your fine-tuned financial model is in the correct location.\")\n",
        "            print(\"Run the fine-tuning script first to create the model.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(\"📥 Loading tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "\n",
        "            print(\"📥 Loading fine-tuned financial model...\")\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.model_path,\n",
        "                num_labels=3  # negative, neutral, positive\n",
        "            )\n",
        "\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            # Load training info if available\n",
        "            info_path = os.path.join(self.model_path, 'training_info.json')\n",
        "            if os.path.exists(info_path):\n",
        "                try:\n",
        "                    with open(info_path, 'r') as f:\n",
        "                        training_info = json.load(f)\n",
        "                        self.training_accuracy = training_info.get('test_accuracy', 'Unknown')\n",
        "                        print(f\"📊 Model training accuracy: {self.training_accuracy}\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Could not load training info: {e}\")\n",
        "\n",
        "            print(\"✅ Financial Fine-tuned FinBERT model loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading fine-tuned model: {e}\")\n",
        "            print(\"Make sure your model files are correctly saved and accessible.\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def predict_sentiment(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment using fine-tuned financial model\"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return self._basic_sentiment_fallback(text)\n",
        "\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"neutral\", \"method\": \"empty_text\"}\n",
        "\n",
        "        try:\n",
        "            # Preprocess text\n",
        "            clean_text = self._preprocess_text(text)\n",
        "\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Make prediction\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "            # Get results\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = torch.max(predictions).item()\n",
        "\n",
        "            # Get all class probabilities\n",
        "            probs = predictions.cpu().numpy()[0]\n",
        "\n",
        "            # Convert to sentiment score (-1 to +1 scale)\n",
        "            if predicted_class == 2:  # positive\n",
        "                sentiment_score = probs[2] - probs[0]  # positive_prob - negative_prob\n",
        "            elif predicted_class == 0:  # negative\n",
        "                sentiment_score = -(probs[0] - probs[2])  # -(negative_prob - positive_prob)\n",
        "            else:  # neutral\n",
        "                sentiment_score = (probs[2] - probs[0]) * 0.5  # Muted sentiment for neutral\n",
        "\n",
        "            label = self.label_mapping[predicted_class]\n",
        "\n",
        "            return {\n",
        "                \"sentiment\": round(sentiment_score, 4),\n",
        "                \"confidence\": round(confidence, 4),\n",
        "                \"label\": label.upper(),\n",
        "                \"method\": \"fine_tuned_financial_finbert\",\n",
        "                \"predicted_class\": predicted_class,\n",
        "                \"class_probabilities\": {\n",
        "                    \"negative\": round(probs[0], 4),\n",
        "                    \"neutral\": round(probs[1], 4),\n",
        "                    \"positive\": round(probs[2], 4)\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Fine-tuned model prediction error: {e}\")\n",
        "            return self._basic_sentiment_fallback(text)\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text for financial analysis\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Preserve financial symbols and numbers\n",
        "        # Keep $ symbols, percentages, and stock tickers\n",
        "        text = re.sub(r'[^\\w\\s.,!?$%-]', '', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _basic_sentiment_fallback(self, text: str) -> Dict:\n",
        "        \"\"\"Enhanced fallback sentiment analysis for financial text\"\"\"\n",
        "        if not text:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"NEUTRAL\", \"method\": \"fallback_empty\"}\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Enhanced financial sentiment keywords\n",
        "        positive_words = [\n",
        "            'buy', 'bull', 'bullish', 'up', 'rise', 'rising', 'gain', 'gains',\n",
        "            'profit', 'profits', 'good', 'great', 'excellent', 'strong', 'growth',\n",
        "            'increase', 'positive', 'optimistic', 'confidence', 'outperform',\n",
        "            'beat', 'exceed', 'upgrade', 'recommend', 'target', 'momentum',\n",
        "            'rally', 'surge', 'boom', 'expansion', 'breakthrough', 'success',\n",
        "            'solid', 'robust', 'healthy', 'promising', 'favorable', 'dividend',\n",
        "            'earnings beat', 'revenue growth', 'market share', 'innovation'\n",
        "        ]\n",
        "\n",
        "        negative_words = [\n",
        "            'sell', 'bear', 'bearish', 'down', 'fall', 'falling', 'loss', 'losses',\n",
        "            'bad', 'terrible', 'weak', 'decline', 'decrease', 'negative',\n",
        "            'pessimistic', 'concern', 'underperform', 'miss', 'downgrade',\n",
        "            'avoid', 'risk', 'crash', 'drop', 'plunge', 'collapse',\n",
        "            'recession', 'crisis', 'warning', 'caution', 'debt', 'bankruptcy',\n",
        "            'lawsuit', 'fraud', 'scandal', 'volatility', 'uncertainty'\n",
        "        ]\n",
        "\n",
        "        neutral_words = [\n",
        "            'hold', 'neutral', 'stable', 'maintain', 'unchanged', 'steady',\n",
        "            'sideways', 'range', 'consolidation', 'wait', 'monitor'\n",
        "        ]\n",
        "\n",
        "        # Count occurrences\n",
        "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "        neutral_count = sum(1 for word in neutral_words if word in text_lower)\n",
        "\n",
        "        total_words = len(text.split())\n",
        "        if total_words == 0:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"NEUTRAL\", \"method\": \"fallback_empty\"}\n",
        "\n",
        "        # Calculate sentiment score\n",
        "        net_sentiment = positive_count - negative_count\n",
        "        sentiment_raw = net_sentiment / max(total_words, 1)\n",
        "        sentiment_score = max(-1.0, min(1.0, sentiment_raw * 3))  # Scale and clamp\n",
        "\n",
        "        # Adjust for neutral words\n",
        "        if neutral_count > 0:\n",
        "            sentiment_score *= (1 - (neutral_count / max(total_words, 1)) * 0.5)\n",
        "\n",
        "        # Determine label and confidence\n",
        "        if sentiment_score > 0.15:\n",
        "            label = \"POSITIVE\"\n",
        "            confidence = min(0.8, abs(sentiment_score) + 0.4)\n",
        "        elif sentiment_score < -0.15:\n",
        "            label = \"NEGATIVE\"\n",
        "            confidence = min(0.8, abs(sentiment_score) + 0.4)\n",
        "        else:\n",
        "            label = \"NEUTRAL\"\n",
        "            confidence = 0.6\n",
        "\n",
        "        return {\n",
        "            \"sentiment\": round(sentiment_score, 4),\n",
        "            \"confidence\": round(confidence, 4),\n",
        "            \"label\": label,\n",
        "            \"method\": \"enhanced_financial_fallback\"\n",
        "        }\n",
        "\n",
        "class FinancialDataCollector:\n",
        "    \"\"\"Enhanced data collector for financial sentiment analysis\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"./finbert_financial_sentiment\"):\n",
        "        self.sentiment_analyzer = FinancialFinBERTAnalyzer(model_path)\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "        # Initialize Reddit client\n",
        "        self.reddit_client = None\n",
        "        if HAS_REDDIT and API_KEYS[\"REDDIT_CLIENT_ID\"] and API_KEYS[\"REDDIT_CLIENT_SECRET\"]:\n",
        "            try:\n",
        "                self.reddit_client = praw.Reddit(\n",
        "                    client_id=API_KEYS[\"REDDIT_CLIENT_ID\"],\n",
        "                    client_secret=API_KEYS[\"REDDIT_CLIENT_SECRET\"],\n",
        "                    user_agent=API_KEYS[\"REDDIT_USER_AGENT\"]\n",
        "                )\n",
        "                print(\"✅ Reddit client initialized\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error initializing Reddit client: {e}\")\n",
        "                self.reddit_client = None\n",
        "\n",
        "    def _normalize_symbol(self, symbol: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract and normalize symbol information\"\"\"\n",
        "        symbol = symbol.upper().strip()\n",
        "\n",
        "        # Handle different exchange formats\n",
        "        if symbol.endswith('.NS') or symbol.endswith('.BO'):\n",
        "            base_symbol = symbol.split('.')[0]\n",
        "            exchange = 'NSE' if symbol.endswith('.NS') else 'BSE'\n",
        "        elif symbol.endswith('.L'):\n",
        "            base_symbol = symbol.split('.')[0]\n",
        "            exchange = 'LSE'\n",
        "        else:\n",
        "            base_symbol = symbol\n",
        "            exchange = 'US'\n",
        "\n",
        "        # Get company information\n",
        "        company_name = \"\"\n",
        "        company_sector = \"\"\n",
        "\n",
        "        if HAS_YFINANCE:\n",
        "            try:\n",
        "                ticker_obj = yf.Ticker(symbol)\n",
        "                info = ticker_obj.info\n",
        "\n",
        "                company_name = info.get('longName', '')\n",
        "                if company_name:\n",
        "                    company_name = re.sub(r'\\b(Ltd|Limited|Inc|Corp|Corporation|Company|Co)\\b\\.?', '', company_name, flags=re.IGNORECASE)\n",
        "                    company_name = company_name.strip()\n",
        "\n",
        "                company_sector = info.get('sector', '')\n",
        "\n",
        "                print(f\"   📋 Company: {company_name}\")\n",
        "                print(f\"   🏢 Sector: {company_sector}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Could not retrieve company info: {e}\")\n",
        "\n",
        "        return {\n",
        "            'original_symbol': symbol,\n",
        "            'base_symbol': base_symbol,\n",
        "            'exchange': exchange,\n",
        "            'company_name': company_name,\n",
        "            'company_sector': company_sector\n",
        "        }\n",
        "\n",
        "    def get_reddit_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get Reddit sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching Reddit data for {symbol}...\")\n",
        "\n",
        "        if not HAS_REDDIT or not self.reddit_client:\n",
        "            print(\"❌ Reddit API not available - using demo data\")\n",
        "            symbol_info = self._normalize_symbol(symbol)\n",
        "            demo_texts = [\n",
        "                f\"{symbol} quarterly results exceeded expectations, strong fundamentals\",\n",
        "                f\"Bullish on {symbol}, technical analysis shows upward momentum\",\n",
        "                f\"{symbol} dividend announcement positively received by market\",\n",
        "                f\"Concerned about {symbol} debt levels, might affect future growth\",\n",
        "                f\"{symbol} management guidance looks promising for next quarter\",\n",
        "                f\"Institutional buying in {symbol} suggests confidence\",\n",
        "                f\"{symbol} sector rotation benefiting the stock significantly\"\n",
        "            ]\n",
        "            return self._analyze_texts(demo_texts, \"Reddit (Demo)\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "        texts = []\n",
        "\n",
        "        # Define subreddits based on exchange\n",
        "        if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "            subreddits = ['IndiaInvestments', 'investing', 'stocks', 'SecurityAnalysis', 'IndianStreetBets']\n",
        "        else:\n",
        "            subreddits = ['investing', 'stocks', 'SecurityAnalysis', 'StockMarket', 'wallstreetbets']\n",
        "\n",
        "        # Search strategies\n",
        "        search_queries = [symbol_info['base_symbol']]\n",
        "        if symbol_info['company_name']:\n",
        "            search_queries.append(f'\"{symbol_info[\"company_name\"]}\"')\n",
        "\n",
        "        for query in search_queries[:2]:\n",
        "            for subreddit_name in subreddits:\n",
        "                if len(texts) >= 50:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    subreddit = self.reddit_client.subreddit(subreddit_name)\n",
        "\n",
        "                    for post in subreddit.search(query, limit=8, time_filter='month', sort='relevance'):\n",
        "                        if self._is_relevant_post(post, symbol_info):\n",
        "                            if post.title and len(post.title) > 20:\n",
        "                                texts.append(post.title)\n",
        "\n",
        "                            if hasattr(post, 'selftext') and post.selftext and len(post.selftext) > 30:\n",
        "                                texts.append(post.selftext[:600])\n",
        "\n",
        "                            # Get comments\n",
        "                            try:\n",
        "                                post.comments.replace_more(limit=0)\n",
        "                                for comment in post.comments[:3]:\n",
        "                                    if hasattr(comment, 'body') and len(comment.body) > 30:\n",
        "                                        if self._is_relevant_comment(comment.body, symbol_info):\n",
        "                                            texts.append(comment.body[:500])\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return self._analyze_texts(texts, \"Reddit\")\n",
        "\n",
        "    def get_news_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get news sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching News data for {symbol}...\")\n",
        "        texts = []\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        # Try NewsAPI\n",
        "        if API_KEYS[\"NEWS_API_KEY\"]:\n",
        "            try:\n",
        "                url = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "                search_terms = [symbol_info['base_symbol']]\n",
        "                if symbol_info['company_name']:\n",
        "                    search_terms.append(f'\"{symbol_info[\"company_name\"]}\"')\n",
        "\n",
        "                query = ' OR '.join(search_terms)\n",
        "                if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "                    query += ' AND (India OR NSE OR BSE OR stock)'\n",
        "                else:\n",
        "                    query += ' AND (stock OR financial OR earnings)'\n",
        "\n",
        "                params = {\n",
        "                    'q': query,\n",
        "                    'apiKey': API_KEYS[\"NEWS_API_KEY\"],\n",
        "                    'language': 'en',\n",
        "                    'sortBy': 'publishedAt',\n",
        "                    'pageSize': 30,\n",
        "                    'from': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "                }\n",
        "\n",
        "                response = self.session.get(url, params=params)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    for article in data.get('articles', []):\n",
        "                        title = article.get('title', '')\n",
        "                        description = article.get('description', '')\n",
        "                        content = article.get('content', '')\n",
        "\n",
        "                        full_text = f\"{title} {description} {content}\".strip()\n",
        "                        if len(full_text) > 30:\n",
        "                            texts.append(full_text[:800])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"NewsAPI error: {e}\")\n",
        "\n",
        "        # Fallback to demo data\n",
        "        if not texts:\n",
        "            company_name = symbol_info['company_name'] or symbol_info['base_symbol']\n",
        "            texts = [\n",
        "                f\"{company_name} reports strong quarterly earnings growth\",\n",
        "                f\"{symbol_info['base_symbol']} stock rallies on positive analyst coverage\",\n",
        "                f\"Institutional investors increase holdings in {company_name}\",\n",
        "                f\"Market volatility creates uncertainty for {symbol_info['base_symbol']} investors\",\n",
        "                f\"{company_name} management provides optimistic forward guidance\",\n",
        "                f\"Regulatory concerns weigh on {symbol_info['base_symbol']} performance\"\n",
        "            ]\n",
        "\n",
        "        return self._analyze_texts(texts, \"News\")\n",
        "\n",
        "    def get_youtube_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get YouTube sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching YouTube data for {symbol}...\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        if not API_KEYS[\"YOUTUBE_API_KEY\"]:\n",
        "            company_name = symbol_info['company_name'] or symbol_info['base_symbol']\n",
        "            demo_texts = [\n",
        "                f\"{symbol_info['base_symbol']} Technical Analysis - Bullish breakout pattern emerging\",\n",
        "                f\"Why {company_name} is my top pick for 2024 - Strong fundamentals\",\n",
        "                f\"{symbol_info['base_symbol']} Stock Analysis: Buy or Sell? Complete review\",\n",
        "                f\"Great analysis! I'm adding {symbol_info['base_symbol']} to my portfolio\",\n",
        "                f\"Disagree with the {symbol_info['base_symbol']} valuation, seems overpriced\",\n",
        "                f\"{company_name} has excellent growth prospects, solid investment\",\n",
        "                f\"Thanks for the detailed {symbol_info['base_symbol']} breakdown!\"\n",
        "            ]\n",
        "            return self._analyze_texts(demo_texts, \"YouTube (Demo)\")\n",
        "\n",
        "        texts = []\n",
        "        try:\n",
        "            search_queries = [\n",
        "                f'\"{symbol_info[\"base_symbol\"]}\" stock analysis',\n",
        "                f'{symbol_info[\"company_name\"]} investment' if symbol_info['company_name'] else None\n",
        "            ]\n",
        "\n",
        "            search_queries = [q for q in search_queries if q]\n",
        "\n",
        "            for query in search_queries[:2]:\n",
        "                search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "                search_params = {\n",
        "                    'part': 'snippet',\n",
        "                    'q': query,\n",
        "                    'type': 'video',\n",
        "                    'maxResults': 15,\n",
        "                    'key': API_KEYS[\"YOUTUBE_API_KEY\"],\n",
        "                    'order': 'relevance'\n",
        "                }\n",
        "\n",
        "                response = self.session.get(search_url, params=search_params)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for item in data.get('items', []):\n",
        "                        snippet = item.get('snippet', {})\n",
        "                        title = snippet.get('title', '')\n",
        "                        description = snippet.get('description', '')\n",
        "\n",
        "                        if title and len(title) > 20:\n",
        "                            texts.append(title)\n",
        "                        if description and len(description) > 30:\n",
        "                            texts.append(description[:400])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"YouTube API error: {e}\")\n",
        "\n",
        "        return self._analyze_texts(texts, \"YouTube\")\n",
        "\n",
        "    def get_twitter_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get Twitter sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching Twitter data for {symbol}...\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        if not API_KEYS[\"TWITTER_BEARER_TOKEN\"]:\n",
        "            demo_texts = [\n",
        "                f\"${symbol_info['base_symbol']} breaking resistance, bullish momentum building 📈\",\n",
        "                f\"Excellent earnings report from ${symbol_info['base_symbol']}, going long\",\n",
        "                f\"${symbol_info['base_symbol']} showing weakness, considering profit taking\",\n",
        "                f\"${symbol_info['base_symbol']} fundamentals remain strong despite market volatility\",\n",
        "                f\"Unusual volume in ${symbol_info['base_symbol']} suggests big move coming\",\n",
        "                f\"${symbol_info['base_symbol']} dividend yield attractive for income investors\",\n",
        "                f\"Love the ${symbol_info['base_symbol']} sector play, great positioning\"\n",
        "            ]\n",
        "            return self._analyze_texts(demo_texts, \"Twitter (Demo)\")\n",
        "\n",
        "        texts = []\n",
        "        try:\n",
        "            url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "            headers = {'Authorization': f'Bearer {API_KEYS[\"TWITTER_BEARER_TOKEN\"]}'}\n",
        "\n",
        "            search_terms = [f'${symbol_info[\"base_symbol\"]}', symbol_info['base_symbol']]\n",
        "            query = f'({\" OR \".join(search_terms)}) (stock OR investment OR earnings OR financial)'\n",
        "\n",
        "            if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "                query += ' (India OR NSE OR BSE)'\n",
        "\n",
        "            query += ' -is:retweet lang:en'\n",
        "\n",
        "            params = {\n",
        "                'query': query,\n",
        "                'max_results': 100,\n",
        "                'tweet.fields': 'public_metrics,created_at'\n",
        "            }\n",
        "\n",
        "            response = self.session.get(url, headers=headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for tweet in data.get('data', []):\n",
        "                    text = tweet.get('text', '')\n",
        "                    if text and len(text) > 25:\n",
        "                        texts.append(text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Twitter API error: {e}\")\n",
        "\n",
        "        return self._analyze_texts(texts, \"Twitter\")\n",
        "\n",
        "    def _is_relevant_post(self, post, symbol_info: Dict) -> bool:\n",
        "        \"\"\"Check if a Reddit post is relevant to the stock\"\"\"\n",
        "        title_lower = post.title.lower()\n",
        "        selftext_lower = getattr(post, 'selftext', '').lower()\n",
        "        full_text = f\"{title_lower} {selftext_lower}\"\n",
        "\n",
        "        # Check for symbol mentions\n",
        "        if symbol_info['base_symbol'].lower() in full_text:\n",
        "            return True\n",
        "\n",
        "        # Check for company name mentions\n",
        "        if symbol_info['company_name']:\n",
        "            company_words = symbol_info['company_name'].lower().split()\n",
        "            if len(company_words) >= 2:\n",
        "                matches = sum(1 for word in company_words if word in full_text and len(word) > 3)\n",
        "                if matches >= 2:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _is_relevant_comment(self, comment_text: str, symbol_info: Dict) -> bool:\n",
        "        \"\"\"Check if a comment is relevant to the stock\"\"\"\n",
        "        comment_lower = comment_text.lower()\n",
        "\n",
        "        if symbol_info['base_symbol'].lower() in comment_lower:\n",
        "            return True\n",
        "\n",
        "        if symbol_info['company_name']:\n",
        "            company_words = symbol_info['company_name'].lower().split()\n",
        "            for word in company_words:\n",
        "                if len(word) > 3 and word in comment_lower:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _analyze_texts(self, texts: List[str], source: str) -> Dict:\n",
        "        \"\"\"Analyze texts using fine-tuned financial FinBERT\"\"\"\n",
        "        if not texts:\n",
        "            return self._empty_result()\n",
        "\n",
        "        valid_texts = []\n",
        "        for text in texts:\n",
        "            if text and isinstance(text, str) and len(text.strip()) > 15:\n",
        "                valid_texts.append(text.strip())\n",
        "\n",
        "        if not valid_texts:\n",
        "            return self._empty_result()\n",
        "\n",
        "        # Analyze with fine-tuned model\n",
        "        results = []\n",
        "        sentiments = []\n",
        "        confidences = []\n",
        "\n",
        "        print(f\"   📊 Analyzing {len(valid_texts)} texts with fine-tuned Financial FinBERT...\")\n",
        "\n",
        "        for text in valid_texts:\n",
        "            try:\n",
        "                result = self.sentiment_analyzer.predict_sentiment(text)\n",
        "                results.append(result)\n",
        "                sentiments.append(result['sentiment'])\n",
        "                confidences.append(result['confidence'])\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Error analyzing text: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not sentiments:\n",
        "            return self._empty_result()\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_sentiment = np.mean(sentiments)\n",
        "        avg_confidence = np.mean(confidences)\n",
        "        sentiment_std = np.std(sentiments) if len(sentiments) > 1 else 0\n",
        "\n",
        "        # Determine overall label\n",
        "        if avg_sentiment > 0.2:\n",
        "            overall_label = \"POSITIVE\"\n",
        "        elif avg_sentiment < -0.2:\n",
        "            overall_label = \"NEGATIVE\"\n",
        "        else:\n",
        "            overall_label = \"NEUTRAL\"\n",
        "\n",
        "        return {\n",
        "            \"count\": len(valid_texts),\n",
        "            \"sentiment\": {\n",
        "                \"score\": round(avg_sentiment, 4),\n",
        "                \"confidence\": round(avg_confidence, 4),\n",
        "                \"std_dev\": round(sentiment_std, 4),\n",
        "                \"label\": overall_label\n",
        "            },\n",
        "            \"method\": \"fine_tuned_financial_finbert\",\n",
        "            \"distribution\": {\n",
        "                \"positive\": sum(1 for s in sentiments if s > 0.15),\n",
        "                \"negative\": sum(1 for s in sentiments if s < -0.15),\n",
        "                \"neutral\": sum(1 for s in sentiments if -0.15 <= s <= 0.15)\n",
        "            },\n",
        "            \"detailed_results\": results[:5]  # Store top 5 detailed results\n",
        "        }\n",
        "\n",
        "    def _empty_result(self) -> Dict:\n",
        "        \"\"\"Return empty result structure\"\"\"\n",
        "        return {\n",
        "            \"count\": 0,\n",
        "            \"sentiment\": {\n",
        "                \"score\": 0,\n",
        "                \"confidence\": 0,\n",
        "                \"std_dev\": 0,\n",
        "                \"label\": \"NEUTRAL\"\n",
        "            },\n",
        "            \"method\": \"no_data\",\n",
        "            \"distribution\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0},\n",
        "            \"detailed_results\": []\n",
        "        }\n",
        "\n",
        "    def get_market_data(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get current market data for the stock\"\"\"\n",
        "        print(f\"📈 Fetching market data for {symbol}...\")\n",
        "\n",
        "        if not HAS_YFINANCE:\n",
        "            print(\"❌ yfinance not available - using demo market data\")\n",
        "            return {\n",
        "                \"current_price\": 150.25,\n",
        "                \"change\": 2.35,\n",
        "                \"change_percent\": 1.59,\n",
        "                \"volume\": 1250000,\n",
        "                \"market_cap\": \"15.2B\",\n",
        "                \"pe_ratio\": 18.5,\n",
        "                \"beta\": 1.15,\n",
        "                \"52_week_high\": 180.50,\n",
        "                \"52_week_low\": 120.30,\n",
        "                \"status\": \"demo\"\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            info = ticker.info\n",
        "            hist = ticker.history(period=\"2d\")\n",
        "\n",
        "            if len(hist) < 1:\n",
        "                return {\"status\": \"no_data\"}\n",
        "\n",
        "            current_price = hist['Close'].iloc[-1] if len(hist) > 0 else 0\n",
        "            prev_close = hist['Close'].iloc[-2] if len(hist) > 1 else current_price\n",
        "\n",
        "            change = current_price - prev_close\n",
        "            change_percent = (change / prev_close * 100) if prev_close != 0 else 0\n",
        "\n",
        "            return {\n",
        "                \"current_price\": round(current_price, 2),\n",
        "                \"change\": round(change, 2),\n",
        "                \"change_percent\": round(change_percent, 2),\n",
        "                \"volume\": int(hist['Volume'].iloc[-1]) if len(hist) > 0 else 0,\n",
        "                \"market_cap\": info.get('marketCap', 'N/A'),\n",
        "                \"pe_ratio\": info.get('trailingPE', 'N/A'),\n",
        "                \"beta\": info.get('beta', 'N/A'),\n",
        "                \"52_week_high\": info.get('fiftyTwoWeekHigh', 'N/A'),\n",
        "                \"52_week_low\": info.get('fiftyTwoWeekLow', 'N/A'),\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error fetching market data: {e}\")\n",
        "            return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "class FinancialSentimentAnalyzer:\n",
        "    \"\"\"Main financial sentiment analysis system\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"./finbert_financial_sentiment\"):\n",
        "        self.data_collector = FinancialDataCollector(model_path)\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def _display_model_status(self):\n",
        "        \"\"\"Display current model status\"\"\"\n",
        "        print(f\"\\n🤖 MODEL STATUS:\")\n",
        "        if self.data_collector.sentiment_analyzer.model:\n",
        "            print(f\"   ✅ Fine-tuned Financial FinBERT: LOADED\")\n",
        "            print(f\"   📊 Training Accuracy: {self.data_collector.sentiment_analyzer.training_accuracy}\")\n",
        "            print(f\"   🔧 Device: {self.data_collector.sentiment_analyzer.device}\")\n",
        "        else:\n",
        "            print(f\"   ❌ Fine-tuned Financial FinBERT: NOT AVAILABLE\")\n",
        "            print(f\"   🔄 Using enhanced fallback sentiment analysis\")\n",
        "\n",
        "    def analyze_stock(self, symbol: str) -> Dict:\n",
        "        \"\"\"Comprehensive financial sentiment analysis\"\"\"\n",
        "        print(f\"🚀 FINANCIAL SENTIMENT ANALYSIS - FINE-TUNED FINBERT\")\n",
        "        print(f\"📊 Analyzing: {symbol}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        self._display_model_status()\n",
        "\n",
        "        # Data sources\n",
        "        sources = {\n",
        "            \"Reddit\": self.data_collector.get_reddit_sentiment,\n",
        "            \"News\": self.data_collector.get_news_sentiment,\n",
        "            \"YouTube\": self.data_collector.get_youtube_sentiment,\n",
        "            \"Twitter\": self.data_collector.get_twitter_sentiment,\n",
        "        }\n",
        "\n",
        "        sources_data = {}\n",
        "        all_scores = []\n",
        "        all_confidences = []\n",
        "        total_samples = 0\n",
        "\n",
        "        # Analyze each source\n",
        "        for source_name, source_func in sources.items():\n",
        "            try:\n",
        "                print(f\"\\n⏳ Processing {source_name}...\")\n",
        "                data = source_func(symbol)\n",
        "                sources_data[source_name] = data\n",
        "\n",
        "                if data[\"count\"] > 0:\n",
        "                    all_scores.append(data[\"sentiment\"][\"score\"])\n",
        "                    all_confidences.append(data[\"sentiment\"][\"confidence\"])\n",
        "                    total_samples += data[\"count\"]\n",
        "\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error with {source_name}: {e}\")\n",
        "                sources_data[source_name] = self.data_collector._empty_result()\n",
        "\n",
        "        # Get market data\n",
        "        print(f\"\\n📈 Fetching market data for {symbol}...\")\n",
        "        market_data = self.data_collector.get_market_data(symbol)\n",
        "\n",
        "        # Calculate overall sentiment\n",
        "        if all_scores:\n",
        "            overall_sentiment = np.mean(all_scores)\n",
        "            overall_confidence = np.mean(all_confidences)\n",
        "            sentiment_std = np.std(all_scores)\n",
        "\n",
        "            # Weight by sample size\n",
        "            weighted_scores = []\n",
        "            total_weight = 0\n",
        "            for source_name, data in sources_data.items():\n",
        "                if data[\"count\"] > 0:\n",
        "                    weight = data[\"count\"]\n",
        "                    weighted_scores.extend([data[\"sentiment\"][\"score\"]] * weight)\n",
        "                    total_weight += weight\n",
        "\n",
        "            weighted_sentiment = np.mean(weighted_scores) if weighted_scores else 0\n",
        "\n",
        "            # Determine overall label\n",
        "            if weighted_sentiment > 0.2:\n",
        "                overall_label = \"POSITIVE\"\n",
        "                emoji = \"📈\"\n",
        "            elif weighted_sentiment < -0.2:\n",
        "                overall_label = \"NEGATIVE\"\n",
        "                emoji = \"📉\"\n",
        "            else:\n",
        "                overall_label = \"NEUTRAL\"\n",
        "                emoji = \"➡️\"\n",
        "        else:\n",
        "            overall_sentiment = 0\n",
        "            overall_confidence = 0\n",
        "            sentiment_std = 0\n",
        "            weighted_sentiment = 0\n",
        "            overall_label = \"NEUTRAL\"\n",
        "            emoji = \"❓\"\n",
        "\n",
        "        # Compile results\n",
        "        results = {\n",
        "            \"symbol\": symbol,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"model_info\": {\n",
        "                \"type\": \"fine_tuned_financial_finbert\",\n",
        "                \"path\": self.model_path,\n",
        "                \"training_accuracy\": self.data_collector.sentiment_analyzer.training_accuracy,\n",
        "                \"device\": str(self.data_collector.sentiment_analyzer.device) if self.data_collector.sentiment_analyzer.device else \"CPU\"\n",
        "            },\n",
        "            \"overall_sentiment\": {\n",
        "                \"score\": round(weighted_sentiment, 4),\n",
        "                \"confidence\": round(overall_confidence, 4),\n",
        "                \"label\": overall_label,\n",
        "                \"emoji\": emoji,\n",
        "                \"std_dev\": round(sentiment_std, 4)\n",
        "            },\n",
        "            \"market_data\": market_data,\n",
        "            \"sources\": sources_data,\n",
        "            \"summary\": {\n",
        "                \"total_samples\": total_samples,\n",
        "                \"sources_analyzed\": len([s for s in sources_data.values() if s[\"count\"] > 0]),\n",
        "                \"analysis_method\": \"fine_tuned_financial_finbert\" if self.data_collector.sentiment_analyzer.model else \"enhanced_fallback\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self._display_results(results)\n",
        "        return results\n",
        "\n",
        "    def _display_results(self, results: Dict):\n",
        "        \"\"\"Display formatted analysis results\"\"\"\n",
        "        print(f\"\\n🎯 ANALYSIS RESULTS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Overall sentiment\n",
        "        overall = results[\"overall_sentiment\"]\n",
        "        print(f\"📊 OVERALL SENTIMENT: {overall['emoji']} {overall['label']}\")\n",
        "        print(f\"   Score: {overall['score']:.4f} (Range: -1.0 to +1.0)\")\n",
        "        print(f\"   Confidence: {overall['confidence']:.2%}\")\n",
        "        print(f\"   Standard Deviation: {overall['std_dev']:.4f}\")\n",
        "\n",
        "        # Market data\n",
        "        if results[\"market_data\"].get(\"status\") == \"success\":\n",
        "            market = results[\"market_data\"]\n",
        "            change_emoji = \"📈\" if market[\"change\"] > 0 else \"📉\" if market[\"change\"] < 0 else \"➡️\"\n",
        "            print(f\"\\n💰 MARKET DATA:\")\n",
        "            print(f\"   Price: ${market['current_price']:.2f} {change_emoji}\")\n",
        "            print(f\"   Change: {market['change']:+.2f} ({market['change_percent']:+.2f}%)\")\n",
        "            print(f\"   Volume: {market['volume']:,}\")\n",
        "            if market['pe_ratio'] != 'N/A':\n",
        "                print(f\"   P/E Ratio: {market['pe_ratio']:.2f}\")\n",
        "\n",
        "        # Source breakdown\n",
        "        print(f\"\\n📱 SOURCE BREAKDOWN:\")\n",
        "        for source_name, data in results[\"sources\"].items():\n",
        "            if data[\"count\"] > 0:\n",
        "                sentiment = data[\"sentiment\"]\n",
        "                emoji = \"📈\" if sentiment[\"score\"] > 0.1 else \"📉\" if sentiment[\"score\"] < -0.1 else \"➡️\"\n",
        "                print(f\"   {source_name}: {emoji} {sentiment['label']} \"\n",
        "                      f\"({sentiment['score']:+.3f}, {data['count']} samples)\")\n",
        "            else:\n",
        "                print(f\"   {source_name}: ❌ No data\")\n",
        "\n",
        "        # Distribution\n",
        "        print(f\"\\n📈 SENTIMENT DISTRIBUTION:\")\n",
        "        total_classified = 0\n",
        "        for source_name, data in results[\"sources\"].items():\n",
        "            if data[\"count\"] > 0:\n",
        "                dist = data[\"distribution\"]\n",
        "                total_classified += sum(dist.values())\n",
        "\n",
        "        if total_classified > 0:\n",
        "            total_pos = sum(data[\"distribution\"][\"positive\"] for data in results[\"sources\"].values())\n",
        "            total_neg = sum(data[\"distribution\"][\"negative\"] for data in results[\"sources\"].values())\n",
        "            total_neu = sum(data[\"distribution\"][\"neutral\"] for data in results[\"sources\"].values())\n",
        "\n",
        "            print(f\"   Positive: {total_pos} ({total_pos/total_classified*100:.1f}%)\")\n",
        "            print(f\"   Negative: {total_neg} ({total_neg/total_classified*100:.1f}%)\")\n",
        "            print(f\"   Neutral:  {total_neu} ({total_neu/total_classified*100:.1f}%)\")\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n📋 SUMMARY:\")\n",
        "        print(f\"   Total Samples Analyzed: {results['summary']['total_samples']}\")\n",
        "        print(f\"   Active Sources: {results['summary']['sources_analyzed']}/4\")\n",
        "        print(f\"   Analysis Method: {results['summary']['analysis_method']}\")\n",
        "        print(f\"   Model Training Accuracy: {results['model_info']['training_accuracy']}\")\n",
        "\n",
        "        # Investment recommendation\n",
        "        self._display_recommendation(results)\n",
        "\n",
        "    def _display_recommendation(self, results: Dict):\n",
        "        \"\"\"Display investment recommendation based on sentiment analysis\"\"\"\n",
        "        print(f\"\\n💡 INVESTMENT INSIGHTS:\")\n",
        "\n",
        "        overall_score = results[\"overall_sentiment\"][\"score\"]\n",
        "        confidence = results[\"overall_sentiment\"][\"confidence\"]\n",
        "        total_samples = results[\"summary\"][\"total_samples\"]\n",
        "\n",
        "        # Generate recommendation\n",
        "        if total_samples < 10:\n",
        "            recommendation = \"⚠️ LIMITED DATA - Insufficient samples for reliable analysis\"\n",
        "            risk_level = \"HIGH\"\n",
        "        elif confidence < 0.6:\n",
        "            recommendation = \"⚠️ LOW CONFIDENCE - Mixed or unclear sentiment signals\"\n",
        "            risk_level = \"MEDIUM-HIGH\"\n",
        "        elif overall_score > 0.3 and confidence > 0.7:\n",
        "            recommendation = \"🟢 POSITIVE OUTLOOK - Strong bullish sentiment detected\"\n",
        "            risk_level = \"LOW-MEDIUM\"\n",
        "        elif overall_score < -0.3 and confidence > 0.7:\n",
        "            recommendation = \"🔴 NEGATIVE OUTLOOK - Strong bearish sentiment detected\"\n",
        "            risk_level = \"MEDIUM-HIGH\"\n",
        "        elif -0.1 <= overall_score <= 0.1:\n",
        "            recommendation = \"🟡 NEUTRAL STANCE - Mixed sentiment, wait for clearer signals\"\n",
        "            risk_level = \"MEDIUM\"\n",
        "        elif overall_score > 0.1:\n",
        "            recommendation = \"🟢 CAUTIOUSLY POSITIVE - Moderate bullish sentiment\"\n",
        "            risk_level = \"MEDIUM\"\n",
        "        else:\n",
        "            recommendation = \"🟡 CAUTIOUSLY NEGATIVE - Moderate bearish sentiment\"\n",
        "            risk_level = \"MEDIUM\"\n",
        "\n",
        "        print(f\"   {recommendation}\")\n",
        "        print(f\"   Risk Level: {risk_level}\")\n",
        "\n",
        "        # Additional insights\n",
        "        if results[\"market_data\"].get(\"status\") == \"success\":\n",
        "            market = results[\"market_data\"]\n",
        "            if market[\"change_percent\"] > 5:\n",
        "                print(f\"   📈 Strong positive price movement ({market['change_percent']:+.2f}%)\")\n",
        "            elif market[\"change_percent\"] < -5:\n",
        "                print(f\"   📉 Strong negative price movement ({market['change_percent']:+.2f}%)\")\n",
        "\n",
        "        print(f\"\\n⚠️  DISCLAIMER: This analysis is for informational purposes only.\")\n",
        "        print(f\"   Always conduct your own research and consult financial advisors.\")\n",
        "\n",
        "    def batch_analyze(self, symbols: List[str]) -> Dict:\n",
        "        \"\"\"Analyze multiple stocks and compare sentiment\"\"\"\n",
        "        print(f\"🔄 BATCH ANALYSIS - {len(symbols)} SYMBOLS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        batch_results = {}\n",
        "        sentiment_summary = []\n",
        "\n",
        "        for i, symbol in enumerate(symbols, 1):\n",
        "            print(f\"\\n📊 [{i}/{len(symbols)}] Analyzing {symbol}...\")\n",
        "            try:\n",
        "                result = self.analyze_stock(symbol)\n",
        "                batch_results[symbol] = result\n",
        "\n",
        "                sentiment_summary.append({\n",
        "                    \"symbol\": symbol,\n",
        "                    \"sentiment_score\": result[\"overall_sentiment\"][\"score\"],\n",
        "                    \"label\": result[\"overall_sentiment\"][\"label\"],\n",
        "                    \"confidence\": result[\"overall_sentiment\"][\"confidence\"],\n",
        "                    \"total_samples\": result[\"summary\"][\"total_samples\"]\n",
        "                })\n",
        "\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error analyzing {symbol}: {e}\")\n",
        "                batch_results[symbol] = {\"error\": str(e)}\n",
        "\n",
        "        # Sort by sentiment score\n",
        "        sentiment_summary.sort(key=lambda x: x[\"sentiment_score\"], reverse=True)\n",
        "\n",
        "        print(f\"\\n🏆 BATCH RESULTS RANKING:\")\n",
        "        print(\"=\" * 70)\n",
        "        for i, item in enumerate(sentiment_summary, 1):\n",
        "            emoji = \"📈\" if item[\"sentiment_score\"] > 0.1 else \"📉\" if item[\"sentiment_score\"] < -0.1 else \"➡️\"\n",
        "            print(f\"{i:2d}. {item['symbol']:8s} {emoji} {item['label']:8s} \"\n",
        "                  f\"({item['sentiment_score']:+.3f}, {item['total_samples']} samples)\")\n",
        "\n",
        "        return {\n",
        "            \"batch_results\": batch_results,\n",
        "            \"ranking\": sentiment_summary,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with examples\"\"\"\n",
        "    print(\"🚀 FINANCIAL SENTIMENT ANALYSIS SYSTEM\")\n",
        "    print(\"🤖 Powered by Fine-tuned FinBERT for Financial Data\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = FinancialSentimentAnalyzer()\n",
        "\n",
        "    # Example usage\n",
        "    print(\"\\n📋 EXAMPLE USAGE:\")\n",
        "    print(\"1. Single stock analysis:\")\n",
        "    print(\"   analyzer.analyze_stock('AAPL')\")\n",
        "    print(\"   analyzer.analyze_stock('RELIANCE.NS')  # Indian stock\")\n",
        "    print(\"\\n2. Batch analysis:\")\n",
        "    print(\"   analyzer.batch_analyze(['AAPL', 'GOOGL', 'MSFT'])\")\n",
        "    print(\"\\n3. Custom analysis:\")\n",
        "    print(\"   result = analyzer.analyze_stock('TSLA')\")\n",
        "    print(\"   print(result['overall_sentiment'])\")\n",
        "\n",
        "    # Interactive mode\n",
        "    while True:\n",
        "        print(f\"\\n\" + \"=\" * 70)\n",
        "        print(\"🔍 INTERACTIVE ANALYSIS MODE\")\n",
        "        choice = input(\"\\n1. Single Stock Analysis\\n2. Batch Analysis\\n3. Exit\\nEnter choice (1-3): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            symbol = input(\"Enter stock symbol (e.g., AAPL, RELIANCE.NS): \").strip().upper()\n",
        "            if symbol:\n",
        "                try:\n",
        "                    analyzer.analyze_stock(symbol)\n",
        "                except KeyboardInterrupt:\n",
        "                    print(\"\\n⏹️ Analysis interrupted by user\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error: {e}\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            symbols_input = input(\"Enter stock symbols separated by commas (e.g., AAPL,GOOGL,MSFT): \").strip()\n",
        "            if symbols_input:\n",
        "                symbols = [s.strip().upper() for s in symbols_input.split(',') if s.strip()]\n",
        "                if symbols:\n",
        "                    try:\n",
        "                        analyzer.batch_analyze(symbols)\n",
        "                    except KeyboardInterrupt:\n",
        "                        print(\"\\n⏹️ Batch analysis interrupted by user\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ Error: {e}\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"👋 Thank you for using Financial Sentiment Analysis!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "# Advanced usage examples\n",
        "def advanced_examples():\n",
        "    \"\"\"Advanced usage examples and features\"\"\"\n",
        "    analyzer = FinancialSentimentAnalyzer()\n",
        "\n",
        "    print(\"🔬 ADVANCED USAGE EXAMPLES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Example 1: Tech stocks comparison\n",
        "    print(\"\\n📱 Tech Stocks Sentiment Comparison:\")\n",
        "    tech_stocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']\n",
        "    tech_results = analyzer.batch_analyze(tech_stocks)\n",
        "\n",
        "    # Example 2: Sector analysis\n",
        "    print(\"\\n🏭 Sector Analysis Example:\")\n",
        "    financial_stocks = ['JPM', 'BAC', 'WFC', 'C']\n",
        "    financial_results = analyzer.batch_analyze(financial_stocks)\n",
        "\n",
        "    # Example 3: Indian stocks\n",
        "    print(\"\\n🇮🇳 Indian Stocks Analysis:\")\n",
        "    indian_stocks = ['RELIANCE.NS', 'TCS.NS', 'INFY.NS', 'HDFCBANK.NS']\n",
        "    indian_results = analyzer.batch_analyze(indian_stocks)\n",
        "\n",
        "    return {\n",
        "        \"tech_analysis\": tech_results,\n",
        "        \"financial_analysis\": financial_results,\n",
        "        \"indian_analysis\": indian_results\n",
        "    }\n",
        "\n",
        "def export_results(results: Dict, filename: str = None):\n",
        "    \"\"\"Export analysis results to JSON file\"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"sentiment_analysis_{timestamp}.json\"\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "        print(f\"📁 Results exported to: {filename}\")\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error exporting results: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_results(filename: str) -> Optional[Dict]:\n",
        "    \"\"\"Load previously saved analysis results\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        print(f\"📁 Results loaded from: {filename}\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading results: {e}\")\n",
        "        return None\n",
        "\n",
        "# Utility functions\n",
        "def get_trending_stocks() -> List[str]:\n",
        "    \"\"\"Get list of trending stocks (demo implementation)\"\"\"\n",
        "    # In a real implementation, this could fetch from:\n",
        "    # - Yahoo Finance trending\n",
        "    # - Reddit WallStreetBets mentions\n",
        "    # - Google Trends for stocks\n",
        "    # - Financial news APIs\n",
        "\n",
        "    trending = [\n",
        "        'AAPL', 'TSLA', 'GOOGL', 'MSFT', 'AMZN', 'NVDA', 'META', 'NFLX',\n",
        "        'AMD', 'BABA', 'PLTR', 'GME', 'AMC', 'SPCE', 'COIN', 'HOOD'\n",
        "    ]\n",
        "\n",
        "    print(f\"📈 Sample trending stocks: {', '.join(trending[:8])}...\")\n",
        "    return trending\n",
        "\n",
        "def sentiment_to_trading_signal(sentiment_score: float, confidence: float) -> str:\n",
        "    \"\"\"Convert sentiment analysis to trading signal\"\"\"\n",
        "    if confidence < 0.6:\n",
        "        return \"HOLD - Low confidence\"\n",
        "\n",
        "    if sentiment_score > 0.4:\n",
        "        return \"STRONG BUY\"\n",
        "    elif sentiment_score > 0.2:\n",
        "        return \"BUY\"\n",
        "    elif sentiment_score > 0.05:\n",
        "        return \"WEAK BUY\"\n",
        "    elif sentiment_score > -0.05:\n",
        "        return \"HOLD\"\n",
        "    elif sentiment_score > -0.2:\n",
        "        return \"WEAK SELL\"\n",
        "    elif sentiment_score > -0.4:\n",
        "        return \"SELL\"\n",
        "    else:\n",
        "        return \"STRONG SELL\"\n",
        "\n",
        "# Performance monitoring\n",
        "def benchmark_model_performance():\n",
        "    \"\"\"Benchmark the fine-tuned model performance\"\"\"\n",
        "    analyzer = FinancialSentimentAnalyzer()\n",
        "\n",
        "    # Test with known sentiment examples\n",
        "    test_cases = [\n",
        "        (\"AAPL quarterly earnings beat expectations significantly\", \"positive\"),\n",
        "        (\"Tesla stock crashes on disappointing delivery numbers\", \"negative\"),\n",
        "        (\"Microsoft maintains steady growth trajectory\", \"neutral\"),\n",
        "        (\"Amazon Prime Day sales surge drives revenue growth\", \"positive\"),\n",
        "        (\"Google faces regulatory challenges in Europe\", \"negative\")\n",
        "    ]\n",
        "\n",
        "    print(\"🧪 MODEL PERFORMANCE BENCHMARK\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    correct_predictions = 0\n",
        "    for text, expected in test_cases:\n",
        "        result = analyzer.data_collector.sentiment_analyzer.predict_sentiment(text)\n",
        "        predicted = result[\"label\"].lower()\n",
        "\n",
        "        is_correct = predicted == expected\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        emoji = \"✅\" if is_correct else \"❌\"\n",
        "        print(f\"{emoji} Expected: {expected:8s} | Predicted: {predicted:8s} | Score: {result['sentiment']:+.3f}\")\n",
        "\n",
        "    accuracy = correct_predictions / len(test_cases) * 100\n",
        "    print(f\"\\n📊 Benchmark Accuracy: {accuracy:.1f}% ({correct_predictions}/{len(test_cases)})\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Run main interactive mode\n",
        "        main()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n👋 Program terminated by user. Goodbye!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Unexpected error: {e}\")\n",
        "        print(\"Please check your configuration and try again.\")\n",
        "\n",
        "# Additional helper functions for advanced users\n",
        "class SentimentTracker:\n",
        "    \"\"\"Track sentiment changes over time\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"./finbert_financial_sentiment\"):\n",
        "        self.analyzer = FinancialSentimentAnalyzer(model_path)\n",
        "        self.history = {}\n",
        "\n",
        "    def track_symbol(self, symbol: str, interval_hours: int = 24):\n",
        "        \"\"\"Track sentiment for a symbol over time\"\"\"\n",
        "        if symbol not in self.history:\n",
        "            self.history[symbol] = []\n",
        "\n",
        "        result = self.analyzer.analyze_stock(symbol)\n",
        "\n",
        "        self.history[symbol].append({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"sentiment_score\": result[\"overall_sentiment\"][\"score\"],\n",
        "            \"confidence\": result[\"overall_sentiment\"][\"confidence\"],\n",
        "            \"total_samples\": result[\"summary\"][\"total_samples\"]\n",
        "        })\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_sentiment_trend(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get sentiment trend analysis for a symbol\"\"\"\n",
        "        if symbol not in self.history or len(self.history[symbol]) < 2:\n",
        "            return {\"status\": \"insufficient_data\"}\n",
        "\n",
        "        history = self.history[symbol]\n",
        "        latest = history[-1]\n",
        "        previous = history[-2]\n",
        "\n",
        "        sentiment_change = latest[\"sentiment_score\"] - previous[\"sentiment_score\"]\n",
        "\n",
        "        if sentiment_change > 0.1:\n",
        "            trend = \"IMPROVING\"\n",
        "            trend_emoji = \"📈\"\n",
        "        elif sentiment_change < -0.1:\n",
        "            trend = \"DECLINING\"\n",
        "            trend_emoji = \"📉\"\n",
        "        else:\n",
        "            trend = \"STABLE\"\n",
        "            trend_emoji = \"➡️\"\n",
        "\n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"trend\": trend,\n",
        "            \"trend_emoji\": trend_emoji,\n",
        "            \"sentiment_change\": round(sentiment_change, 4),\n",
        "            \"current_sentiment\": latest[\"sentiment_score\"],\n",
        "            \"current_confidence\": latest[\"confidence\"],\n",
        "            \"data_points\": len(history)\n",
        "        }\n",
        "\n",
        "# Usage examples for different scenarios\n",
        "def demo_usage():\n",
        "    \"\"\"Demonstrate various usage scenarios\"\"\"\n",
        "    print(\"🎬 DEMONSTRATION MODE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    analyzer = FinancialSentimentAnalyzer()\n",
        "\n",
        "    # Scenario 1: Pre-earnings analysis\n",
        "    print(\"\\n📊 Scenario 1: Pre-earnings sentiment check\")\n",
        "    result = analyzer.analyze_stock(\"AAPL\")\n",
        "\n",
        "    # Scenario 2: Portfolio sentiment overview\n",
        "    print(\"\\n💼 Scenario 2: Portfolio sentiment overview\")\n",
        "    portfolio = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
        "    batch_result = analyzer.batch_analyze(portfolio)\n",
        "\n",
        "    # Scenario 3: Sector comparison\n",
        "    print(\"\\n🏭 Scenario 3: Sector sentiment comparison\")\n",
        "    banking_stocks = ['JPM', 'BAC', 'WFC']\n",
        "    banking_result = analyzer.batch_analyze(banking_stocks)\n",
        "\n",
        "    return {\n",
        "        \"single_analysis\": result,\n",
        "        \"portfolio_analysis\": batch_result,\n",
        "        \"sector_analysis\": banking_result\n",
        "    }\n",
        "\n",
        "# Configuration and setup helper\n",
        "def setup_api_keys():\n",
        "    \"\"\"Guide users through API key setup\"\"\"\n",
        "    print(\"🔧 API KEY SETUP GUIDE\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"For full functionality, set up these optional API keys:\")\n",
        "    print(\"1. Reddit API: reddit_client_id, reddit_client_secret\")\n",
        "    print(\"2. News API: news_api\")\n",
        "    print(\"3. YouTube API: YOUTUBE_API_KEY\")\n",
        "    print(\"4. Twitter API: twitter_api\")\n",
        "    print(\"\\nSet as environment variables or in Google Colab secrets.\")\n",
        "    print(\"The system will work with demo data if APIs are not available.\")\n",
        "\n",
        "# Export the main classes for external use\n",
        "__all__ = [\n",
        "    'FinancialSentimentAnalyzer',\n",
        "    'FinancialFinBERTAnalyzer',\n",
        "    'FinancialDataCollector',\n",
        "    'SentimentTracker',\n",
        "    'export_results',\n",
        "    'load_results',\n",
        "    'sentiment_to_trading_signal',\n",
        "    'benchmark_model_performance'\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6vfEVlP5AC7",
        "outputId": "7c915b9d-1e3c-4583-bde2-43308591eaa4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 FINANCIAL SENTIMENT ANALYSIS SYSTEM\n",
            "🤖 Powered by Fine-tuned FinBERT for Financial Data\n",
            "======================================================================\n",
            "🚀 Initializing Financial Fine-tuned FinBERT model...\n",
            "📍 Model path: ./finbert_financial_sentiment\n",
            "🔧 Using device: cuda\n",
            "📥 Loading tokenizer...\n",
            "📥 Loading fine-tuned financial model...\n",
            "📊 Model training accuracy: 0.9436\n",
            "✅ Financial Fine-tuned FinBERT model loaded successfully!\n",
            "✅ Reddit client initialized\n",
            "\n",
            "📋 EXAMPLE USAGE:\n",
            "1. Single stock analysis:\n",
            "   analyzer.analyze_stock('AAPL')\n",
            "   analyzer.analyze_stock('RELIANCE.NS')  # Indian stock\n",
            "\n",
            "2. Batch analysis:\n",
            "   analyzer.batch_analyze(['AAPL', 'GOOGL', 'MSFT'])\n",
            "\n",
            "3. Custom analysis:\n",
            "   result = analyzer.analyze_stock('TSLA')\n",
            "   print(result['overall_sentiment'])\n",
            "\n",
            "======================================================================\n",
            "🔍 INTERACTIVE ANALYSIS MODE\n",
            "\n",
            "1. Single Stock Analysis\n",
            "2. Batch Analysis\n",
            "3. Exit\n",
            "Enter choice (1-3): HDFCBANK.NS,HINDALCO.NS,WIPRO.NS\n",
            "❌ Invalid choice. Please enter 1, 2, or 3.\n",
            "\n",
            "======================================================================\n",
            "🔍 INTERACTIVE ANALYSIS MODE\n",
            "\n",
            "1. Single Stock Analysis\n",
            "2. Batch Analysis\n",
            "3. Exit\n",
            "Enter choice (1-3): 2\n",
            "Enter stock symbols separated by commas (e.g., AAPL,GOOGL,MSFT): HDFCBANK.NS,HINDALCO.NS,WIPRO.NS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 BATCH ANALYSIS - 3 SYMBOLS\n",
            "======================================================================\n",
            "\n",
            "📊 [1/3] Analyzing HDFCBANK.NS...\n",
            "🚀 FINANCIAL SENTIMENT ANALYSIS - FINE-TUNED FINBERT\n",
            "📊 Analyzing: HDFCBANK.NS\n",
            "======================================================================\n",
            "\n",
            "🤖 MODEL STATUS:\n",
            "   ✅ Fine-tuned Financial FinBERT: LOADED\n",
            "   📊 Training Accuracy: 0.9436\n",
            "   🔧 Device: cuda\n",
            "\n",
            "⏳ Processing Reddit...\n",
            "🔍 Fetching Reddit data for HDFCBANK.NS...\n",
            "   📋 Company: HDFC Bank\n",
            "   🏢 Sector: Financial Services\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   📊 Analyzing 28 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing News...\n",
            "🔍 Fetching News data for HDFCBANK.NS...\n",
            "   📋 Company: HDFC Bank\n",
            "   🏢 Sector: Financial Services\n",
            "   📊 Analyzing 30 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing YouTube...\n",
            "🔍 Fetching YouTube data for HDFCBANK.NS...\n",
            "   📋 Company: HDFC Bank\n",
            "   🏢 Sector: Financial Services\n",
            "   📊 Analyzing 54 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing Twitter...\n",
            "🔍 Fetching Twitter data for HDFCBANK.NS...\n",
            "   📋 Company: HDFC Bank\n",
            "   🏢 Sector: Financial Services\n",
            "\n",
            "📈 Fetching market data for HDFCBANK.NS...\n",
            "📈 Fetching market data for HDFCBANK.NS...\n",
            "\n",
            "🎯 ANALYSIS RESULTS\n",
            "======================================================================\n",
            "📊 OVERALL SENTIMENT: ➡️ NEUTRAL\n",
            "   Score: 0.1252 (Range: -1.0 to +1.0)\n",
            "   Confidence: 90.77%\n",
            "   Standard Deviation: 0.0931\n",
            "\n",
            "💰 MARKET DATA:\n",
            "   Price: $2001.00 📉\n",
            "   Change: -11.20 (-0.56%)\n",
            "   Volume: 1,629,393\n",
            "   P/E Ratio: 22.86\n",
            "\n",
            "📱 SOURCE BREAKDOWN:\n",
            "   Reddit: 📈 NEUTRAL (+0.176, 28 samples)\n",
            "   News: 📈 POSITIVE (+0.252, 30 samples)\n",
            "   YouTube: ➡️ NEUTRAL (+0.028, 54 samples)\n",
            "   Twitter: ❌ No data\n",
            "\n",
            "📈 SENTIMENT DISTRIBUTION:\n",
            "   Positive: 23 (20.5%)\n",
            "   Negative: 5 (4.5%)\n",
            "   Neutral:  84 (75.0%)\n",
            "\n",
            "📋 SUMMARY:\n",
            "   Total Samples Analyzed: 112\n",
            "   Active Sources: 3/4\n",
            "   Analysis Method: fine_tuned_financial_finbert\n",
            "   Model Training Accuracy: 0.9436\n",
            "\n",
            "💡 INVESTMENT INSIGHTS:\n",
            "   🟢 CAUTIOUSLY POSITIVE - Moderate bullish sentiment\n",
            "   Risk Level: MEDIUM\n",
            "\n",
            "⚠️  DISCLAIMER: This analysis is for informational purposes only.\n",
            "   Always conduct your own research and consult financial advisors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 [2/3] Analyzing HINDALCO.NS...\n",
            "🚀 FINANCIAL SENTIMENT ANALYSIS - FINE-TUNED FINBERT\n",
            "📊 Analyzing: HINDALCO.NS\n",
            "======================================================================\n",
            "\n",
            "🤖 MODEL STATUS:\n",
            "   ✅ Fine-tuned Financial FinBERT: LOADED\n",
            "   📊 Training Accuracy: 0.9436\n",
            "   🔧 Device: cuda\n",
            "\n",
            "⏳ Processing Reddit...\n",
            "🔍 Fetching Reddit data for HINDALCO.NS...\n",
            "   📋 Company: Hindalco Industries\n",
            "   🏢 Sector: Basic Materials\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ Processing News...\n",
            "🔍 Fetching News data for HINDALCO.NS...\n",
            "   📋 Company: Hindalco Industries\n",
            "   🏢 Sector: Basic Materials\n",
            "   📊 Analyzing 30 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing YouTube...\n",
            "🔍 Fetching YouTube data for HINDALCO.NS...\n",
            "   📋 Company: Hindalco Industries\n",
            "   🏢 Sector: Basic Materials\n",
            "   📊 Analyzing 54 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing Twitter...\n",
            "🔍 Fetching Twitter data for HINDALCO.NS...\n",
            "   📋 Company: Hindalco Industries\n",
            "   🏢 Sector: Basic Materials\n",
            "\n",
            "📈 Fetching market data for HINDALCO.NS...\n",
            "📈 Fetching market data for HINDALCO.NS...\n",
            "\n",
            "🎯 ANALYSIS RESULTS\n",
            "======================================================================\n",
            "📊 OVERALL SENTIMENT: ➡️ NEUTRAL\n",
            "   Score: -0.1106 (Range: -1.0 to +1.0)\n",
            "   Confidence: 93.44%\n",
            "   Standard Deviation: 0.2516\n",
            "\n",
            "💰 MARKET DATA:\n",
            "   Price: $685.15 📈\n",
            "   Change: +12.70 (+1.89%)\n",
            "   Volume: 729,201\n",
            "   P/E Ratio: 9.53\n",
            "\n",
            "📱 SOURCE BREAKDOWN:\n",
            "   Reddit: ❌ No data\n",
            "   News: 📉 NEGATIVE (-0.434, 30 samples)\n",
            "   YouTube: ➡️ NEUTRAL (+0.069, 54 samples)\n",
            "   Twitter: ❌ No data\n",
            "\n",
            "📈 SENTIMENT DISTRIBUTION:\n",
            "   Positive: 10 (11.9%)\n",
            "   Negative: 23 (27.4%)\n",
            "   Neutral:  51 (60.7%)\n",
            "\n",
            "📋 SUMMARY:\n",
            "   Total Samples Analyzed: 84\n",
            "   Active Sources: 2/4\n",
            "   Analysis Method: fine_tuned_financial_finbert\n",
            "   Model Training Accuracy: 0.9436\n",
            "\n",
            "💡 INVESTMENT INSIGHTS:\n",
            "   🟡 CAUTIOUSLY NEGATIVE - Moderate bearish sentiment\n",
            "   Risk Level: MEDIUM\n",
            "\n",
            "⚠️  DISCLAIMER: This analysis is for informational purposes only.\n",
            "   Always conduct your own research and consult financial advisors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 [3/3] Analyzing WIPRO.NS...\n",
            "🚀 FINANCIAL SENTIMENT ANALYSIS - FINE-TUNED FINBERT\n",
            "📊 Analyzing: WIPRO.NS\n",
            "======================================================================\n",
            "\n",
            "🤖 MODEL STATUS:\n",
            "   ✅ Fine-tuned Financial FinBERT: LOADED\n",
            "   📊 Training Accuracy: 0.9436\n",
            "   🔧 Device: cuda\n",
            "\n",
            "⏳ Processing Reddit...\n",
            "🔍 Fetching Reddit data for WIPRO.NS...\n",
            "   📋 Company: Wipro\n",
            "   🏢 Sector: Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   📊 Analyzing 14 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing News...\n",
            "🔍 Fetching News data for WIPRO.NS...\n",
            "   📋 Company: Wipro\n",
            "   🏢 Sector: Technology\n",
            "   📊 Analyzing 30 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing YouTube...\n",
            "🔍 Fetching YouTube data for WIPRO.NS...\n",
            "   📋 Company: Wipro\n",
            "   🏢 Sector: Technology\n",
            "   📊 Analyzing 56 texts with fine-tuned Financial FinBERT...\n",
            "\n",
            "⏳ Processing Twitter...\n",
            "🔍 Fetching Twitter data for WIPRO.NS...\n",
            "   📋 Company: Wipro\n",
            "   🏢 Sector: Technology\n",
            "\n",
            "📈 Fetching market data for WIPRO.NS...\n",
            "📈 Fetching market data for WIPRO.NS...\n",
            "\n",
            "🎯 ANALYSIS RESULTS\n",
            "======================================================================\n",
            "📊 OVERALL SENTIMENT: ➡️ NEUTRAL\n",
            "   Score: 0.0057 (Range: -1.0 to +1.0)\n",
            "   Confidence: 91.65%\n",
            "   Standard Deviation: 0.2499\n",
            "\n",
            "💰 MARKET DATA:\n",
            "   Price: $243.74 📈\n",
            "   Change: +0.99 (+0.41%)\n",
            "   Volume: 1,573,542\n",
            "   P/E Ratio: 19.01\n",
            "\n",
            "📱 SOURCE BREAKDOWN:\n",
            "   Reddit: 📉 NEGATIVE (-0.391, 14 samples)\n",
            "   News: 📈 POSITIVE (+0.214, 30 samples)\n",
            "   YouTube: ➡️ NEUTRAL (-0.006, 56 samples)\n",
            "   Twitter: ❌ No data\n",
            "\n",
            "📈 SENTIMENT DISTRIBUTION:\n",
            "   Positive: 13 (13.0%)\n",
            "   Negative: 13 (13.0%)\n",
            "   Neutral:  74 (74.0%)\n",
            "\n",
            "📋 SUMMARY:\n",
            "   Total Samples Analyzed: 100\n",
            "   Active Sources: 3/4\n",
            "   Analysis Method: fine_tuned_financial_finbert\n",
            "   Model Training Accuracy: 0.9436\n",
            "\n",
            "💡 INVESTMENT INSIGHTS:\n",
            "   🟡 NEUTRAL STANCE - Mixed sentiment, wait for clearer signals\n",
            "   Risk Level: MEDIUM\n",
            "\n",
            "⚠️  DISCLAIMER: This analysis is for informational purposes only.\n",
            "   Always conduct your own research and consult financial advisors.\n",
            "\n",
            "🏆 BATCH RESULTS RANKING:\n",
            "======================================================================\n",
            " 1. HDFCBANK.NS 📈 NEUTRAL  (+0.125, 112 samples)\n",
            " 2. WIPRO.NS ➡️ NEUTRAL  (+0.006, 100 samples)\n",
            " 3. HINDALCO.NS 📉 NEUTRAL  (-0.111, 84 samples)\n",
            "\n",
            "======================================================================\n",
            "🔍 INTERACTIVE ANALYSIS MODE\n",
            "\n",
            "\n",
            "👋 Program terminated by user. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# financial_sentiment_analysis_enhanced.py - Complete Sentiment Analysis with Fine-tuned FinBERT and Headlines\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Try to import optional libraries\n",
        "try:\n",
        "    import feedparser\n",
        "    HAS_FEEDPARSER = True\n",
        "except ImportError:\n",
        "    HAS_FEEDPARSER = False\n",
        "    print(\"⚠️ feedparser not installed. GNews and RSS feeds will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    import praw\n",
        "    HAS_REDDIT = True\n",
        "except ImportError:\n",
        "    HAS_REDDIT = False\n",
        "    print(\"⚠️ praw not installed. Reddit data will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    import torch\n",
        "    HAS_TRANSFORMERS = True\n",
        "except ImportError:\n",
        "    HAS_TRANSFORMERS = False\n",
        "    print(\"⚠️ transformers not installed. Fine-tuned FinBERT will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    import yfinance as yf\n",
        "    import pandas as pd\n",
        "    HAS_YFINANCE = True\n",
        "except ImportError:\n",
        "    HAS_YFINANCE = False\n",
        "    print(\"⚠️ yfinance not installed. Stock market data snapshot will be unavailable.\")\n",
        "\n",
        "# API Configuration - Using environment variables or None\n",
        "def get_api_key(key_name: str) -> Optional[str]:\n",
        "    \"\"\"Safely get API keys from environment or return None\"\"\"\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(key_name)\n",
        "    except:\n",
        "        return os.environ.get(key_name)\n",
        "\n",
        "API_KEYS = {\n",
        "    \"REDDIT_CLIENT_ID\": get_api_key('reddit_client_id'),\n",
        "    \"REDDIT_CLIENT_SECRET\": get_api_key('reddit_client_secret'),\n",
        "    \"REDDIT_USER_AGENT\": \"windows:FINANCIAL_SENTIMENT:1.0 (by u/FinancialAnalyst)\",\n",
        "    \"YOUTUBE_API_KEY\": get_api_key('YOUTUBE_API_KEY'),\n",
        "    \"TWITTER_BEARER_TOKEN\": get_api_key('twitter_api'),\n",
        "    \"ALPHA_VANTAGE_API_KEY\": get_api_key('alpha_vantage'),\n",
        "    \"NEWS_API_KEY\": get_api_key('news_api'),\n",
        "}\n",
        "\n",
        "class FinancialFinBERTAnalyzer:\n",
        "    \"\"\"Fine-tuned FinBERT sentiment analyzer trained on financial data\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"./finbert_financial_sentiment\"):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if HAS_TRANSFORMERS else None\n",
        "        self.label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "        self.training_accuracy = \"Unknown\"\n",
        "\n",
        "        print(f\"🚀 Initializing Financial Fine-tuned FinBERT model...\")\n",
        "        print(f\"📍 Model path: {model_path}\")\n",
        "        if self.device:\n",
        "            print(f\"🔧 Using device: {self.device}\")\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the fine-tuned financial sentiment model\"\"\"\n",
        "        if not HAS_TRANSFORMERS:\n",
        "            print(\"❌ transformers library not available - using fallback sentiment analysis\")\n",
        "            return\n",
        "\n",
        "        if not os.path.exists(self.model_path):\n",
        "            print(f\"❌ Model path not found: {self.model_path}\")\n",
        "            print(\"Please ensure your fine-tuned financial model is in the correct location.\")\n",
        "            print(\"Run the fine-tuning script first to create the model.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(\"📥 Loading tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "\n",
        "            print(\"📥 Loading fine-tuned financial model...\")\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.model_path,\n",
        "                num_labels=3  # negative, neutral, positive\n",
        "            )\n",
        "\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            # Load training info if available\n",
        "            info_path = os.path.join(self.model_path, 'training_info.json')\n",
        "            if os.path.exists(info_path):\n",
        "                try:\n",
        "                    with open(info_path, 'r') as f:\n",
        "                        training_info = json.load(f)\n",
        "                        self.training_accuracy = training_info.get('test_accuracy', 'Unknown')\n",
        "                        print(f\"📊 Model training accuracy: {self.training_accuracy}\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Could not load training info: {e}\")\n",
        "\n",
        "            print(\"✅ Financial Fine-tuned FinBERT model loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading fine-tuned model: {e}\")\n",
        "            print(\"Make sure your model files are correctly saved and accessible.\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def predict_sentiment_with_text(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment using fine-tuned financial model and return original text\"\"\"\n",
        "        # Get the sentiment prediction\n",
        "        sentiment_result = self.predict_sentiment(text)\n",
        "\n",
        "        # Add the original text and a truncated version for display\n",
        "        sentiment_result[\"original_text\"] = text\n",
        "        sentiment_result[\"display_text\"] = self._truncate_text(text, 150)\n",
        "\n",
        "        return sentiment_result\n",
        "\n",
        "    def predict_sentiment(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment using fine-tuned financial model\"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return self._basic_sentiment_fallback(text)\n",
        "\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"neutral\", \"method\": \"empty_text\"}\n",
        "\n",
        "        try:\n",
        "            # Preprocess text\n",
        "            clean_text = self._preprocess_text(text)\n",
        "\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Make prediction\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "            # Get results\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = torch.max(predictions).item()\n",
        "\n",
        "            # Get all class probabilities\n",
        "            probs = predictions.cpu().numpy()[0]\n",
        "\n",
        "            # Convert to sentiment score (-1 to +1 scale)\n",
        "            if predicted_class == 2:  # positive\n",
        "                sentiment_score = probs[2] - probs[0]  # positive_prob - negative_prob\n",
        "            elif predicted_class == 0:  # negative\n",
        "                sentiment_score = -(probs[0] - probs[2])  # -(negative_prob - positive_prob)\n",
        "            else:  # neutral\n",
        "                sentiment_score = (probs[2] - probs[0]) * 0.5  # Muted sentiment for neutral\n",
        "\n",
        "            label = self.label_mapping[predicted_class]\n",
        "\n",
        "            return {\n",
        "                \"sentiment\": round(sentiment_score, 4),\n",
        "                \"confidence\": round(confidence, 4),\n",
        "                \"label\": label.upper(),\n",
        "                \"method\": \"fine_tuned_financial_finbert\",\n",
        "                \"predicted_class\": predicted_class,\n",
        "                \"class_probabilities\": {\n",
        "                    \"negative\": round(probs[0], 4),\n",
        "                    \"neutral\": round(probs[1], 4),\n",
        "                    \"positive\": round(probs[2], 4)\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Fine-tuned model prediction error: {e}\")\n",
        "            return self._basic_sentiment_fallback(text)\n",
        "\n",
        "    def _truncate_text(self, text: str, max_length: int = 150) -> str:\n",
        "        \"\"\"Truncate text for display purposes\"\"\"\n",
        "        if len(text) <= max_length:\n",
        "            return text\n",
        "        return text[:max_length] + \"...\"\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text for financial analysis\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Preserve financial symbols and numbers\n",
        "        # Keep $ symbols, percentages, and stock tickers\n",
        "        text = re.sub(r'[^\\w\\s.,!?$%-]', '', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _basic_sentiment_fallback(self, text: str) -> Dict:\n",
        "        \"\"\"Enhanced fallback sentiment analysis for financial text\"\"\"\n",
        "        if not text:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"NEUTRAL\", \"method\": \"fallback_empty\"}\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Enhanced financial sentiment keywords\n",
        "        positive_words = [\n",
        "            'buy', 'bull', 'bullish', 'up', 'rise', 'rising', 'gain', 'gains',\n",
        "            'profit', 'profits', 'good', 'great', 'excellent', 'strong', 'growth',\n",
        "            'increase', 'positive', 'optimistic', 'confidence', 'outperform',\n",
        "            'beat', 'exceed', 'upgrade', 'recommend', 'target', 'momentum',\n",
        "            'rally', 'surge', 'boom', 'expansion', 'breakthrough', 'success',\n",
        "            'solid', 'robust', 'healthy', 'promising', 'favorable', 'dividend',\n",
        "            'earnings beat', 'revenue growth', 'market share', 'innovation'\n",
        "        ]\n",
        "\n",
        "        negative_words = [\n",
        "            'sell', 'bear', 'bearish', 'down', 'fall', 'falling', 'loss', 'losses',\n",
        "            'bad', 'terrible', 'weak', 'decline', 'decrease', 'negative',\n",
        "            'pessimistic', 'concern', 'underperform', 'miss', 'downgrade',\n",
        "            'avoid', 'risk', 'crash', 'drop', 'plunge', 'collapse',\n",
        "            'recession', 'crisis', 'warning', 'caution', 'debt', 'bankruptcy',\n",
        "            'lawsuit', 'fraud', 'scandal', 'volatility', 'uncertainty'\n",
        "        ]\n",
        "\n",
        "        neutral_words = [\n",
        "            'hold', 'neutral', 'stable', 'maintain', 'unchanged', 'steady',\n",
        "            'sideways', 'range', 'consolidation', 'wait', 'monitor'\n",
        "        ]\n",
        "\n",
        "        # Count occurrences\n",
        "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "        neutral_count = sum(1 for word in neutral_words if word in text_lower)\n",
        "\n",
        "        total_words = len(text.split())\n",
        "        if total_words == 0:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"NEUTRAL\", \"method\": \"fallback_empty\"}\n",
        "\n",
        "        # Calculate sentiment score\n",
        "        net_sentiment = positive_count - negative_count\n",
        "        sentiment_raw = net_sentiment / max(total_words, 1)\n",
        "        sentiment_score = max(-1.0, min(1.0, sentiment_raw * 3))  # Scale and clamp\n",
        "\n",
        "        # Adjust for neutral words\n",
        "        if neutral_count > 0:\n",
        "            sentiment_score *= (1 - (neutral_count / max(total_words, 1)) * 0.5)\n",
        "\n",
        "        # Determine label and confidence\n",
        "        if sentiment_score > 0.15:\n",
        "            label = \"POSITIVE\"\n",
        "            confidence = min(0.8, abs(sentiment_score) + 0.4)\n",
        "        elif sentiment_score < -0.15:\n",
        "            label = \"NEGATIVE\"\n",
        "            confidence = min(0.8, abs(sentiment_score) + 0.4)\n",
        "        else:\n",
        "            label = \"NEUTRAL\"\n",
        "            confidence = 0.6\n",
        "\n",
        "        return {\n",
        "            \"sentiment\": round(sentiment_score, 4),\n",
        "            \"confidence\": round(confidence, 4),\n",
        "            \"label\": label,\n",
        "            \"method\": \"enhanced_financial_fallback\"\n",
        "        }\n",
        "\n",
        "class FinancialDataCollector:\n",
        "    \"\"\"Enhanced data collector for financial sentiment analysis with headline tracking\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"./finbert_financial_sentiment\"):\n",
        "        self.sentiment_analyzer = FinancialFinBERTAnalyzer(model_path)\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "        # Initialize Reddit client\n",
        "        self.reddit_client = None\n",
        "        if HAS_REDDIT and API_KEYS[\"REDDIT_CLIENT_ID\"] and API_KEYS[\"REDDIT_CLIENT_SECRET\"]:\n",
        "            try:\n",
        "                self.reddit_client = praw.Reddit(\n",
        "                    client_id=API_KEYS[\"REDDIT_CLIENT_ID\"],\n",
        "                    client_secret=API_KEYS[\"REDDIT_CLIENT_SECRET\"],\n",
        "                    user_agent=API_KEYS[\"REDDIT_USER_AGENT\"]\n",
        "                )\n",
        "                print(\"✅ Reddit client initialized\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error initializing Reddit client: {e}\")\n",
        "                self.reddit_client = None\n",
        "\n",
        "    def _normalize_symbol(self, symbol: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract and normalize symbol information\"\"\"\n",
        "        symbol = symbol.upper().strip()\n",
        "\n",
        "        # Handle different exchange formats\n",
        "        if symbol.endswith('.NS') or symbol.endswith('.BO'):\n",
        "            base_symbol = symbol.split('.')[0]\n",
        "            exchange = 'NSE' if symbol.endswith('.NS') else 'BSE'\n",
        "        elif symbol.endswith('.L'):\n",
        "            base_symbol = symbol.split('.')[0]\n",
        "            exchange = 'LSE'\n",
        "        else:\n",
        "            base_symbol = symbol\n",
        "            exchange = 'US'\n",
        "\n",
        "        # Get company information\n",
        "        company_name = \"\"\n",
        "        company_sector = \"\"\n",
        "\n",
        "        if HAS_YFINANCE:\n",
        "            try:\n",
        "                ticker_obj = yf.Ticker(symbol)\n",
        "                info = ticker_obj.info\n",
        "\n",
        "                company_name = info.get('longName', '')\n",
        "                if company_name:\n",
        "                    company_name = re.sub(r'\\b(Ltd|Limited|Inc|Corp|Corporation|Company|Co)\\b\\.?', '', company_name, flags=re.IGNORECASE)\n",
        "                    company_name = company_name.strip()\n",
        "\n",
        "                company_sector = info.get('sector', '')\n",
        "\n",
        "                print(f\"   📋 Company: {company_name}\")\n",
        "                print(f\"   🏢 Sector: {company_sector}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Could not retrieve company info: {e}\")\n",
        "\n",
        "        return {\n",
        "            'original_symbol': symbol,\n",
        "            'base_symbol': base_symbol,\n",
        "            'exchange': exchange,\n",
        "            'company_name': company_name,\n",
        "            'company_sector': company_sector\n",
        "        }\n",
        "\n",
        "    def get_reddit_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get Reddit sentiment analysis with headlines\"\"\"\n",
        "        print(f\"🔍 Fetching Reddit data for {symbol}...\")\n",
        "\n",
        "        if not HAS_REDDIT or not self.reddit_client:\n",
        "            print(\"❌ Reddit API not available - using demo data\")\n",
        "            symbol_info = self._normalize_symbol(symbol)\n",
        "            demo_data = [\n",
        "                {\n",
        "                    \"text\": f\"{symbol} quarterly results exceeded expectations, strong fundamentals\",\n",
        "                    \"source\": \"r/investing\",\n",
        "                    \"type\": \"post_title\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Bullish on {symbol}, technical analysis shows upward momentum\",\n",
        "                    \"source\": \"r/stocks\",\n",
        "                    \"type\": \"comment\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"{symbol} dividend announcement positively received by market\",\n",
        "                    \"source\": \"r/investing\",\n",
        "                    \"type\": \"post_content\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Concerned about {symbol} debt levels, might affect future growth\",\n",
        "                    \"source\": \"r/SecurityAnalysis\",\n",
        "                    \"type\": \"comment\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"{symbol} management guidance looks promising for next quarter\",\n",
        "                    \"source\": \"r/investing\",\n",
        "                    \"type\": \"post_title\"\n",
        "                }\n",
        "            ]\n",
        "            return self._analyze_texts_with_sources(demo_data, \"Reddit (Demo)\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "        text_data = []\n",
        "\n",
        "        # Define subreddits based on exchange\n",
        "        if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "            subreddits = ['IndiaInvestments', 'investing', 'stocks', 'SecurityAnalysis', 'IndianStreetBets']\n",
        "        else:\n",
        "            subreddits = ['investing', 'stocks', 'SecurityAnalysis', 'StockMarket', 'wallstreetbets']\n",
        "\n",
        "        # Search strategies\n",
        "        search_queries = [symbol_info['base_symbol']]\n",
        "        if symbol_info['company_name']:\n",
        "            search_queries.append(f'\"{symbol_info[\"company_name\"]}\"')\n",
        "\n",
        "        for query in search_queries[:2]:\n",
        "            for subreddit_name in subreddits:\n",
        "                if len(text_data) >= 50:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    subreddit = self.reddit_client.subreddit(subreddit_name)\n",
        "\n",
        "                    for post in subreddit.search(query, limit=8, time_filter='month', sort='relevance'):\n",
        "                        if self._is_relevant_post(post, symbol_info):\n",
        "                            if post.title and len(post.title) > 20:\n",
        "                                text_data.append({\n",
        "                                    \"text\": post.title,\n",
        "                                    \"source\": f\"r/{subreddit_name}\",\n",
        "                                    \"type\": \"post_title\",\n",
        "                                    \"url\": f\"https://reddit.com{post.permalink}\"\n",
        "                                })\n",
        "\n",
        "                            if hasattr(post, 'selftext') and post.selftext and len(post.selftext) > 30:\n",
        "                                text_data.append({\n",
        "                                    \"text\": post.selftext[:600],\n",
        "                                    \"source\": f\"r/{subreddit_name}\",\n",
        "                                    \"type\": \"post_content\",\n",
        "                                    \"url\": f\"https://reddit.com{post.permalink}\"\n",
        "                                })\n",
        "\n",
        "                            # Get comments\n",
        "                            try:\n",
        "                                post.comments.replace_more(limit=0)\n",
        "                                for comment in post.comments[:3]:\n",
        "                                    if hasattr(comment, 'body') and len(comment.body) > 30:\n",
        "                                        if self._is_relevant_comment(comment.body, symbol_info):\n",
        "                                            text_data.append({\n",
        "                                                \"text\": comment.body[:500],\n",
        "                                                \"source\": f\"r/{subreddit_name}\",\n",
        "                                                \"type\": \"comment\",\n",
        "                                                \"url\": f\"https://reddit.com{post.permalink}\"\n",
        "                                            })\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"Reddit\")\n",
        "\n",
        "    def get_news_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get news sentiment analysis with headlines\"\"\"\n",
        "        print(f\"🔍 Fetching News data for {symbol}...\")\n",
        "        text_data = []\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        # Try NewsAPI\n",
        "        if API_KEYS[\"NEWS_API_KEY\"]:\n",
        "            try:\n",
        "                url = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "                search_terms = [symbol_info['base_symbol']]\n",
        "                if symbol_info['company_name']:\n",
        "                    search_terms.append(f'\"{symbol_info[\"company_name\"]}\"')\n",
        "\n",
        "                query = ' OR '.join(search_terms)\n",
        "                if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "                    query += ' AND (India OR NSE OR BSE OR stock)'\n",
        "                else:\n",
        "                    query += ' AND (stock OR financial OR earnings)'\n",
        "\n",
        "                params = {\n",
        "                    'q': query,\n",
        "                    'apiKey': API_KEYS[\"NEWS_API_KEY\"],\n",
        "                    'language': 'en',\n",
        "                    'sortBy': 'publishedAt',\n",
        "                    'pageSize': 30,\n",
        "                    'from': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "                }\n",
        "\n",
        "                response = self.session.get(url, params=params)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    for article in data.get('articles', []):\n",
        "                        title = article.get('title', '')\n",
        "                        description = article.get('description', '')\n",
        "                        content = article.get('content', '')\n",
        "                        source_name = article.get('source', {}).get('name', 'Unknown')\n",
        "                        url = article.get('url', '')\n",
        "\n",
        "                        if title and len(title) > 20:\n",
        "                            text_data.append({\n",
        "                                \"text\": title,\n",
        "                                \"source\": source_name,\n",
        "                                \"type\": \"headline\",\n",
        "                                \"url\": url,\n",
        "                                \"published_at\": article.get('publishedAt', '')\n",
        "                            })\n",
        "\n",
        "                        if description and len(description) > 30:\n",
        "                            text_data.append({\n",
        "                                \"text\": description,\n",
        "                                \"source\": source_name,\n",
        "                                \"type\": \"description\",\n",
        "                                \"url\": url,\n",
        "                                \"published_at\": article.get('publishedAt', '')\n",
        "                            })\n",
        "\n",
        "                        if content and len(content) > 50:\n",
        "                            text_data.append({\n",
        "                                \"text\": content[:800],\n",
        "                                \"source\": source_name,\n",
        "                                \"type\": \"content\",\n",
        "                                \"url\": url,\n",
        "                                \"published_at\": article.get('publishedAt', '')\n",
        "                            })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"NewsAPI error: {e}\")\n",
        "\n",
        "        # Fallback to demo data\n",
        "        if not text_data:\n",
        "            company_name = symbol_info['company_name'] or symbol_info['base_symbol']\n",
        "            text_data = [\n",
        "                {\n",
        "                    \"text\": f\"{company_name} reports strong quarterly earnings growth\",\n",
        "                    \"source\": \"Financial Times\",\n",
        "                    \"type\": \"headline\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"{symbol_info['base_symbol']} stock rallies on positive analyst coverage\",\n",
        "                    \"source\": \"Reuters\",\n",
        "                    \"type\": \"headline\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Institutional investors increase holdings in {company_name}\",\n",
        "                    \"source\": \"Bloomberg\",\n",
        "                    \"type\": \"headline\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Market volatility creates uncertainty for {symbol_info['base_symbol']} investors\",\n",
        "                    \"source\": \"CNBC\",\n",
        "                    \"type\": \"headline\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"{company_name} management provides optimistic forward guidance\",\n",
        "                    \"source\": \"Wall Street Journal\",\n",
        "                    \"type\": \"headline\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"News\")\n",
        "\n",
        "    def get_youtube_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get YouTube sentiment analysis with video titles\"\"\"\n",
        "        print(f\"🔍 Fetching YouTube data for {symbol}...\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        if not API_KEYS[\"YOUTUBE_API_KEY\"]:\n",
        "            company_name = symbol_info['company_name'] or symbol_info['base_symbol']\n",
        "            demo_data = [\n",
        "                {\n",
        "                    \"text\": f\"{symbol_info['base_symbol']} Technical Analysis - Bullish breakout pattern emerging\",\n",
        "                    \"source\": \"Financial Analysis Channel\",\n",
        "                    \"type\": \"video_title\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Why {company_name} is my top pick for 2024 - Strong fundamentals\",\n",
        "                    \"source\": \"Stock Guru\",\n",
        "                    \"type\": \"video_title\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"{symbol_info['base_symbol']} Stock Analysis: Buy or Sell? Complete review\",\n",
        "                    \"source\": \"Investment Insights\",\n",
        "                    \"type\": \"video_title\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Great analysis! I'm adding {symbol_info['base_symbol']} to my portfolio\",\n",
        "                    \"source\": \"YouTube Comments\",\n",
        "                    \"type\": \"comment\"\n",
        "                }\n",
        "            ]\n",
        "            return self._analyze_texts_with_sources(demo_data, \"YouTube (Demo)\")\n",
        "\n",
        "        text_data = []\n",
        "        try:\n",
        "            search_queries = [\n",
        "                f'\"{symbol_info[\"base_symbol\"]}\" stock analysis',\n",
        "                f'{symbol_info[\"company_name\"]} investment' if symbol_info['company_name'] else None\n",
        "            ]\n",
        "\n",
        "            search_queries = [q for q in search_queries if q]\n",
        "\n",
        "            for query in search_queries[:2]:\n",
        "                search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "                search_params = {\n",
        "                    'part': 'snippet',\n",
        "                    'q': query,\n",
        "                    'type': 'video',\n",
        "                    'maxResults': 15,\n",
        "                    'key': API_KEYS[\"YOUTUBE_API_KEY\"],\n",
        "                    'order': 'relevance'\n",
        "                }\n",
        "\n",
        "                response = self.session.get(search_url, params=search_params)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for item in data.get('items', []):\n",
        "                        snippet = item.get('snippet', {})\n",
        "                        title = snippet.get('title', '')\n",
        "                        description = snippet.get('description', '')\n",
        "                        channel_title = snippet.get('channelTitle', 'Unknown Channel')\n",
        "                        video_id = item.get('id', {}).get('videoId', '')\n",
        "                        video_url = f\"https://www.youtube.com/watch?v={video_id}\" if video_id else \"\"\n",
        "\n",
        "                        if title and len(title) > 20:\n",
        "                            text_data.append({\n",
        "                                \"text\": title,\n",
        "                                \"source\": channel_title,\n",
        "                                \"type\": \"video_title\",\n",
        "                                \"url\": video_url\n",
        "                            })\n",
        "                        if description and len(description) > 30:\n",
        "                            text_data.append({\n",
        "                                \"text\": description[:400],\n",
        "                                \"source\": channel_title,\n",
        "                                \"type\": \"video_description\",\n",
        "                                \"url\": video_url\n",
        "                            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"YouTube API error: {e}\")\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"YouTube\")\n",
        "\n",
        "    def get_twitter_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get Twitter sentiment analysis with tweets\"\"\"\n",
        "        print(f\"🔍 Fetching Twitter data for {symbol}...\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        if not API_KEYS[\"TWITTER_BEARER_TOKEN\"]:\n",
        "            demo_data = [\n",
        "                {\n",
        "                    \"text\": f\"${symbol_info['base_symbol']} breaking resistance, bullish momentum building 📈\",\n",
        "                    \"source\": \"Twitter\",\n",
        "                    \"type\": \"tweet\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Excellent earnings report from ${symbol_info['base_symbol']}, going long\",\n",
        "                    \"source\": \"Twitter\",\n",
        "                    \"type\": \"tweet\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"${symbol_info['base_symbol']} showing weakness, considering profit taking\",\n",
        "                    \"source\": \"Twitter\",\n",
        "                    \"type\": \"tweet\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"${symbol_info['base_symbol']} fundamentals remain strong despite market volatility\",\n",
        "                    \"source\": \"Twitter\",\n",
        "                    \"type\": \"tweet\"\n",
        "                }\n",
        "            ]\n",
        "            return self._analyze_texts_with_sources(demo_data, \"Twitter (Demo)\")\n",
        "\n",
        "        text_data = []\n",
        "        try:\n",
        "            url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "            headers = {'Authorization': f'Bearer {API_KEYS[\"TWITTER_BEARER_TOKEN\"]}'}\n",
        "\n",
        "            search_terms = [f'${symbol_info[\"base_symbol\"]}', symbol_info['base_symbol']]\n",
        "            query = f'({\" OR \".join(search_terms)}) (stock OR investment OR earnings OR financial)'\n",
        "\n",
        "            if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "                query += ' (India OR NSE OR BSE)'\n",
        "\n",
        "            query += ' -is:retweet lang:en'\n",
        "\n",
        "            params = {\n",
        "                'query': query,\n",
        "                'max_results': 100,\n",
        "                'tweet.fields': 'public_metrics,created_at'\n",
        "            }\n",
        "\n",
        "            response = self.session.get(url, headers=headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for tweet in data.get('data', []):\n",
        "                    text = tweet.get('text', '')\n",
        "                    if text and len(text) > 25:\n",
        "                        text_data.append({\n",
        "                            \"text\": text,\n",
        "                            \"source\": \"Twitter\",\n",
        "                            \"type\": \"tweet\",\n",
        "                            \"created_at\": tweet.get('created_at', '')\n",
        "                        })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Twitter API error: {e}\")\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"Twitter\")\n",
        "\n",
        "    def get_rss_feeds_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get RSS feeds sentiment analysis with headlines\"\"\"\n",
        "        print(f\"🔍 Fetching RSS feeds data for {symbol}...\")\n",
        "\n",
        "        if not HAS_FEEDPARSER:\n",
        "            print(\"⚠️ feedparser not available - using demo data\")\n",
        "            symbol_info = self._normalize_symbol(symbol)\n",
        "            demo_data = [\n",
        "                {\n",
        "                    \"text\": f\"{symbol} shows resilience amid market turbulence\",\n",
        "                    \"source\": \"MarketWatch RSS\",\n",
        "                    \"type\": \"rss_title\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Analysts upgrade {symbol} following strong performance metrics\",\n",
        "                    \"source\": \"Yahoo Finance RSS\",\n",
        "                    \"type\": \"rss_title\"\n",
        "                }\n",
        "            ]\n",
        "            return self._analyze_texts_with_sources(demo_data, \"RSS (Demo)\")\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "        text_data = []\n",
        "\n",
        "        # RSS feed URLs\n",
        "        rss_feeds = [\n",
        "            \"https://feeds.finance.yahoo.com/rss/2.0/headline\",\n",
        "            \"https://www.marketwatch.com/rss/topstories\",\n",
        "            \"https://feeds.bloomberg.com/markets/news.rss\",\n",
        "            \"https://www.cnbc.com/id/100003114/device/rss/rss.html\"\n",
        "        ]\n",
        "\n",
        "        for feed_url in rss_feeds:\n",
        "            try:\n",
        "                feed = feedparser.parse(feed_url)\n",
        "\n",
        "                for entry in feed.entries[:10]:  # Limit to 10 entries per feed\n",
        "                    title = entry.get('title', '')\n",
        "                    summary = entry.get('summary', '')\n",
        "\n",
        "                    # Check if the entry is relevant to the symbol\n",
        "                    if self._is_rss_entry_relevant(title + ' ' + summary, symbol_info):\n",
        "                        if title and len(title) > 20:\n",
        "                            text_data.append({\n",
        "                                \"text\": title,\n",
        "                                \"source\": feed.feed.get('title', 'RSS Feed'),\n",
        "                                \"type\": \"rss_title\",\n",
        "                                \"url\": entry.get('link', ''),\n",
        "                                \"published_at\": entry.get('published', '')\n",
        "                            })\n",
        "\n",
        "                        if summary and len(summary) > 30:\n",
        "                            text_data.append({\n",
        "                                \"text\": summary[:500],\n",
        "                                \"source\": feed.feed.get('title', 'RSS Feed'),\n",
        "                                \"type\": \"rss_summary\",\n",
        "                                \"url\": entry.get('link', ''),\n",
        "                                \"published_at\": entry.get('published', '')\n",
        "                            })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing RSS feed {feed_url}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"RSS Feeds\")\n",
        "\n",
        "    def get_comprehensive_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get comprehensive sentiment analysis from all sources\"\"\"\n",
        "        print(f\"\\n🎯 Starting comprehensive sentiment analysis for {symbol}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        # Initialize results\n",
        "        all_sources = {}\n",
        "        overall_headlines = {\n",
        "            \"most_influential\": [],\n",
        "            \"top_positive\": [],\n",
        "            \"top_negative\": [],\n",
        "            \"all_analyzed\": []\n",
        "        }\n",
        "\n",
        "        # Get sentiment from all sources\n",
        "        sources = [\n",
        "            (\"News\", self.get_news_sentiment),\n",
        "            (\"Reddit\", self.get_reddit_sentiment),\n",
        "            (\"Twitter\", self.get_twitter_sentiment),\n",
        "            (\"YouTube\", self.get_youtube_sentiment),\n",
        "            (\"RSS\", self.get_rss_feeds_sentiment)\n",
        "        ]\n",
        "\n",
        "        total_count = 0\n",
        "        weighted_sentiments = []\n",
        "        source_weights = {\"News\": 0.3, \"Reddit\": 0.2, \"Twitter\": 0.2, \"YouTube\": 0.15, \"RSS\": 0.15}\n",
        "\n",
        "        for source_name, source_func in sources:\n",
        "            try:\n",
        "                print(f\"\\n📊 Processing {source_name}...\")\n",
        "                result = source_func(symbol)\n",
        "                all_sources[source_name] = result\n",
        "\n",
        "                # Collect headlines from all sources\n",
        "                if result.get('headlines'):\n",
        "                    overall_headlines[\"most_influential\"].extend(result['headlines'].get(\"most_influential\", []))\n",
        "                    overall_headlines[\"top_positive\"].extend(result['headlines'].get(\"top_positive\", []))\n",
        "                    overall_headlines[\"top_negative\"].extend(result['headlines'].get(\"top_negative\", []))\n",
        "                    overall_headlines[\"all_analyzed\"].extend(result['headlines'].get(\"all_analyzed\", []))\n",
        "\n",
        "                # Calculate weighted sentiment\n",
        "                if result.get('count', 0) > 0:\n",
        "                    weight = source_weights.get(source_name, 0.1)\n",
        "                    sentiment_score = result['sentiment']['score']\n",
        "                    confidence = result['sentiment']['confidence']\n",
        "                    weighted_sentiments.append({\n",
        "                        'sentiment': sentiment_score,\n",
        "                        'weight': weight,\n",
        "                        'confidence': confidence,\n",
        "                        'count': result['count']\n",
        "                    })\n",
        "                    total_count += result['count']\n",
        "\n",
        "                print(f\"   ✅ {source_name}: {result['count']} items, sentiment: {result['sentiment']['score']:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Error processing {source_name}: {e}\")\n",
        "                all_sources[source_name] = self._empty_result_with_headlines()\n",
        "\n",
        "        # Calculate overall sentiment\n",
        "        if weighted_sentiments:\n",
        "            # Weighted average considering confidence and count\n",
        "            total_weighted_score = 0\n",
        "            total_weights = 0\n",
        "\n",
        "            for item in weighted_sentiments:\n",
        "                effective_weight = item['weight'] * item['confidence'] * min(item['count'] / 10, 1.0)\n",
        "                total_weighted_score += item['sentiment'] * effective_weight\n",
        "                total_weights += effective_weight\n",
        "\n",
        "            overall_sentiment = total_weighted_score / total_weights if total_weights > 0 else 0\n",
        "        else:\n",
        "            overall_sentiment = 0\n",
        "\n",
        "        # Determine overall label\n",
        "        if overall_sentiment > 0.2:\n",
        "            overall_label = \"POSITIVE\"\n",
        "        elif overall_sentiment < -0.2:\n",
        "            overall_label = \"NEGATIVE\"\n",
        "        else:\n",
        "            overall_label = \"NEUTRAL\"\n",
        "\n",
        "        # Sort and limit headlines\n",
        "        overall_headlines[\"most_influential\"] = sorted(\n",
        "            overall_headlines[\"most_influential\"],\n",
        "            key=lambda x: abs(x.get('sentiment_score', 0)),\n",
        "            reverse=True\n",
        "        )[:10]\n",
        "\n",
        "        overall_headlines[\"top_positive\"] = sorted(\n",
        "            [h for h in overall_headlines[\"top_positive\"] if h.get('sentiment_score', 0) > 0.15],\n",
        "            key=lambda x: x.get('sentiment_score', 0),\n",
        "            reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        overall_headlines[\"top_negative\"] = sorted(\n",
        "            [h for h in overall_headlines[\"top_negative\"] if h.get('sentiment_score', 0) < -0.15],\n",
        "            key=lambda x: x.get('sentiment_score', 0)\n",
        "        )[:5]\n",
        "\n",
        "        # Generate trading signal\n",
        "        trading_signal = self._generate_trading_signal(overall_sentiment, overall_headlines)\n",
        "\n",
        "        return {\n",
        "            \"symbol\": symbol_info['original_symbol'],\n",
        "            \"company_info\": symbol_info,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"total_data_points\": total_count,\n",
        "            \"overall_sentiment\": {\n",
        "                \"score\": round(overall_sentiment, 4),\n",
        "                \"label\": overall_label,\n",
        "                \"confidence\": self._calculate_overall_confidence(weighted_sentiments)\n",
        "            },\n",
        "            \"source_breakdown\": all_sources,\n",
        "            \"headlines\": overall_headlines,\n",
        "            \"trading_signal\": trading_signal,\n",
        "            \"model_info\": {\n",
        "                \"method\": \"fine_tuned_financial_finbert\",\n",
        "                \"training_accuracy\": self.sentiment_analyzer.training_accuracy\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def display_analysis_results(self, results: Dict):\n",
        "        \"\"\"Display comprehensive analysis results with headlines\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"🎯 FINANCIAL SENTIMENT ANALYSIS REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"📈 Symbol: {results['symbol']}\")\n",
        "        if results['company_info']['company_name']:\n",
        "            print(f\"🏢 Company: {results['company_info']['company_name']}\")\n",
        "        if results['company_info']['company_sector']:\n",
        "            print(f\"🏭 Sector: {results['company_info']['company_sector']}\")\n",
        "        print(f\"🕐 Analysis Time: {results['analysis_timestamp']}\")\n",
        "        print(f\"📊 Total Data Points: {results['total_data_points']}\")\n",
        "\n",
        "        # Overall sentiment\n",
        "        sentiment = results['overall_sentiment']\n",
        "        print(f\"\\n🎯 OVERALL SENTIMENT\")\n",
        "        print(f\"   Score: {sentiment['score']:.4f}\")\n",
        "        print(f\"   Label: {sentiment['label']}\")\n",
        "        print(f\"   Confidence: {sentiment['confidence']:.4f}\")\n",
        "\n",
        "        # Trading signal\n",
        "        signal = results['trading_signal']\n",
        "        print(f\"\\n📈 TRADING SIGNAL\")\n",
        "        print(f\"   Recommendation: {signal['recommendation']}\")\n",
        "        print(f\"   Strength: {signal['strength']}\")\n",
        "        print(f\"   Reason: {signal['reason']}\")\n",
        "\n",
        "        # Source breakdown\n",
        "        print(f\"\\n📊 SOURCE BREAKDOWN\")\n",
        "        for source_name, source_data in results['source_breakdown'].items():\n",
        "            sentiment_info = source_data['sentiment']\n",
        "            print(f\"   {source_name}: {source_data['count']} items | \"\n",
        "                  f\"Sentiment: {sentiment_info['score']:.3f} ({sentiment_info['label']}) | \"\n",
        "                  f\"Confidence: {sentiment_info['confidence']:.3f}\")\n",
        "\n",
        "        # Key headlines that influenced sentiment\n",
        "        headlines = results['headlines']\n",
        "\n",
        "        if headlines['most_influential']:\n",
        "            print(f\"\\n🔥 MOST INFLUENTIAL HEADLINES/MESSAGES\")\n",
        "            for i, headline in enumerate(headlines['most_influential'][:5], 1):\n",
        "                print(f\"   {i}. [{headline['source']}] {headline['display_text']}\")\n",
        "                print(f\"      Sentiment: {headline['sentiment_score']:.3f} | Confidence: {headline['confidence']:.3f}\")\n",
        "                if headline.get('url'):\n",
        "                    print(f\"      URL: {headline['url']}\")\n",
        "                print()\n",
        "\n",
        "        if headlines['top_positive']:\n",
        "            print(f\"📈 TOP POSITIVE SIGNALS\")\n",
        "            for i, headline in enumerate(headlines['top_positive'][:3], 1):\n",
        "                print(f\"   {i}. [{headline['source']}] {headline['display_text']}\")\n",
        "                print(f\"      Sentiment: +{headline['sentiment_score']:.3f}\")\n",
        "                print()\n",
        "\n",
        "        if headlines['top_negative']:\n",
        "            print(f\"📉 TOP NEGATIVE SIGNALS\")\n",
        "            for i, headline in enumerate(headlines['top_negative'][:3], 1):\n",
        "                print(f\"   {i}. [{headline['source']}] {headline['display_text']}\")\n",
        "                print(f\"      Sentiment: {headline['sentiment_score']:.3f}\")\n",
        "                print()\n",
        "\n",
        "        # Model info\n",
        "        print(f\"🤖 MODEL INFORMATION\")\n",
        "        print(f\"   Method: {results['model_info']['method']}\")\n",
        "        print(f\"   Training Accuracy: {results['model_info']['training_accuracy']}\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def _is_relevant_post(self, post, symbol_info: Dict) -> bool:\n",
        "        \"\"\"Check if a Reddit post is relevant to the stock\"\"\"\n",
        "        title_lower = post.title.lower()\n",
        "        selftext_lower = getattr(post, 'selftext', '').lower()\n",
        "        full_text = f\"{title_lower} {selftext_lower}\"\n",
        "\n",
        "        # Check for symbol mentions\n",
        "        if symbol_info['base_symbol'].lower() in full_text:\n",
        "            return True\n",
        "\n",
        "        # Check for company name mentions\n",
        "        if symbol_info['company_name']:\n",
        "            company_words = symbol_info['company_name'].lower().split()\n",
        "            if len(company_words) >= 2:\n",
        "                matches = sum(1 for word in company_words if word in full_text and len(word) > 3)\n",
        "                if matches >= 2:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _is_relevant_comment(self, comment_text: str, symbol_info: Dict) -> bool:\n",
        "        \"\"\"Check if a comment is relevant to the stock\"\"\"\n",
        "        comment_lower = comment_text.lower()\n",
        "\n",
        "        if symbol_info['base_symbol'].lower() in comment_lower:\n",
        "            return True\n",
        "\n",
        "        if symbol_info['company_name']:\n",
        "            company_words = symbol_info['company_name'].lower().split()\n",
        "            for word in company_words:\n",
        "                if len(word) > 3 and word in comment_lower:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _is_rss_entry_relevant(self, entry_text: str, symbol_info: Dict) -> bool:\n",
        "        \"\"\"Check if an RSS entry is relevant to the stock\"\"\"\n",
        "        entry_lower = entry_text.lower()\n",
        "\n",
        "        # Check for symbol mentions\n",
        "        if symbol_info['base_symbol'].lower() in entry_lower:\n",
        "            return True\n",
        "\n",
        "        # Check for company name mentions\n",
        "        if symbol_info['company_name']:\n",
        "            company_words = symbol_info['company_name'].lower().split()\n",
        "            for word in company_words:\n",
        "                if len(word) > 3 and word in entry_lower:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _analyze_texts_with_sources(self, text_data: List[Dict], source: str) -> Dict:\n",
        "        \"\"\"Analyze texts with their sources and track headlines that influenced sentiment\"\"\"\n",
        "        if not text_data:\n",
        "            return self._empty_result_with_headlines()\n",
        "\n",
        "        valid_text_data = []\n",
        "        for item in text_data:\n",
        "            if item.get('text') and isinstance(item['text'], str) and len(item['text'].strip()) > 15:\n",
        "                valid_text_data.append(item)\n",
        "\n",
        "        if not valid_text_data:\n",
        "            return self._empty_result_with_headlines()\n",
        "\n",
        "        # Analyze with fine-tuned model\n",
        "        results = []\n",
        "        sentiments = []\n",
        "        confidences = []\n",
        "        analyzed_items = []\n",
        "\n",
        "        print(f\"   📊 Analyzing {len(valid_text_data)} texts with fine-tuned Financial FinBERT...\")\n",
        "\n",
        "        for item in valid_text_data:\n",
        "            try:\n",
        "                text = item['text']\n",
        "                result = self.sentiment_analyzer.predict_sentiment_with_text(text)\n",
        "                results.append(result)\n",
        "                sentiments.append(result['sentiment'])\n",
        "                confidences.append(result['confidence'])\n",
        "\n",
        "                # Store the analyzed item with metadata\n",
        "                analyzed_item = {\n",
        "                    \"text\": text,\n",
        "                    \"display_text\": result.get('display_text', text[:150] + \"...\"),\n",
        "                    \"sentiment_score\": result['sentiment'],\n",
        "                    \"confidence\": result['confidence'],\n",
        "                    \"label\": result['label'],\n",
        "                    \"source\": item.get('source', 'Unknown'),\n",
        "                    \"type\": item.get('type', 'text'),\n",
        "                    \"url\": item.get('url', ''),\n",
        "                    \"published_at\": item.get('published_at', ''),\n",
        "                    \"created_at\": item.get('created_at', '')\n",
        "                }\n",
        "                analyzed_items.append(analyzed_item)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Error analyzing text: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not sentiments:\n",
        "            return self._empty_result_with_headlines()\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_sentiment = np.mean(sentiments)\n",
        "        avg_confidence = np.mean(confidences)\n",
        "        sentiment_std = np.std(sentiments) if len(sentiments) > 1 else 0\n",
        "\n",
        "        # Determine overall label\n",
        "        if avg_sentiment > 0.2:\n",
        "            overall_label = \"POSITIVE\"\n",
        "        elif avg_sentiment < -0.2:\n",
        "            overall_label = \"NEGATIVE\"\n",
        "        else:\n",
        "            overall_label = \"NEUTRAL\"\n",
        "\n",
        "        # Sort analyzed items by absolute sentiment score (most influential first)\n",
        "        analyzed_items.sort(key=lambda x: abs(x['sentiment_score']), reverse=True)\n",
        "\n",
        "        # Get top headlines/messages that influenced sentiment\n",
        "        top_positive = [item for item in analyzed_items if item['sentiment_score'] > 0.15][:3]\n",
        "        top_negative = [item for item in analyzed_items if item['sentiment_score'] < -0.15][:3]\n",
        "        most_influential = analyzed_items[:5]\n",
        "\n",
        "        return {\n",
        "            \"count\": len(valid_text_data),\n",
        "            \"sentiment\": {\n",
        "                \"score\": round(avg_sentiment, 4),\n",
        "                \"confidence\": round(avg_confidence, 4),\n",
        "                \"std_dev\": round(sentiment_std, 4),\n",
        "                \"label\": overall_label\n",
        "            },\n",
        "            \"method\": \"fine_tuned_financial_finbert\",\n",
        "            \"distribution\": {\n",
        "                \"positive\": sum(1 for s in sentiments if s > 0.15),\n",
        "                \"negative\": sum(1 for s in sentiments if s < -0.15),\n",
        "                \"neutral\": sum(1 for s in sentiments if -0.15 <= s <= 0.15)\n",
        "            },\n",
        "            \"detailed_results\": results[:5],\n",
        "            \"headlines\": {\n",
        "                \"most_influential\": most_influential,\n",
        "                \"top_positive\": top_positive,\n",
        "                \"top_negative\": top_negative,\n",
        "                \"all_analyzed\": analyzed_items\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _empty_result_with_headlines(self) -> Dict:\n",
        "        \"\"\"Return empty result structure with headlines\"\"\"\n",
        "        return {\n",
        "            \"count\": 0,\n",
        "            \"sentiment\": {\n",
        "                \"score\": 0,\n",
        "                \"confidence\": 0,\n",
        "                \"std_dev\": 0,\n",
        "                \"label\": \"NEUTRAL\"\n",
        "            },\n",
        "            \"method\": \"no_data\",\n",
        "            \"distribution\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0},\n",
        "            \"detailed_results\": [],\n",
        "            \"headlines\": {\n",
        "                \"most_influential\": [],\n",
        "                \"top_positive\": [],\n",
        "                \"top_negative\": [],\n",
        "                \"all_analyzed\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _generate_trading_signal(self, sentiment_score: float, headlines: Dict) -> Dict:\n",
        "        \"\"\"Generate trading signal based on sentiment and headlines\"\"\"\n",
        "\n",
        "        # Count strong signals\n",
        "        strong_positive = len([h for h in headlines['top_positive'] if h.get('sentiment_score', 0) > 0.4])\n",
        "        strong_negative = len([h for h in headlines['top_negative'] if h.get('sentiment_score', 0) < -0.4])\n",
        "\n",
        "        # Determine recommendation\n",
        "        if sentiment_score > 0.3 and strong_positive >= 2:\n",
        "            recommendation = \"STRONG BUY\"\n",
        "            strength = \"High\"\n",
        "            reason = f\"Very positive sentiment ({sentiment_score:.3f}) with {strong_positive} strong positive signals\"\n",
        "        elif sentiment_score > 0.15:\n",
        "            recommendation = \"BUY\"\n",
        "            strength = \"Medium\"\n",
        "            reason = f\"Positive sentiment ({sentiment_score:.3f}) from multiple sources\"\n",
        "        elif sentiment_score < -0.3 and strong_negative >= 2:\n",
        "            recommendation = \"STRONG SELL\"\n",
        "            strength = \"High\"\n",
        "            reason = f\"Very negative sentiment ({sentiment_score:.3f}) with {strong_negative} strong negative signals\"\n",
        "        elif sentiment_score < -0.15:\n",
        "            recommendation = \"SELL\"\n",
        "            strength = \"Medium\"\n",
        "            reason = f\"Negative sentiment ({sentiment_score:.3f}) from multiple sources\"\n",
        "        else:\n",
        "            recommendation = \"HOLD\"\n",
        "            strength = \"Low\"\n",
        "            reason = f\"Neutral sentiment ({sentiment_score:.3f}) - wait for clearer signals\"\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": recommendation,\n",
        "            \"strength\": strength,\n",
        "            \"reason\": reason,\n",
        "            \"sentiment_score\": sentiment_score,\n",
        "            \"strong_positive_signals\": strong_positive,\n",
        "            \"strong_negative_signals\": strong_negative\n",
        "        }\n",
        "\n",
        "    def _calculate_overall_confidence(self, weighted_sentiments: List[Dict]) -> float:\n",
        "        \"\"\"Calculate overall confidence based on all sources\"\"\"\n",
        "        if not weighted_sentiments:\n",
        "            return 0.0\n",
        "\n",
        "        total_confidence = 0\n",
        "        total_weight = 0\n",
        "\n",
        "        for item in weighted_sentiments:\n",
        "            weight = item['weight'] * min(item['count'] / 10, 1.0)  # Normalize by count\n",
        "            total_confidence += item['confidence'] * weight\n",
        "            total_weight += weight\n",
        "\n",
        "        return round(total_confidence / total_weight if total_weight > 0 else 0.0, 4)\n",
        "\n",
        "# Usage Example and Main Function\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the enhanced sentiment analysis\"\"\"\n",
        "\n",
        "    # Initialize the collector\n",
        "    collector = FinancialDataCollector()\n",
        "\n",
        "    # Example usage\n",
        "    symbol = input(\"Enter stock symbol (e.g., AAPL, TSLA, RELIANCE.NS): \").strip()\n",
        "\n",
        "    if not symbol:\n",
        "        symbol = \"AAPL\"  # Default symbol\n",
        "\n",
        "    print(f\"\\n🚀 Starting comprehensive financial sentiment analysis for {symbol}\")\n",
        "\n",
        "    # Get comprehensive sentiment analysis\n",
        "    results = collector.get_comprehensive_sentiment(symbol)\n",
        "\n",
        "    # Display results\n",
        "    collector.display_analysis_results(results)\n",
        "\n",
        "    # Save results to file\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"sentiment_analysis_{symbol}_{timestamp}.json\"\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "        print(f\"\\n💾 Results saved to: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving results: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esLBiDtbjpqa",
        "outputId": "a121a201-2c87-4c11-ef87-07355b458d3e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing Financial Fine-tuned FinBERT model...\n",
            "📍 Model path: ./finbert_financial_sentiment\n",
            "🔧 Using device: cuda\n",
            "📥 Loading tokenizer...\n",
            "📥 Loading fine-tuned financial model...\n",
            "📊 Model training accuracy: 0.9436\n",
            "✅ Financial Fine-tuned FinBERT model loaded successfully!\n",
            "✅ Reddit client initialized\n",
            "Enter stock symbol (e.g., AAPL, TSLA, RELIANCE.NS): NTPC.NS\n",
            "\n",
            "🚀 Starting comprehensive financial sentiment analysis for NTPC.NS\n",
            "\n",
            "🎯 Starting comprehensive sentiment analysis for NTPC.NS\n",
            "============================================================\n",
            "   📋 Company: NTPC\n",
            "   🏢 Sector: Utilities\n",
            "\n",
            "📊 Processing News...\n",
            "🔍 Fetching News data for NTPC.NS...\n",
            "   📋 Company: NTPC\n",
            "   🏢 Sector: Utilities\n",
            "   📊 Analyzing 90 texts with fine-tuned Financial FinBERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ News: 90 items, sentiment: 0.172\n",
            "\n",
            "📊 Processing Reddit...\n",
            "🔍 Fetching Reddit data for NTPC.NS...\n",
            "   📋 Company: NTPC\n",
            "   🏢 Sector: Utilities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   📊 Analyzing 8 texts with fine-tuned Financial FinBERT...\n",
            "   ✅ Reddit: 8 items, sentiment: 0.002\n",
            "\n",
            "📊 Processing Twitter...\n",
            "🔍 Fetching Twitter data for NTPC.NS...\n",
            "   📋 Company: NTPC\n",
            "   🏢 Sector: Utilities\n",
            "   ✅ Twitter: 0 items, sentiment: 0.000\n",
            "\n",
            "📊 Processing YouTube...\n",
            "🔍 Fetching YouTube data for NTPC.NS...\n",
            "   📋 Company: NTPC\n",
            "   🏢 Sector: Utilities\n",
            "   📊 Analyzing 60 texts with fine-tuned Financial FinBERT...\n",
            "   ✅ YouTube: 60 items, sentiment: 0.018\n",
            "\n",
            "📊 Processing RSS...\n",
            "🔍 Fetching RSS feeds data for NTPC.NS...\n",
            "   📋 Company: NTPC\n",
            "   🏢 Sector: Utilities\n",
            "   ✅ RSS: 0 items, sentiment: 0.000\n",
            "\n",
            "================================================================================\n",
            "🎯 FINANCIAL SENTIMENT ANALYSIS REPORT\n",
            "================================================================================\n",
            "📈 Symbol: NTPC.NS\n",
            "🏢 Company: NTPC\n",
            "🏭 Sector: Utilities\n",
            "🕐 Analysis Time: 2025-08-04T06:06:08.847620\n",
            "📊 Total Data Points: 158\n",
            "\n",
            "🎯 OVERALL SENTIMENT\n",
            "   Score: 0.0863\n",
            "   Label: NEUTRAL\n",
            "   Confidence: 0.9207\n",
            "\n",
            "📈 TRADING SIGNAL\n",
            "   Recommendation: HOLD\n",
            "   Strength: Low\n",
            "   Reason: Neutral sentiment (0.086) - wait for clearer signals\n",
            "\n",
            "📊 SOURCE BREAKDOWN\n",
            "   News: 90 items | Sentiment: 0.172 (NEUTRAL) | Confidence: 0.886\n",
            "   Reddit: 8 items | Sentiment: 0.002 (NEUTRAL) | Confidence: 0.982\n",
            "   Twitter: 0 items | Sentiment: 0.000 (NEUTRAL) | Confidence: 0.000\n",
            "   YouTube: 60 items | Sentiment: 0.018 (NEUTRAL) | Confidence: 0.926\n",
            "   RSS: 0 items | Sentiment: 0.000 (NEUTRAL) | Confidence: 0.000\n",
            "\n",
            "🔥 MOST INFLUENTIAL HEADLINES/MESSAGES\n",
            "   1. [PR Newswire UK] DELRAY BEACH, Fla., Aug 1, 2025 /PRNewswire/ --According to MarketsandMarkets™, the Software-defined Wide Area Network Market is expected to grow to U...\n",
            "      Sentiment: 0.957 | Confidence: 0.972\n",
            "      URL: https://www.prnewswire.co.uk/news-releases/software-defined-wide-area-network-sd-wan-market-worth-21-67-billion-by-2030--marketsandmarkets-302519469.html\n",
            "\n",
            "   2. [PR Newswire UK] DELRAY BEACH, Fla., Aug 1, 2025 /PRNewswire/ --According to MarketsandMarkets, the Software-defined Wide Area Network Market is expected to grow to US...\n",
            "      Sentiment: 0.955 | Confidence: 0.969\n",
            "      URL: https://www.prnewswire.co.uk/news-releases/software-defined-wide-area-network-sd-wan-market-worth-21-67-billion-by-2030--marketsandmarkets-302519469.html\n",
            "\n",
            "   3. [The Times of India] Flysbs Aviation's IPO is set to open on Friday. Investor interest is high, indicated by a strong grey market premium. The IPO aims to raise Rupees 102...\n",
            "      Sentiment: 0.940 | Confidence: 0.954\n",
            "      URL: https://economictimes.indiatimes.com/markets/ipos/fpos/flysbs-aviation-ipo-to-open-with-strong-buzz-as-gmp-soars-to-86/articleshow/123032974.cms\n",
            "\n",
            "   4. [The Times of India] BIAL completes India's largest unlisted bond issuance in the airport sector, raising Rs 9,000 crore\n",
            "      Sentiment: 0.931 | Confidence: 0.949\n",
            "      URL: https://economictimes.indiatimes.com/markets/bonds/bial-completes-indias-largest-unlisted-bond-issuance-in-the-airport-sector-raising-rs-9000-crore/articleshow/123043233.cms\n",
            "\n",
            "   5. [The Times of India] PC Jeweller Q1 Results: Profit rises 4% to Rs 162 crore, revenue up at Rs 808 crore\n",
            "      Sentiment: 0.929 | Confidence: 0.961\n",
            "      URL: https://economictimes.indiatimes.com/markets/stocks/earnings/pc-jeweller-q1-results-profit-rises-4-to-rs-162-crore-revenue-up-at-rs-808-crore/articleshow/123046625.cms\n",
            "\n",
            "📈 TOP POSITIVE SIGNALS\n",
            "   1. [PR Newswire UK] DELRAY BEACH, Fla., Aug 1, 2025 /PRNewswire/ --According to MarketsandMarkets™, the Software-defined Wide Area Network Market is expected to grow to U...\n",
            "      Sentiment: +0.957\n",
            "\n",
            "   2. [PR Newswire UK] DELRAY BEACH, Fla., Aug 1, 2025 /PRNewswire/ --According to MarketsandMarkets, the Software-defined Wide Area Network Market is expected to grow to US...\n",
            "      Sentiment: +0.955\n",
            "\n",
            "   3. [The Times of India] Flysbs Aviation's IPO is set to open on Friday. Investor interest is high, indicated by a strong grey market premium. The IPO aims to raise Rupees 102...\n",
            "      Sentiment: +0.940\n",
            "\n",
            "📉 TOP NEGATIVE SIGNALS\n",
            "   1. [The Times of India] The Indian rupee hit a record low, closing at 87.59 against the dollar. This decline occurred due to worries about US tariffs and continuous selling o...\n",
            "      Sentiment: -0.915\n",
            "\n",
            "   2. [BusinessLine] Markets end marginally lower after Trump’s tariff surprise, FMCG stocks outperform\n",
            "      Sentiment: -0.913\n",
            "\n",
            "   3. [The Times of India] Coal India Q1 Results: State-owned coal miner Coal India reported 20% YoY decline in its consolidated net profit at Rs 8734 crore in the first quarter...\n",
            "      Sentiment: -0.909\n",
            "\n",
            "🤖 MODEL INFORMATION\n",
            "   Method: fine_tuned_financial_finbert\n",
            "   Training Accuracy: 0.9436\n",
            "================================================================================\n",
            "\n",
            "💾 Results saved to: sentiment_analysis_NTPC.NS_20250804_060608.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# financial_sentiment_analysis_complete.py - Complete Sentiment Analysis for Google Colab\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install required packages if not available\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages for Colab\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\n",
        "        'transformers',\n",
        "        'torch',\n",
        "        'yfinance',\n",
        "        'pandas',\n",
        "        'feedparser',\n",
        "        'praw',\n",
        "        'numpy'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            print(f\"📦 Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Call installation function\n",
        "try:\n",
        "    install_requirements()\n",
        "    print(\"✅ All packages installed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Some packages might not be installed: {e}\")\n",
        "\n",
        "# Try to import optional libraries\n",
        "try:\n",
        "    import feedparser\n",
        "    HAS_FEEDPARSER = True\n",
        "except ImportError:\n",
        "    HAS_FEEDPARSER = False\n",
        "    print(\"⚠️ feedparser not installed. RSS feeds will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    import praw\n",
        "    HAS_REDDIT = True\n",
        "except ImportError:\n",
        "    HAS_REDDIT = False\n",
        "    print(\"⚠️ praw not installed. Reddit data will be unavailable.\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    import torch\n",
        "    HAS_TRANSFORMERS = True\n",
        "except ImportError:\n",
        "    HAS_TRANSFORMERS = False\n",
        "    print(\"⚠️ transformers not installed. Will use fallback sentiment analysis.\")\n",
        "\n",
        "try:\n",
        "    import yfinance as yf\n",
        "    import pandas as pd\n",
        "    HAS_YFINANCE = True\n",
        "except ImportError:\n",
        "    HAS_YFINANCE = False\n",
        "    print(\"⚠️ yfinance not installed. Stock market data snapshot will be unavailable.\")\n",
        "\n",
        "# API Configuration - Using environment variables or None\n",
        "def get_api_key(key_name: str) -> Optional[str]:\n",
        "    \"\"\"Safely get API keys from environment or return None\"\"\"\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(key_name)\n",
        "    except:\n",
        "        return os.environ.get(key_name)\n",
        "\n",
        "API_KEYS = {\n",
        "    \"REDDIT_CLIENT_ID\": get_api_key('reddit_client_id'),\n",
        "    \"REDDIT_CLIENT_SECRET\": get_api_key('reddit_client_secret'),\n",
        "    \"REDDIT_USER_AGENT\": \"windows:FINANCIAL_SENTIMENT:1.0 (by u/FinancialAnalyst)\",\n",
        "    \"YOUTUBE_API_KEY\": get_api_key('YOUTUBE_API_KEY'),\n",
        "    \"TWITTER_BEARER_TOKEN\": get_api_key('twitter_api'),\n",
        "    \"ALPHA_VANTAGE_API_KEY\": get_api_key('alpha_vantage'),\n",
        "    \"NEWS_API_KEY\": get_api_key('news_api'),\n",
        "}\n",
        "\n",
        "class FinancialFinBERTAnalyzer:\n",
        "    \"\"\"Financial sentiment analyzer with fallback to pre-trained models\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"ProsusAI/finbert\"):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if HAS_TRANSFORMERS else None\n",
        "        self.label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "        self.training_accuracy = \"Pre-trained FinBERT\"\n",
        "\n",
        "        print(f\"🚀 Initializing Financial FinBERT model...\")\n",
        "        print(f\"📍 Model: {model_path}\")\n",
        "        if self.device:\n",
        "            print(f\"🔧 Using device: {self.device}\")\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the financial sentiment model\"\"\"\n",
        "        if not HAS_TRANSFORMERS:\n",
        "            print(\"❌ transformers library not available - using fallback sentiment analysis\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(\"📥 Loading pre-trained FinBERT model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
        "\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            print(\"✅ FinBERT model loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {e}\")\n",
        "            print(\"Using fallback sentiment analysis...\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def predict_sentiment_with_text(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment and return original text\"\"\"\n",
        "        sentiment_result = self.predict_sentiment(text)\n",
        "        sentiment_result[\"original_text\"] = text\n",
        "        sentiment_result[\"display_text\"] = self._truncate_text(text, 150)\n",
        "        return sentiment_result\n",
        "\n",
        "    def predict_sentiment(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment using FinBERT model\"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return self._basic_sentiment_fallback(text)\n",
        "\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"neutral\", \"method\": \"empty_text\"}\n",
        "\n",
        "        try:\n",
        "            clean_text = self._preprocess_text(text)\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = torch.max(predictions).item()\n",
        "            probs = predictions.cpu().numpy()[0]\n",
        "\n",
        "            # Convert to sentiment score (-1 to +1 scale)\n",
        "            if predicted_class == 2:  # positive\n",
        "                sentiment_score = probs[2] - probs[0]\n",
        "            elif predicted_class == 0:  # negative\n",
        "                sentiment_score = -(probs[0] - probs[2])\n",
        "            else:  # neutral\n",
        "                sentiment_score = (probs[2] - probs[0]) * 0.5\n",
        "\n",
        "            label = self.label_mapping[predicted_class]\n",
        "\n",
        "            return {\n",
        "                \"sentiment\": round(sentiment_score, 4),\n",
        "                \"confidence\": round(confidence, 4),\n",
        "                \"label\": label.upper(),\n",
        "                \"method\": \"finbert\",\n",
        "                \"predicted_class\": predicted_class,\n",
        "                \"class_probabilities\": {\n",
        "                    \"negative\": round(probs[0], 4),\n",
        "                    \"neutral\": round(probs[1], 4),\n",
        "                    \"positive\": round(probs[2], 4)\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Model prediction error: {e}\")\n",
        "            return self._basic_sentiment_fallback(text)\n",
        "\n",
        "    def _truncate_text(self, text: str, max_length: int = 150) -> str:\n",
        "        \"\"\"Truncate text for display purposes\"\"\"\n",
        "        if len(text) <= max_length:\n",
        "            return text\n",
        "        return text[:max_length] + \"...\"\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text for financial analysis\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'[^\\w\\s.,!?$%-]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def _basic_sentiment_fallback(self, text: str) -> Dict:\n",
        "        \"\"\"Enhanced fallback sentiment analysis for financial text\"\"\"\n",
        "        if not text:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"NEUTRAL\", \"method\": \"fallback_empty\"}\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        positive_words = [\n",
        "            'buy', 'bull', 'bullish', 'up', 'rise', 'rising', 'gain', 'gains',\n",
        "            'profit', 'profits', 'good', 'great', 'excellent', 'strong', 'growth',\n",
        "            'increase', 'positive', 'optimistic', 'confidence', 'outperform',\n",
        "            'beat', 'exceed', 'upgrade', 'recommend', 'target', 'momentum',\n",
        "            'rally', 'surge', 'boom', 'expansion', 'breakthrough', 'success',\n",
        "            'solid', 'robust', 'healthy', 'promising', 'favorable', 'dividend'\n",
        "        ]\n",
        "\n",
        "        negative_words = [\n",
        "            'sell', 'bear', 'bearish', 'down', 'fall', 'falling', 'loss', 'losses',\n",
        "            'bad', 'terrible', 'weak', 'decline', 'decrease', 'negative',\n",
        "            'pessimistic', 'concern', 'underperform', 'miss', 'downgrade',\n",
        "            'avoid', 'risk', 'crash', 'drop', 'plunge', 'collapse',\n",
        "            'recession', 'crisis', 'warning', 'caution', 'debt', 'bankruptcy'\n",
        "        ]\n",
        "\n",
        "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "        total_words = len(text.split())\n",
        "        if total_words == 0:\n",
        "            return {\"sentiment\": 0.0, \"confidence\": 0.0, \"label\": \"NEUTRAL\", \"method\": \"fallback_empty\"}\n",
        "\n",
        "        net_sentiment = positive_count - negative_count\n",
        "        sentiment_raw = net_sentiment / max(total_words, 1)\n",
        "        sentiment_score = max(-1.0, min(1.0, sentiment_raw * 3))\n",
        "\n",
        "        if sentiment_score > 0.15:\n",
        "            label = \"POSITIVE\"\n",
        "            confidence = min(0.8, abs(sentiment_score) + 0.4)\n",
        "        elif sentiment_score < -0.15:\n",
        "            label = \"NEGATIVE\"\n",
        "            confidence = min(0.8, abs(sentiment_score) + 0.4)\n",
        "        else:\n",
        "            label = \"NEUTRAL\"\n",
        "            confidence = 0.6\n",
        "\n",
        "        return {\n",
        "            \"sentiment\": round(sentiment_score, 4),\n",
        "            \"confidence\": round(confidence, 4),\n",
        "            \"label\": label,\n",
        "            \"method\": \"enhanced_financial_fallback\"\n",
        "        }\n",
        "\n",
        "class FinancialDataCollector:\n",
        "    \"\"\"Enhanced data collector for financial sentiment analysis\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"ProsusAI/finbert\"):\n",
        "        self.sentiment_analyzer = FinancialFinBERTAnalyzer(model_path)\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "        # Initialize Reddit client\n",
        "        self.reddit_client = None\n",
        "        if HAS_REDDIT and API_KEYS[\"REDDIT_CLIENT_ID\"] and API_KEYS[\"REDDIT_CLIENT_SECRET\"]:\n",
        "            try:\n",
        "                self.reddit_client = praw.Reddit(\n",
        "                    client_id=API_KEYS[\"REDDIT_CLIENT_ID\"],\n",
        "                    client_secret=API_KEYS[\"REDDIT_CLIENT_SECRET\"],\n",
        "                    user_agent=API_KEYS[\"REDDIT_USER_AGENT\"]\n",
        "                )\n",
        "                print(\"✅ Reddit client initialized\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error initializing Reddit client: {e}\")\n",
        "                self.reddit_client = None\n",
        "\n",
        "    def _normalize_symbol(self, symbol: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract and normalize symbol information\"\"\"\n",
        "        symbol = symbol.upper().strip()\n",
        "\n",
        "        if symbol.endswith('.NS') or symbol.endswith('.BO'):\n",
        "            base_symbol = symbol.split('.')[0]\n",
        "            exchange = 'NSE' if symbol.endswith('.NS') else 'BSE'\n",
        "        elif symbol.endswith('.L'):\n",
        "            base_symbol = symbol.split('.')[0]\n",
        "            exchange = 'LSE'\n",
        "        else:\n",
        "            base_symbol = symbol\n",
        "            exchange = 'US'\n",
        "\n",
        "        company_name = \"\"\n",
        "        company_sector = \"\"\n",
        "\n",
        "        if HAS_YFINANCE:\n",
        "            try:\n",
        "                ticker_obj = yf.Ticker(symbol)\n",
        "                info = ticker_obj.info\n",
        "\n",
        "                company_name = info.get('longName', '')\n",
        "                if company_name:\n",
        "                    company_name = re.sub(r'\\b(Ltd|Limited|Inc|Corp|Corporation|Company|Co)\\b\\.?', '', company_name, flags=re.IGNORECASE)\n",
        "                    company_name = company_name.strip()\n",
        "\n",
        "                company_sector = info.get('sector', '')\n",
        "                print(f\"   📋 Company: {company_name}\")\n",
        "                print(f\"   🏢 Sector: {company_sector}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Could not retrieve company info: {e}\")\n",
        "\n",
        "        return {\n",
        "            'original_symbol': symbol,\n",
        "            'base_symbol': base_symbol,\n",
        "            'exchange': exchange,\n",
        "            'company_name': company_name,\n",
        "            'company_sector': company_sector\n",
        "        }\n",
        "\n",
        "    def get_demo_data(self, symbol: str, source_name: str) -> List[Dict]:\n",
        "        \"\"\"Generate demo data for testing purposes\"\"\"\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "        company_name = symbol_info['company_name'] or symbol_info['base_symbol']\n",
        "\n",
        "        demo_templates = {\n",
        "            \"Reddit\": [\n",
        "                f\"{symbol} quarterly results exceeded expectations, strong fundamentals\",\n",
        "                f\"Bullish on {symbol}, technical analysis shows upward momentum\",\n",
        "                f\"{symbol} dividend announcement positively received by market\",\n",
        "                f\"Concerned about {symbol} debt levels, might affect future growth\",\n",
        "                f\"{company_name} management guidance looks promising for next quarter\",\n",
        "                f\"${symbol} showing strong support at current levels\",\n",
        "                f\"Long-term outlook for {company_name} remains positive despite volatility\"\n",
        "            ],\n",
        "            \"News\": [\n",
        "                f\"{company_name} reports strong quarterly earnings growth\",\n",
        "                f\"{symbol} stock rallies on positive analyst coverage\",\n",
        "                f\"Institutional investors increase holdings in {company_name}\",\n",
        "                f\"Market volatility creates uncertainty for {symbol} investors\",\n",
        "                f\"{company_name} management provides optimistic forward guidance\",\n",
        "                f\"{symbol} beats earnings expectations for third consecutive quarter\",\n",
        "                f\"New product launch drives {company_name} stock higher\"\n",
        "            ],\n",
        "            \"YouTube\": [\n",
        "                f\"{symbol} Technical Analysis - Bullish breakout pattern emerging\",\n",
        "                f\"Why {company_name} is my top pick for 2024 - Strong fundamentals\",\n",
        "                f\"{symbol} Stock Analysis: Buy or Sell? Complete review\",\n",
        "                f\"Great analysis! I'm adding {symbol} to my portfolio\",\n",
        "                f\"{company_name} deep dive - undervalued opportunity?\",\n",
        "                f\"Options trading strategy for {symbol} earnings play\"\n",
        "            ],\n",
        "            \"Twitter\": [\n",
        "                f\"${symbol} breaking resistance, bullish momentum building 📈\",\n",
        "                f\"Excellent earnings report from ${symbol}, going long\",\n",
        "                f\"${symbol} showing weakness, considering profit taking\",\n",
        "                f\"${symbol} fundamentals remain strong despite market volatility\",\n",
        "                f\"Chart setup on ${symbol} looks very promising\",\n",
        "                f\"${symbol} volume spike suggests institutional accumulation\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        texts = demo_templates.get(source_name, demo_templates[\"News\"])\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                \"text\": text,\n",
        "                \"source\": f\"{source_name} Demo\",\n",
        "                \"type\": \"demo_content\"\n",
        "            }\n",
        "            for text in texts[:5]  # Limit to 5 items\n",
        "        ]\n",
        "\n",
        "    def get_reddit_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get Reddit sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching Reddit data for {symbol}...\")\n",
        "\n",
        "        # Use demo data for now\n",
        "        demo_data = self.get_demo_data(symbol, \"Reddit\")\n",
        "        return self._analyze_texts_with_sources(demo_data, \"Reddit\")\n",
        "\n",
        "    def get_news_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get news sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching News data for {symbol}...\")\n",
        "\n",
        "        # Try to get real news data first\n",
        "        text_data = []\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        if API_KEYS[\"NEWS_API_KEY\"]:\n",
        "            try:\n",
        "                url = \"https://newsapi.org/v2/everything\"\n",
        "                search_terms = [symbol_info['base_symbol']]\n",
        "                if symbol_info['company_name']:\n",
        "                    search_terms.append(f'\"{symbol_info[\"company_name\"]}\"')\n",
        "\n",
        "                query = ' OR '.join(search_terms)\n",
        "                if symbol_info['exchange'] in ['NSE', 'BSE']:\n",
        "                    query += ' AND (India OR NSE OR BSE OR stock)'\n",
        "                else:\n",
        "                    query += ' AND (stock OR financial OR earnings)'\n",
        "\n",
        "                params = {\n",
        "                    'q': query,\n",
        "                    'apiKey': API_KEYS[\"NEWS_API_KEY\"],\n",
        "                    'language': 'en',\n",
        "                    'sortBy': 'publishedAt',\n",
        "                    'pageSize': 20,\n",
        "                    'from': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "                }\n",
        "\n",
        "                response = self.session.get(url, params=params, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    for article in data.get('articles', []):\n",
        "                        title = article.get('title', '')\n",
        "                        description = article.get('description', '')\n",
        "                        source_name = article.get('source', {}).get('name', 'Unknown')\n",
        "\n",
        "                        if title and len(title) > 20:\n",
        "                            text_data.append({\n",
        "                                \"text\": title,\n",
        "                                \"source\": source_name,\n",
        "                                \"type\": \"headline\",\n",
        "                                \"url\": article.get('url', '')\n",
        "                            })\n",
        "\n",
        "                        if description and len(description) > 30:\n",
        "                            text_data.append({\n",
        "                                \"text\": description,\n",
        "                                \"source\": source_name,\n",
        "                                \"type\": \"description\",\n",
        "                                \"url\": article.get('url', '')\n",
        "                            })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"NewsAPI error: {e}\")\n",
        "\n",
        "        # Fallback to demo data if no real data\n",
        "        if not text_data:\n",
        "            text_data = self.get_demo_data(symbol, \"News\")\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"News\")\n",
        "\n",
        "    def get_youtube_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get YouTube sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching YouTube data for {symbol}...\")\n",
        "\n",
        "        # Use demo data for now\n",
        "        demo_data = self.get_demo_data(symbol, \"YouTube\")\n",
        "        return self._analyze_texts_with_sources(demo_data, \"YouTube\")\n",
        "\n",
        "    def get_twitter_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get Twitter sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching Twitter data for {symbol}...\")\n",
        "\n",
        "        # Use demo data for now\n",
        "        demo_data = self.get_demo_data(symbol, \"Twitter\")\n",
        "        return self._analyze_texts_with_sources(demo_data, \"Twitter\")\n",
        "\n",
        "    def get_rss_feeds_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get RSS feeds sentiment analysis\"\"\"\n",
        "        print(f\"🔍 Fetching RSS feeds data for {symbol}...\")\n",
        "\n",
        "        text_data = []\n",
        "\n",
        "        if HAS_FEEDPARSER:\n",
        "            try:\n",
        "                # Try a few financial RSS feeds\n",
        "                rss_feeds = [\n",
        "                    \"https://feeds.finance.yahoo.com/rss/2.0/headline\",\n",
        "                    \"https://www.marketwatch.com/rss/topstories\"\n",
        "                ]\n",
        "\n",
        "                for feed_url in rss_feeds:\n",
        "                    try:\n",
        "                        feed = feedparser.parse(feed_url)\n",
        "                        for entry in feed.entries[:5]:\n",
        "                            title = entry.get('title', '')\n",
        "                            if title and len(title) > 20:\n",
        "                                text_data.append({\n",
        "                                    \"text\": title,\n",
        "                                    \"source\": \"RSS Feed\",\n",
        "                                    \"type\": \"rss_title\"\n",
        "                                })\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"RSS error: {e}\")\n",
        "\n",
        "        # Fallback to demo data if no real data\n",
        "        if not text_data:\n",
        "            symbol_info = self._normalize_symbol(symbol)\n",
        "            text_data = [\n",
        "                {\n",
        "                    \"text\": f\"{symbol} shows resilience amid market turbulence\",\n",
        "                    \"source\": \"MarketWatch RSS\",\n",
        "                    \"type\": \"rss_title\"\n",
        "                },\n",
        "                {\n",
        "                    \"text\": f\"Analysts upgrade {symbol} following strong performance metrics\",\n",
        "                    \"source\": \"Yahoo Finance RSS\",\n",
        "                    \"type\": \"rss_title\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        return self._analyze_texts_with_sources(text_data, \"RSS Feeds\")\n",
        "\n",
        "    def get_comprehensive_sentiment(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get comprehensive sentiment analysis from all sources\"\"\"\n",
        "        print(f\"\\n🎯 Starting comprehensive sentiment analysis for {symbol}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        symbol_info = self._normalize_symbol(symbol)\n",
        "\n",
        "        all_sources = {}\n",
        "        overall_headlines = {\n",
        "            \"most_influential\": [],\n",
        "            \"top_positive\": [],\n",
        "            \"top_negative\": [],\n",
        "            \"all_analyzed\": []\n",
        "        }\n",
        "\n",
        "        sources = [\n",
        "            (\"News\", self.get_news_sentiment),\n",
        "            (\"Reddit\", self.get_reddit_sentiment),\n",
        "            (\"Twitter\", self.get_twitter_sentiment),\n",
        "            (\"YouTube\", self.get_youtube_sentiment),\n",
        "            (\"RSS\", self.get_rss_feeds_sentiment)\n",
        "        ]\n",
        "\n",
        "        total_count = 0\n",
        "        weighted_sentiments = []\n",
        "        source_weights = {\"News\": 0.3, \"Reddit\": 0.2, \"Twitter\": 0.2, \"YouTube\": 0.15, \"RSS\": 0.15}\n",
        "\n",
        "        for source_name, source_func in sources:\n",
        "            try:\n",
        "                print(f\"\\n📊 Processing {source_name}...\")\n",
        "                result = source_func(symbol)\n",
        "                all_sources[source_name] = result\n",
        "\n",
        "                if result.get('headlines'):\n",
        "                    overall_headlines[\"most_influential\"].extend(result['headlines'].get(\"most_influential\", []))\n",
        "                    overall_headlines[\"top_positive\"].extend(result['headlines'].get(\"top_positive\", []))\n",
        "                    overall_headlines[\"top_negative\"].extend(result['headlines'].get(\"top_negative\", []))\n",
        "                    overall_headlines[\"all_analyzed\"].extend(result['headlines'].get(\"all_analyzed\", []))\n",
        "\n",
        "                if result.get('count', 0) > 0:\n",
        "                    weight = source_weights.get(source_name, 0.1)\n",
        "                    sentiment_score = result['sentiment']['score']\n",
        "                    confidence = result['sentiment']['confidence']\n",
        "                    weighted_sentiments.append({\n",
        "                        'sentiment': sentiment_score,\n",
        "                        'weight': weight,\n",
        "                        'confidence': confidence,\n",
        "                        'count': result['count']\n",
        "                    })\n",
        "                    total_count += result['count']\n",
        "\n",
        "                print(f\"   ✅ {source_name}: {result['count']} items, sentiment: {result['sentiment']['score']:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Error processing {source_name}: {e}\")\n",
        "                all_sources[source_name] = self._empty_result_with_headlines()\n",
        "\n",
        "        # Calculate overall sentiment\n",
        "        if weighted_sentiments:\n",
        "            total_weighted_score = 0\n",
        "            total_weights = 0\n",
        "\n",
        "            for item in weighted_sentiments:\n",
        "                effective_weight = item['weight'] * item['confidence'] * min(item['count'] / 10, 1.0)\n",
        "                total_weighted_score += item['sentiment'] * effective_weight\n",
        "                total_weights += effective_weight\n",
        "\n",
        "            overall_sentiment = total_weighted_score / total_weights if total_weights > 0 else 0\n",
        "        else:\n",
        "            overall_sentiment = 0\n",
        "\n",
        "        # Determine overall label\n",
        "        if overall_sentiment > 0.2:\n",
        "            overall_label = \"POSITIVE\"\n",
        "        elif overall_sentiment < -0.2:\n",
        "            overall_label = \"NEGATIVE\"\n",
        "        else:\n",
        "            overall_label = \"NEUTRAL\"\n",
        "\n",
        "        # Sort and limit headlines\n",
        "        overall_headlines[\"most_influential\"] = sorted(\n",
        "            overall_headlines[\"most_influential\"],\n",
        "            key=lambda x: abs(x.get('sentiment_score', 0)),\n",
        "            reverse=True\n",
        "        )[:10]\n",
        "\n",
        "        overall_headlines[\"top_positive\"] = sorted(\n",
        "            [h for h in overall_headlines[\"top_positive\"] if h.get('sentiment_score', 0) > 0.15],\n",
        "            key=lambda x: x.get('sentiment_score', 0),\n",
        "            reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        overall_headlines[\"top_negative\"] = sorted(\n",
        "            [h for h in overall_headlines[\"top_negative\"] if h.get('sentiment_score', 0) < -0.15],\n",
        "            key=lambda x: x.get('sentiment_score', 0)\n",
        "        )[:5]\n",
        "\n",
        "        trading_signal = self._generate_trading_signal(overall_sentiment, overall_headlines)\n",
        "\n",
        "        return {\n",
        "            \"symbol\": symbol_info['original_symbol'],\n",
        "            \"company_info\": symbol_info,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"total_data_points\": total_count,\n",
        "            \"overall_sentiment\": {\n",
        "                \"score\": round(overall_sentiment, 4),\n",
        "                \"label\": overall_label,\n",
        "                \"confidence\": self._calculate_overall_confidence(weighted_sentiments)\n",
        "            },\n",
        "            \"source_breakdown\": all_sources,\n",
        "            \"headlines\": overall_headlines,\n",
        "            \"trading_signal\": trading_signal,\n",
        "            \"model_info\": {\n",
        "                \"method\": \"finbert\",\n",
        "                \"training_accuracy\": self.sentiment_analyzer.training_accuracy\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_batch_stocks(self, symbols: List[str]) -> Dict:\n",
        "        \"\"\"Analyze multiple stocks and return batch results\"\"\"\n",
        "        print(f\"\\n🎯 Starting batch sentiment analysis for {len(symbols)} stocks\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        batch_results = {}\n",
        "        batch_summary = {\n",
        "            \"total_stocks\": len(symbols),\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"top_positive\": [],\n",
        "            \"top_negative\": [],\n",
        "            \"strongest_signals\": []\n",
        "        }\n",
        "\n",
        "        for i, symbol in enumerate(symbols, 1):\n",
        "            print(f\"\\n📊 Analyzing stock {i}/{len(symbols)}: {symbol}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            try:\n",
        "                result = self.get_comprehensive_sentiment(symbol)\n",
        "                batch_results[symbol] = result\n",
        "\n",
        "                sentiment_score = result['overall_sentiment']['score']\n",
        "                trading_signal = result['trading_signal']\n",
        "\n",
        "                stock_summary = {\n",
        "                    \"symbol\": symbol,\n",
        "                    \"sentiment_score\": sentiment_score,\n",
        "                    \"sentiment_label\": result['overall_sentiment']['label'],\n",
        "                    \"confidence\": result['overall_sentiment']['confidence'],\n",
        "                    \"trading_recommendation\": trading_signal['recommendation'],\n",
        "                    \"signal_strength\": trading_signal['strength'],\n",
        "                    \"total_data_points\": result['total_data_points'],\n",
        "                    \"company_name\": result['company_info'].get('company_name', ''),\n",
        "                    \"sector\": result['company_info'].get('company_sector', '')\n",
        "                }\n",
        "\n",
        "                if sentiment_score > 0.2:\n",
        "                    batch_summary[\"top_positive\"].append(stock_summary)\n",
        "                elif sentiment_score < -0.2:\n",
        "                    batch_summary[\"top_negative\"].append(stock_summary)\n",
        "\n",
        "                if result['overall_sentiment']['confidence'] > 0.7:\n",
        "                    batch_summary[\"strongest_signals\"].append(stock_summary)\n",
        "\n",
        "                print(f\"✅ Completed {symbol}: {result['overall_sentiment']['label']} ({sentiment_score:.3f})\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error analyzing {symbol}: {e}\")\n",
        "                batch_results[symbol] = {\"error\": str(e)}\n",
        "                continue\n",
        "\n",
        "        # Sort summary lists\n",
        "        batch_summary[\"top_positive\"] = sorted(\n",
        "            batch_summary[\"top_positive\"],\n",
        "            key=lambda x: x['sentiment_score'],\n",
        "            reverse=True\n",
        "        )[:10]\n",
        "\n",
        "        batch_summary[\"top_negative\"] = sorted(\n",
        "            batch_summary[\"top_negative\"],\n",
        "            key=lambda x: x['sentiment_score']\n",
        "        )[:10]\n",
        "\n",
        "        batch_summary[\"strongest_signals\"] = sorted(\n",
        "            batch_summary[\"strongest_signals\"],\n",
        "            key=lambda x: x['confidence'],\n",
        "            reverse=True\n",
        "        )[:10]\n",
        "\n",
        "        return {\n",
        "            \"batch_summary\": batch_summary,\n",
        "            \"detailed_results\": batch_results\n",
        "        }\n",
        "\n",
        "    def display_batch_results(self, batch_results: Dict):\n",
        "        \"\"\"Display batch analysis results\"\"\"\n",
        "        summary = batch_results[\"batch_summary\"]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(f\"📊 BATCH SENTIMENT ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        print(f\"📈 Total Stocks Analyzed: {summary['total_stocks']}\")\n",
        "        print(f\"🕐 Analysis Time: {summary['analysis_timestamp']}\")\n",
        "\n",
        "        if summary[\"top_positive\"]:\n",
        "            print(f\"\\n📈 TOP POSITIVE STOCKS ({len(summary['top_positive'])})\")\n",
        "            print(\"-\" * 80)\n",
        "            for i, stock in enumerate(summary[\"top_positive\"], 1):\n",
        "                print(f\"{i:2d}. {stock['symbol']:10s} | \"\n",
        "                      f\"Sentiment: +{stock['sentiment_score']:6.3f} | \"\n",
        "                      f\"Signal: {stock['trading_recommendation']:12s} | \"\n",
        "                      f\"Confidence: {stock['confidence']:5.3f}\")\n",
        "                if stock['company_name']:\n",
        "                    print(f\"     📋 {stock['company_name']}\")\n",
        "                print()\n",
        "\n",
        "        if summary[\"top_negative\"]:\n",
        "            print(f\"\\n📉 TOP NEGATIVE STOCKS ({len(summary['top_negative'])})\")\n",
        "            print(\"-\" * 80)\n",
        "            for i, stock in enumerate(summary[\"top_negative\"], 1):\n",
        "                print(f\"{i:2d}. {stock['symbol']:10s} | \"\n",
        "                      f\"Sentiment: {stock['sentiment_score']:7.3f} | \"\n",
        "                      f\"Signal: {stock['trading_recommendation']:12s} | \"\n",
        "                      f\"Confidence: {stock['confidence']:5.3f}\")\n",
        "                if stock['company_name']:\n",
        "                    print(f\"     📋 {stock['company_name']}\")\n",
        "                print()\n",
        "\n",
        "        if summary[\"strongest_signals\"]:\n",
        "            print(f\"\\n🎯 STRONGEST SIGNALS (High Confidence)\")\n",
        "            print(\"-\" * 80)\n",
        "            for i, stock in enumerate(summary[\"strongest_signals\"], 1):\n",
        "                print(f\"{i:2d}. {stock['symbol']:10s} | \"\n",
        "                      f\"Sentiment: {stock['sentiment_score']:7.3f} | \"\n",
        "                      f\"Signal: {stock['trading_recommendation']:12s} | \"\n",
        "                      f\"Confidence: {stock['confidence']:5.3f}\")\n",
        "                if stock['company_name']:\n",
        "                    print(f\"     📋 {stock['company_name']}\")\n",
        "                print()\n",
        "\n",
        "        print(\"=\"*100)\n",
        "\n",
        "    def display_analysis_results(self, results: Dict):\n",
        "        \"\"\"Display comprehensive analysis results with headlines\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"🎯 FINANCIAL SENTIMENT ANALYSIS REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"📈 Symbol: {results['symbol']}\")\n",
        "        if results['company_info']['company_name']:\n",
        "            print(f\"🏢 Company: {results['company_info']['company_name']}\")\n",
        "        if results['company_info']['company_sector']:\n",
        "            print(f\"🏭 Sector: {results['company_info']['company_sector']}\")\n",
        "        print(f\"🕐 Analysis Time: {results['analysis_timestamp']}\")\n",
        "        print(f\"📊 Total Data Points: {results['total_data_points']}\")\n",
        "\n",
        "        # Overall sentiment\n",
        "        sentiment = results['overall_sentiment']\n",
        "        print(f\"\\n🎯 OVERALL SENTIMENT\")\n",
        "        print(f\"   Score: {sentiment['score']:.4f}\")\n",
        "        print(f\"   Label: {sentiment['label']}\")\n",
        "        print(f\"   Confidence: {sentiment['confidence']:.4f}\")\n",
        "\n",
        "        # Trading signal\n",
        "        signal = results['trading_signal']\n",
        "        print(f\"\\n📈 TRADING SIGNAL\")\n",
        "        print(f\"   Recommendation: {signal['recommendation']}\")\n",
        "        print(f\"   Strength: {signal['strength']}\")\n",
        "        print(f\"   Reason: {signal['reason']}\")\n",
        "\n",
        "        # Source breakdown\n",
        "        print(f\"\\n📊 SOURCE BREAKDOWN\")\n",
        "        for source_name, source_data in results['source_breakdown'].items():\n",
        "            sentiment_info = source_data['sentiment']\n",
        "            print(f\"   {source_name}: {source_data['count']} items | \"\n",
        "                  f\"Sentiment: {sentiment_info['score']:.3f} ({sentiment_info['label']}) | \"\n",
        "                  f\"Confidence: {sentiment_info['confidence']:.3f}\")\n",
        "\n",
        "        # Key headlines that influenced sentiment\n",
        "        headlines = results['headlines']\n",
        "\n",
        "        if headlines['most_influential']:\n",
        "            print(f\"\\n🔥 MOST INFLUENTIAL HEADLINES/MESSAGES\")\n",
        "            for i, headline in enumerate(headlines['most_influential'][:5], 1):\n",
        "                print(f\"   {i}. [{headline['source']}] {headline['display_text']}\")\n",
        "                print(f\"      Sentiment: {headline['sentiment_score']:.3f} | Confidence: {headline['confidence']:.3f}\")\n",
        "                if headline.get('url'):\n",
        "                    print(f\"      URL: {headline['url']}\")\n",
        "                print()\n",
        "\n",
        "        if headlines['top_positive']:\n",
        "            print(f\"📈 TOP POSITIVE SIGNALS\")\n",
        "            for i, headline in enumerate(headlines['top_positive'][:3], 1):\n",
        "                print(f\"   {i}. [{headline['source']}] {headline['display_text']}\")\n",
        "                print(f\"      Sentiment: +{headline['sentiment_score']:.3f}\")\n",
        "                print()\n",
        "\n",
        "        if headlines['top_negative']:\n",
        "            print(f\"📉 TOP NEGATIVE SIGNALS\")\n",
        "            for i, headline in enumerate(headlines['top_negative'][:3], 1):\n",
        "                print(f\"   {i}. [{headline['source']}] {headline['display_text']}\")\n",
        "                print(f\"      Sentiment: {headline['sentiment_score']:.3f}\")\n",
        "                print()\n",
        "\n",
        "        # Model info\n",
        "        print(f\"🤖 MODEL INFORMATION\")\n",
        "        print(f\"   Method: {results['model_info']['method']}\")\n",
        "        print(f\"   Training Accuracy: {results['model_info']['training_accuracy']}\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def _analyze_texts_with_sources(self, text_data: List[Dict], source: str) -> Dict:\n",
        "        \"\"\"Analyze texts with their sources and track headlines that influenced sentiment\"\"\"\n",
        "        if not text_data:\n",
        "            return self._empty_result_with_headlines()\n",
        "\n",
        "        valid_text_data = []\n",
        "        for item in text_data:\n",
        "            if item.get('text') and isinstance(item['text'], str) and len(item['text'].strip()) > 15:\n",
        "                valid_text_data.append(item)\n",
        "\n",
        "        if not valid_text_data:\n",
        "            return self._empty_result_with_headlines()\n",
        "\n",
        "        results = []\n",
        "        sentiments = []\n",
        "        confidences = []\n",
        "        analyzed_items = []\n",
        "\n",
        "        print(f\"   📊 Analyzing {len(valid_text_data)} texts with FinBERT...\")\n",
        "\n",
        "        for item in valid_text_data:\n",
        "            try:\n",
        "                text = item['text']\n",
        "                result = self.sentiment_analyzer.predict_sentiment_with_text(text)\n",
        "                results.append(result)\n",
        "                sentiments.append(result['sentiment'])\n",
        "                confidences.append(result['confidence'])\n",
        "\n",
        "                analyzed_item = {\n",
        "                    \"text\": text,\n",
        "                    \"display_text\": result.get('display_text', text[:150] + \"...\"),\n",
        "                    \"sentiment_score\": result['sentiment'],\n",
        "                    \"confidence\": result['confidence'],\n",
        "                    \"label\": result['label'],\n",
        "                    \"source\": item.get('source', 'Unknown'),\n",
        "                    \"type\": item.get('type', 'text'),\n",
        "                    \"url\": item.get('url', ''),\n",
        "                    \"published_at\": item.get('published_at', ''),\n",
        "                    \"created_at\": item.get('created_at', '')\n",
        "                }\n",
        "                analyzed_items.append(analyzed_item)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Error analyzing text: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not sentiments:\n",
        "            return self._empty_result_with_headlines()\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_sentiment = np.mean(sentiments)\n",
        "        avg_confidence = np.mean(confidences)\n",
        "        sentiment_std = np.std(sentiments) if len(sentiments) > 1 else 0\n",
        "\n",
        "        # Determine overall label\n",
        "        if avg_sentiment > 0.2:\n",
        "            overall_label = \"POSITIVE\"\n",
        "        elif avg_sentiment < -0.2:\n",
        "            overall_label = \"NEGATIVE\"\n",
        "        else:\n",
        "            overall_label = \"NEUTRAL\"\n",
        "\n",
        "        # Sort analyzed items by absolute sentiment score\n",
        "        analyzed_items.sort(key=lambda x: abs(x['sentiment_score']), reverse=True)\n",
        "\n",
        "        # Get top headlines/messages that influenced sentiment\n",
        "        top_positive = [item for item in analyzed_items if item['sentiment_score'] > 0.15][:3]\n",
        "        top_negative = [item for item in analyzed_items if item['sentiment_score'] < -0.15][:3]\n",
        "        most_influential = analyzed_items[:5]\n",
        "\n",
        "        return {\n",
        "            \"count\": len(valid_text_data),\n",
        "            \"sentiment\": {\n",
        "                \"score\": round(avg_sentiment, 4),\n",
        "                \"confidence\": round(avg_confidence, 4),\n",
        "                \"std_dev\": round(sentiment_std, 4),\n",
        "                \"label\": overall_label\n",
        "            },\n",
        "            \"method\": \"finbert\",\n",
        "            \"distribution\": {\n",
        "                \"positive\": sum(1 for s in sentiments if s > 0.15),\n",
        "                \"negative\": sum(1 for s in sentiments if s < -0.15),\n",
        "                \"neutral\": sum(1 for s in sentiments if -0.15 <= s <= 0.15)\n",
        "            },\n",
        "            \"detailed_results\": results[:5],\n",
        "            \"headlines\": {\n",
        "                \"most_influential\": most_influential,\n",
        "                \"top_positive\": top_positive,\n",
        "                \"top_negative\": top_negative,\n",
        "                \"all_analyzed\": analyzed_items\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _empty_result_with_headlines(self) -> Dict:\n",
        "        \"\"\"Return empty result structure with headlines\"\"\"\n",
        "        return {\n",
        "            \"count\": 0,\n",
        "            \"sentiment\": {\n",
        "                \"score\": 0,\n",
        "                \"confidence\": 0,\n",
        "                \"std_dev\": 0,\n",
        "                \"label\": \"NEUTRAL\"\n",
        "            },\n",
        "            \"method\": \"no_data\",\n",
        "            \"distribution\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0},\n",
        "            \"detailed_results\": [],\n",
        "            \"headlines\": {\n",
        "                \"most_influential\": [],\n",
        "                \"top_positive\": [],\n",
        "                \"top_negative\": [],\n",
        "                \"all_analyzed\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _generate_trading_signal(self, sentiment_score: float, headlines: Dict) -> Dict:\n",
        "        \"\"\"Generate trading signal based on sentiment and headlines\"\"\"\n",
        "\n",
        "        strong_positive = len([h for h in headlines['top_positive'] if h.get('sentiment_score', 0) > 0.4])\n",
        "        strong_negative = len([h for h in headlines['top_negative'] if h.get('sentiment_score', 0) < -0.4])\n",
        "\n",
        "        if sentiment_score > 0.3 and strong_positive >= 2:\n",
        "            recommendation = \"STRONG BUY\"\n",
        "            strength = \"High\"\n",
        "            reason = f\"Very positive sentiment ({sentiment_score:.3f}) with {strong_positive} strong positive signals\"\n",
        "        elif sentiment_score > 0.15:\n",
        "            recommendation = \"BUY\"\n",
        "            strength = \"Medium\"\n",
        "            reason = f\"Positive sentiment ({sentiment_score:.3f}) from multiple sources\"\n",
        "        elif sentiment_score < -0.3 and strong_negative >= 2:\n",
        "            recommendation = \"STRONG SELL\"\n",
        "            strength = \"High\"\n",
        "            reason = f\"Very negative sentiment ({sentiment_score:.3f}) with {strong_negative} strong negative signals\"\n",
        "        elif sentiment_score < -0.15:\n",
        "            recommendation = \"SELL\"\n",
        "            strength = \"Medium\"\n",
        "            reason = f\"Negative sentiment ({sentiment_score:.3f}) from multiple sources\"\n",
        "        else:\n",
        "            recommendation = \"HOLD\"\n",
        "            strength = \"Low\"\n",
        "            reason = f\"Neutral sentiment ({sentiment_score:.3f}) - wait for clearer signals\"\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": recommendation,\n",
        "            \"strength\": strength,\n",
        "            \"reason\": reason,\n",
        "            \"sentiment_score\": sentiment_score,\n",
        "            \"strong_positive_signals\": strong_positive,\n",
        "            \"strong_negative_signals\": strong_negative\n",
        "        }\n",
        "\n",
        "    def _calculate_overall_confidence(self, weighted_sentiments: List[Dict]) -> float:\n",
        "        \"\"\"Calculate overall confidence based on all sources\"\"\"\n",
        "        if not weighted_sentiments:\n",
        "            return 0.0\n",
        "\n",
        "        total_confidence = 0\n",
        "        total_weight = 0\n",
        "\n",
        "        for item in weighted_sentiments:\n",
        "            weight = item['weight'] * min(item['count'] / 10, 1.0)\n",
        "            total_confidence += item['confidence'] * weight\n",
        "            total_weight += weight\n",
        "\n",
        "        return round(total_confidence / total_weight if total_weight > 0 else 0.0, 4)\n",
        "\n",
        "# Menu System Functions\n",
        "def display_menu():\n",
        "    \"\"\"Display the main menu\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🚀 FINANCIAL SENTIMENT ANALYSIS SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. 📈 Single Stock Analysis\")\n",
        "    print(\"2. 📊 Batch Stock Analysis\")\n",
        "    print(\"3. 🧪 Quick Test (AAPL)\")\n",
        "    print(\"4. 🚪 Exit\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def get_user_choice():\n",
        "    \"\"\"Get and validate user choice\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(\"Enter your choice (1-4): \").strip()\n",
        "            if choice in ['1', '2', '3', '4']:\n",
        "                return int(choice)\n",
        "            else:\n",
        "                print(\"❌ Invalid choice. Please enter 1, 2, 3, or 4.\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n👋 Goodbye!\")\n",
        "            return 4\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}. Please try again.\")\n",
        "\n",
        "def get_single_stock():\n",
        "    \"\"\"Get single stock symbol from user\"\"\"\n",
        "    while True:\n",
        "        symbol = input(\"\\nEnter stock symbol (e.g., AAPL, TSLA, RELIANCE.NS): \").strip().upper()\n",
        "        if symbol:\n",
        "            return symbol\n",
        "        else:\n",
        "            print(\"❌ Please enter a valid stock symbol.\")\n",
        "\n",
        "def get_batch_stocks():\n",
        "    \"\"\"Get multiple stock symbols from user\"\"\"\n",
        "    print(\"\\nEnter stock symbols for batch analysis:\")\n",
        "    print(\"💡 You can enter them in any of these ways:\")\n",
        "    print(\"   • Comma separated: AAPL, TSLA, MSFT\")\n",
        "    print(\"   • Space separated: AAPL TSLA MSFT\")\n",
        "    print(\"   • One per line (press Enter twice when done)\")\n",
        "\n",
        "    symbols = []\n",
        "\n",
        "    input_line = input(\"\\nEnter symbols: \").strip()\n",
        "\n",
        "    if input_line:\n",
        "        if ',' in input_line:\n",
        "            symbols = [s.strip().upper() for s in input_line.split(',') if s.strip()]\n",
        "        else:\n",
        "            symbols = [s.strip().upper() for s in input_line.split() if s.strip()]\n",
        "    else:\n",
        "        print(\"Enter one symbol per line (press Enter on empty line to finish):\")\n",
        "        while True:\n",
        "            symbol = input(\"Symbol: \").strip().upper()\n",
        "            if not symbol:\n",
        "                break\n",
        "            symbols.append(symbol)\n",
        "\n",
        "    unique_symbols = []\n",
        "    for symbol in symbols:\n",
        "        if symbol not in unique_symbols:\n",
        "            unique_symbols.append(symbol)\n",
        "\n",
        "    if not unique_symbols:\n",
        "        print(\"❌ No valid symbols entered.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n✅ Will analyze {len(unique_symbols)} stocks: {', '.join(unique_symbols)}\")\n",
        "    confirm = input(\"Proceed? (y/n): \").strip().lower()\n",
        "\n",
        "    if confirm in ['y', 'yes']:\n",
        "        return unique_symbols\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def save_results_to_file(results, filename_prefix=\"analysis\"):\n",
        "    \"\"\"Save results to JSON file\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{filename_prefix}_{timestamp}.json\"\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "        print(f\"\\n💾 Results saved to: {filename}\")\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving results: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main Application Function\n",
        "def main():\n",
        "    \"\"\"Main application with continuous menu system\"\"\"\n",
        "\n",
        "    print(\"🚀 Initializing Financial Sentiment Analysis System...\")\n",
        "\n",
        "    try:\n",
        "        collector = FinancialDataCollector()\n",
        "        print(\"✅ System initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing system: {e}\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            display_menu()\n",
        "            choice = get_user_choice()\n",
        "\n",
        "            if choice == 1:\n",
        "                # Single stock analysis\n",
        "                print(\"\\n📈 SINGLE STOCK ANALYSIS\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "                symbol = get_single_stock()\n",
        "                print(f\"\\n🎯 Analyzing {symbol}...\")\n",
        "\n",
        "                results = collector.get_comprehensive_sentiment(symbol)\n",
        "                collector.display_analysis_results(results)\n",
        "\n",
        "                save_choice = input(\"\\n💾 Save results to file? (y/n): \").strip().lower()\n",
        "                if save_choice in ['y', 'yes']:\n",
        "                    save_results_to_file(results, f\"single_stock_{symbol}\")\n",
        "\n",
        "            elif choice == 2:\n",
        "                # Batch stock analysis\n",
        "                print(\"\\n📊 BATCH STOCK ANALYSIS\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "                symbols = get_batch_stocks()\n",
        "                if not symbols:\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n🎯 Starting batch analysis for {len(symbols)} stocks...\")\n",
        "\n",
        "                batch_results = collector.analyze_batch_stocks(symbols)\n",
        "                collector.display_batch_results(batch_results)\n",
        "\n",
        "                save_choice = input(\"\\n💾 Save batch results to file? (y/n): \").strip().lower()\n",
        "                if save_choice in ['y', 'yes']:\n",
        "                    save_results_to_file(batch_results, \"batch_analysis\")\n",
        "\n",
        "            elif choice == 3:\n",
        "                # Quick test\n",
        "                print(\"\\n🧪 QUICK TEST WITH AAPL\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "                print(\"🎯 Running quick test analysis...\")\n",
        "                results = collector.get_comprehensive_sentiment(\"AAPL\")\n",
        "                collector.display_analysis_results(results)\n",
        "\n",
        "            elif choice == 4:\n",
        "                # Exit\n",
        "                print(\"\\n👋 Thank you for using Financial Sentiment Analysis System!\")\n",
        "                print(\"📈 Happy Trading!\")\n",
        "                break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n🛑 Operation interrupted by user.\")\n",
        "            print(\"👋 Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
        "            print(\"Please try again or restart the application.\")\n",
        "            continue\n",
        "\n",
        "# Quick test function for development\n",
        "def quick_test():\n",
        "    \"\"\"Quick test function for development\"\"\"\n",
        "    print(\"🧪 Running quick test...\")\n",
        "\n",
        "    try:\n",
        "        collector = FinancialDataCollector()\n",
        "\n",
        "        test_symbol = \"AAPL\"\n",
        "        print(f\"\\n🧪 Testing with {test_symbol}...\")\n",
        "\n",
        "        results = collector.get_comprehensive_sentiment(test_symbol)\n",
        "        collector.display_analysis_results(results)\n",
        "\n",
        "        print(\"\\n✅ Quick test completed successfully!\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Quick test failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Colab-specific helper functions\n",
        "def setup_colab_environment():\n",
        "    \"\"\"Setup the Colab environment with necessary configurations\"\"\"\n",
        "    print(\"🔧 Setting up Google Colab environment...\")\n",
        "\n",
        "    # Install packages if needed\n",
        "    install_requirements()\n",
        "\n",
        "    # Create output directory for saving results\n",
        "    os.makedirs('sentiment_results', exist_ok=True)\n",
        "\n",
        "    print(\"✅ Colab environment setup complete!\")\n",
        "\n",
        "def run_example_analysis():\n",
        "    \"\"\"Run an example analysis for demonstration\"\"\"\n",
        "    print(\"🎯 Running Example Analysis...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        collector = FinancialDataCollector()\n",
        "\n",
        "        # Example stocks to analyze\n",
        "        example_stocks = [\"AAPL\", \"TSLA\", \"MSFT\"]\n",
        "\n",
        "        print(f\"📊 Analyzing example stocks: {', '.join(example_stocks)}\")\n",
        "\n",
        "        for symbol in example_stocks:\n",
        "            print(f\"\\n📈 Analyzing {symbol}...\")\n",
        "            result = collector.get_comprehensive_sentiment(symbol)\n",
        "\n",
        "            print(f\"   Sentiment: {result['overall_sentiment']['label']} \"\n",
        "                  f\"({result['overall_sentiment']['score']:.3f})\")\n",
        "            print(f\"   Trading Signal: {result['trading_signal']['recommendation']}\")\n",
        "            print(f\"   Data Points: {result['total_data_points']}\")\n",
        "\n",
        "        print(\"\\n✅ Example analysis completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Example analysis failed: {e}\")\n",
        "\n",
        "# Entry points for different use cases\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"🔍 Detected Google Colab environment\")\n",
        "        setup_colab_environment()\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"🔍 Running in standard Python environment\")\n",
        "\n",
        "    # Run the main application\n",
        "    main()\n",
        "\n",
        "# Additional utility functions for Colab users\n",
        "def analyze_stock(symbol: str):\n",
        "    \"\"\"Quick function to analyze a single stock - useful for Colab cells\"\"\"\n",
        "    try:\n",
        "        collector = FinancialDataCollector()\n",
        "        result = collector.get_comprehensive_sentiment(symbol)\n",
        "        collector.display_analysis_results(result)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error analyzing {symbol}: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_multiple_stocks(symbols: List[str]):\n",
        "    \"\"\"Quick function to analyze multiple stocks - useful for Colab cells\"\"\"\n",
        "    try:\n",
        "        collector = FinancialDataCollector()\n",
        "        batch_results = collector.analyze_batch_stocks(symbols)\n",
        "        collector.display_batch_results(batch_results)\n",
        "        return batch_results\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error analyzing stocks: {e}\")\n",
        "        return None\n",
        "\n",
        "# Usage examples for Colab:\n",
        "\"\"\"\n",
        "# Example usage in Google Colab:\n",
        "\n",
        "# 1. Run the full interactive system:\n",
        "# main()\n",
        "\n",
        "# 2. Quick single stock analysis:\n",
        "# result = analyze_stock(\"AAPL\")\n",
        "\n",
        "# 3. Quick batch analysis:\n",
        "# results = analyze_multiple_stocks([\"AAPL\", \"TSLA\", \"MSFT\"])\n",
        "\n",
        "# 4. Run example demonstration:\n",
        "# run_example_analysis()\n",
        "\n",
        "# 5. Quick test:\n",
        "# quick_test()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lE0nY5SclfBj",
        "outputId": "b223d662-5971-48e2-c7df-2726e148317a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All packages installed successfully!\n",
            "🔍 Detected Google Colab environment\n",
            "🔧 Setting up Google Colab environment...\n",
            "✅ Colab environment setup complete!\n",
            "🚀 Initializing Financial Sentiment Analysis System...\n",
            "🚀 Initializing Financial FinBERT model...\n",
            "📍 Model: ProsusAI/finbert\n",
            "🔧 Using device: cuda\n",
            "📥 Loading pre-trained FinBERT model...\n",
            "✅ FinBERT model loaded successfully!\n",
            "✅ Reddit client initialized\n",
            "✅ System initialized successfully!\n",
            "\n",
            "============================================================\n",
            "🚀 FINANCIAL SENTIMENT ANALYSIS SYSTEM\n",
            "============================================================\n",
            "1. 📈 Single Stock Analysis\n",
            "2. 📊 Batch Stock Analysis\n",
            "3. 🧪 Quick Test (AAPL)\n",
            "4. 🚪 Exit\n",
            "============================================================\n",
            "Enter your choice (1-4): 1\n",
            "\n",
            "📈 SINGLE STOCK ANALYSIS\n",
            "------------------------------\n",
            "\n",
            "Enter stock symbol (e.g., AAPL, TSLA, RELIANCE.NS): EICHERMOT.NS\n",
            "\n",
            "🎯 Analyzing EICHERMOT.NS...\n",
            "\n",
            "🎯 Starting comprehensive sentiment analysis for EICHERMOT.NS\n",
            "============================================================\n",
            "   📋 Company: Eicher Motors\n",
            "   🏢 Sector: Consumer Cyclical\n",
            "\n",
            "📊 Processing News...\n",
            "🔍 Fetching News data for EICHERMOT.NS...\n",
            "   📋 Company: Eicher Motors\n",
            "   🏢 Sector: Consumer Cyclical\n",
            "   📊 Analyzing 40 texts with FinBERT...\n",
            "   ✅ News: 40 items, sentiment: -0.005\n",
            "\n",
            "📊 Processing Reddit...\n",
            "🔍 Fetching Reddit data for EICHERMOT.NS...\n",
            "   📋 Company: Eicher Motors\n",
            "   🏢 Sector: Consumer Cyclical\n",
            "   📊 Analyzing 5 texts with FinBERT...\n",
            "   ✅ Reddit: 5 items, sentiment: -0.689\n",
            "\n",
            "📊 Processing Twitter...\n",
            "🔍 Fetching Twitter data for EICHERMOT.NS...\n",
            "   📋 Company: Eicher Motors\n",
            "   🏢 Sector: Consumer Cyclical\n",
            "   📊 Analyzing 5 texts with FinBERT...\n",
            "   ✅ Twitter: 5 items, sentiment: -0.552\n",
            "\n",
            "📊 Processing YouTube...\n",
            "🔍 Fetching YouTube data for EICHERMOT.NS...\n",
            "   📋 Company: Eicher Motors\n",
            "   🏢 Sector: Consumer Cyclical\n",
            "   📊 Analyzing 5 texts with FinBERT...\n",
            "   ✅ YouTube: 5 items, sentiment: 0.154\n",
            "\n",
            "📊 Processing RSS...\n",
            "🔍 Fetching RSS feeds data for EICHERMOT.NS...\n",
            "   📊 Analyzing 5 texts with FinBERT...\n",
            "   ✅ RSS: 5 items, sentiment: 0.342\n",
            "\n",
            "================================================================================\n",
            "🎯 FINANCIAL SENTIMENT ANALYSIS REPORT\n",
            "================================================================================\n",
            "📈 Symbol: EICHERMOT.NS\n",
            "🏢 Company: Eicher Motors\n",
            "🏭 Sector: Consumer Cyclical\n",
            "🕐 Analysis Time: 2025-08-04T06:10:14.256150\n",
            "📊 Total Data Points: 60\n",
            "\n",
            "🎯 OVERALL SENTIMENT\n",
            "   Score: -0.1489\n",
            "   Label: NEUTRAL\n",
            "   Confidence: 0.8537\n",
            "\n",
            "📈 TRADING SIGNAL\n",
            "   Recommendation: HOLD\n",
            "   Strength: Low\n",
            "   Reason: Neutral sentiment (-0.149) - wait for clearer signals\n",
            "\n",
            "📊 SOURCE BREAKDOWN\n",
            "   News: 40 items | Sentiment: -0.005 (NEUTRAL) | Confidence: 0.879\n",
            "   Reddit: 5 items | Sentiment: -0.689 (NEGATIVE) | Confidence: 0.909\n",
            "   Twitter: 5 items | Sentiment: -0.552 (NEGATIVE) | Confidence: 0.858\n",
            "   YouTube: 5 items | Sentiment: 0.154 (NEUTRAL) | Confidence: 0.754\n",
            "   RSS: 5 items | Sentiment: 0.342 (POSITIVE) | Confidence: 0.774\n",
            "\n",
            "🔥 MOST INFLUENTIAL HEADLINES/MESSAGES\n",
            "   1. [Reddit Demo] EICHERMOT.NS quarterly results exceeded expectations, strong fundamentals\n",
            "      Sentiment: -0.933 | Confidence: 0.956\n",
            "\n",
            "   2. [Twitter Demo] $EICHERMOT.NS fundamentals remain strong despite market volatility\n",
            "      Sentiment: -0.932 | Confidence: 0.955\n",
            "\n",
            "   3. [BusinessLine] TVS Motor Q1 results: Net profit up 35% to ₹779 cr; revenue crosses ₹10,000 cr\n",
            "      Sentiment: -0.931 | Confidence: 0.953\n",
            "      URL: https://www.thehindubusinessline.com/companies/tvs-motor-q1-results-net-profit-up-35-to-779-cr-revenue-crosses-10000-cr/article69877593.ece\n",
            "\n",
            "   4. [The Times of India] Eicher Motors reported a 9% YoY rise in Q1FY26 net profit to Rs 1,205 crore, with revenue up 15% to Rs 5,042 crore, driven by strong Royal Enfield sal...\n",
            "      Sentiment: -0.926 | Confidence: 0.947\n",
            "      URL: https://economictimes.indiatimes.com/markets/stocks/news/eicher-motors-shares-in-focus-after-q1-pat-rises-9-yoy-should-you-buy/articleshow/123034083.cms\n",
            "\n",
            "   5. [BusinessLine] Q1 Results Today Live Updates 1st August 2025: Find all the latest updates related to Q1FY26 results of ITC, Adani Power, Tata Power Company, Godrej P...\n",
            "      Sentiment: 0.920 | Confidence: 0.949\n",
            "      URL: https://www.thehindubusinessline.com/markets/stock-markets/q1-results-today-live-itc-adani-power-tata-power-godrej-properties-upl-delhivery-multi-commodity-exchange-of-india-01-august-2025-adani-enterprises-maruti-sun-pharma-dabur-vedanta-hul/article69877851.ece\n",
            "\n",
            "📈 TOP POSITIVE SIGNALS\n",
            "   1. [BusinessLine] Q1 Results Today Live Updates 1st August 2025: Find all the latest updates related to Q1FY26 results of ITC, Adani Power, Tata Power Company, Godrej P...\n",
            "      Sentiment: +0.920\n",
            "\n",
            "   2. [BusinessLine] Q1 Results Highlights 1st August 2025: Find all the latest updates related to Q1FY26 results of ITC, Adani Power, Tata Power Company, Godrej Propertie...\n",
            "      Sentiment: +0.919\n",
            "\n",
            "   3. [YouTube Demo] EICHERMOT.NS Stock Analysis: Buy or Sell? Complete review\n",
            "      Sentiment: +0.915\n",
            "\n",
            "📉 TOP NEGATIVE SIGNALS\n",
            "   1. [Reddit Demo] EICHERMOT.NS quarterly results exceeded expectations, strong fundamentals\n",
            "      Sentiment: -0.933\n",
            "\n",
            "   2. [Twitter Demo] $EICHERMOT.NS fundamentals remain strong despite market volatility\n",
            "      Sentiment: -0.932\n",
            "\n",
            "   3. [BusinessLine] TVS Motor Q1 results: Net profit up 35% to ₹779 cr; revenue crosses ₹10,000 cr\n",
            "      Sentiment: -0.931\n",
            "\n",
            "🤖 MODEL INFORMATION\n",
            "   Method: finbert\n",
            "   Training Accuracy: Pre-trained FinBERT\n",
            "================================================================================\n",
            "\n",
            "💾 Save results to file? (y/n): n\n",
            "\n",
            "============================================================\n",
            "🚀 FINANCIAL SENTIMENT ANALYSIS SYSTEM\n",
            "============================================================\n",
            "1. 📈 Single Stock Analysis\n",
            "2. 📊 Batch Stock Analysis\n",
            "3. 🧪 Quick Test (AAPL)\n",
            "4. 🚪 Exit\n",
            "============================================================\n",
            "\n",
            "👋 Goodbye!\n",
            "\n",
            "👋 Thank you for using Financial Sentiment Analysis System!\n",
            "📈 Happy Trading!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Example usage in Google Colab:\\n\\n# 1. Run the full interactive system:\\n# main()\\n\\n# 2. Quick single stock analysis:\\n# result = analyze_stock(\"AAPL\")\\n\\n# 3. Quick batch analysis:\\n# results = analyze_multiple_stocks([\"AAPL\", \"TSLA\", \"MSFT\"])\\n\\n# 4. Run example demonstration:\\n# run_example_analysis()\\n\\n# 5. Quick test:\\n# quick_test()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}